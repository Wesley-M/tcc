"repo","content"
"Scikit-Opt","





















# [scikit-opt](https://github.com/guofei9987/scikit-opt)

[![pypi](https://img.shields.io/pypi/v/scikit-opt)](https://pypi.org/project/scikit-opt/)
[![build status](https://travis-ci.com/guofei9987/scikit-opt.svg?branch=master)](https://travis-ci.com/guofei9987/scikit-opt)
[![codecov](https://codecov.io/gh/guofei9987/scikit-opt/branch/master/graph/badge.svg)](https://codecov.io/gh/guofei9987/scikit-opt)
[![license](https://img.shields.io/pypi/l/scikit-opt.svg)](https://github.com/guofei9987/scikit-opt/blob/master/license)
![python](https://img.shields.io/badge/python->=3.5-green.svg)
![platform](https://img.shields.io/badge/platform-windows%20|%20linux%20|%20macos-green.svg)
[![fork](https://img.shields.io/github/forks/guofei9987/scikit-opt?style=social)](https://github.com/guofei9987/scikit-opt/fork)
[![downloads](https://pepy.tech/badge/scikit-opt)](https://pepy.tech/project/scikit-opt)
[![discussions](https://img.shields.io/badge/discussions-green.svg)](https://github.com/guofei9987/scikit-opt/discussions)




swarm intelligence in python  
(genetic algorithm, particle swarm optimization, simulated annealing, ant colony algorithm, immune algorithm, artificial fish swarm algorithm in python)


- **documentation:** [https://scikit-opt.github.io/scikit-opt/#/en/](https://scikit-opt.github.io/scikit-opt/#/en/)
- **ÊñáÊ°£Ôºö** [https://scikit-opt.github.io/scikit-opt/#/zh/](https://scikit-opt.github.io/scikit-opt/#/zh/)  
- **source code:** [https://github.com/guofei9987/scikit-opt](https://github.com/guofei9987/scikit-opt)
- **help us improve scikit-opt** [https://www.wjx.cn/jq/50964691.aspx](https://www.wjx.cn/jq/50964691.aspx)

# install
```bash
pip install scikit-opt
```

for the current developer version:
```bach
git clone git@github.com:guofei9987/scikit-opt.git
cd scikit-opt
pip install .
```

# features
## feature1: udf

**udf** (user defined function) is available now!

for example, you just worked out a new type of `selection` function.  
now, your `selection` function is like this:  
-> demo code: [examples/demo_ga_udf.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#l1)
```python
# step1: define your own operator:
def selection_tournament(algorithm, tourn_size):
    fitv = algorithm.fitv
    sel_index = []
    for i in range(algorithm.size_pop):
        aspirants_index = np.random.choice(range(algorithm.size_pop), size=tourn_size)
        sel_index.append(max(aspirants_index, key=lambda i: fitv[i]))
    algorithm.chrom = algorithm.chrom[sel_index, :]  # next generation
    return algorithm.chrom


```

import and build ga  
-> demo code: [examples/demo_ga_udf.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#l12)
```python
import numpy as np
from sko.ga import ga, ga_tsp

demo_func = lambda x: x[0] ** 2 + (x[1] - 0.05) ** 2 + (x[2] - 0.5) ** 2
ga = ga(func=demo_func, n_dim=3, size_pop=100, max_iter=500, prob_mut=0.001,
        lb=[-1, -10, -5], ub=[2, 10, 2], precision=[1e-7, 1e-7, 1])

```
regist your udf to ga  
-> demo code: [examples/demo_ga_udf.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#l20)
```python
ga.register(operator_name='selection', operator=selection_tournament, tourn_size=3)
```

scikit-opt also provide some operators  
-> demo code: [examples/demo_ga_udf.py#s4](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#l22)
```python
from sko.operators import ranking, selection, crossover, mutation

ga.register(operator_name='ranking', operator=ranking.ranking). \
    register(operator_name='crossover', operator=crossover.crossover_2point). \
    register(operator_name='mutation', operator=mutation.mutation)
```
now do ga as usual  
-> demo code: [examples/demo_ga_udf.py#s5](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#l28)
```python
best_x, best_y = ga.run()
print('best_x:', best_x, '\n', 'best_y:', best_y)
```

> until now, the **udf** surport `crossover`, `mutation`, `selection`, `ranking` of ga
> scikit-opt provide a dozen of operators, see [here](https://github.com/guofei9987/scikit-opt/tree/master/sko/operators)

for advanced users:

-> demo code: [examples/demo_ga_udf.py#s6](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#l31)
```python
class myga(ga):
    def selection(self, tourn_size=3):
        fitv = self.fitv
        sel_index = []
        for i in range(self.size_pop):
            aspirants_index = np.random.choice(range(self.size_pop), size=tourn_size)
            sel_index.append(max(aspirants_index, key=lambda i: fitv[i]))
        self.chrom = self.chrom[sel_index, :]  # next generation
        return self.chrom

    ranking = ranking.ranking


demo_func = lambda x: x[0] ** 2 + (x[1] - 0.05) ** 2 + (x[2] - 0.5) ** 2
my_ga = myga(func=demo_func, n_dim=3, size_pop=100, max_iter=500, lb=[-1, -10, -5], ub=[2, 10, 2],
             precision=[1e-7, 1e-7, 1])
best_x, best_y = my_ga.run()
print('best_x:', best_x, '\n', 'best_y:', best_y)
```

##  feature2: continue to run
(new in version 0.3.6)  
run an algorithm for 10 iterations, and then run another 20 iterations base on the 10 iterations before:
```python
from sko.ga import ga

func = lambda x: x[0] ** 2
ga = ga(func=func, n_dim=1)
ga.run(10)
ga.run(20)
```

## feature3: 4-ways to accelerate
- vectorization
- multithreading
- multiprocessing
- cached

see [https://github.com/guofei9987/scikit-opt/blob/master/examples/example_function_modes.py](https://github.com/guofei9987/scikit-opt/blob/master/examples/example_function_modes.py)



## feature4: gpu computation
 we are developing gpu computation, which will be stable on version 1.0.0  
an example is already available: [https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_gpu.py](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_gpu.py)


# quick start

## 1. differential evolution
**step1**Ôºödefine your problem  
-> demo code: [examples/demo_de.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_de.py#l1)
```python
'''
min f(x1, x2, x3) = x1^2 + x2^2 + x3^2
s.t.
    x1*x2 >= 1
    x1*x2 <= 5
    x2 + x3 = 1
    0 <= x1, x2, x3 <= 5
'''


def obj_func(p):
    x1, x2, x3 = p
    return x1 ** 2 + x2 ** 2 + x3 ** 2


constraint_eq = [
    lambda x: 1 - x[1] - x[2]
]

constraint_ueq = [
    lambda x: 1 - x[0] * x[1],
    lambda x: x[0] * x[1] - 5
]

```

**step2**: do differential evolution  
-> demo code: [examples/demo_de.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_de.py#l25)
```python
from sko.de import de

de = de(func=obj_func, n_dim=3, size_pop=50, max_iter=800, lb=[0, 0, 0], ub=[5, 5, 5],
        constraint_eq=constraint_eq, constraint_ueq=constraint_ueq)

best_x, best_y = de.run()
print('best_x:', best_x, '\n', 'best_y:', best_y)

```

## 2. genetic algorithm

**step1**Ôºödefine your problem  
-> demo code: [examples/demo_ga.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga.py#l1)
```python
import numpy as np


def schaffer(p):
    '''
    this function has plenty of local minimum, with strong shocks
    global minimum at (0,0) with value 0
    https://en.wikipedia.org/wiki/test_functions_for_optimization
    '''
    x1, x2 = p
    part1 = np.square(x1) - np.square(x2)
    part2 = np.square(x1) + np.square(x2)
    return 0.5 + (np.square(np.sin(part1)) - 0.5) / np.square(1 + 0.001 * part2)


```

**step2**: do genetic algorithm  
-> demo code: [examples/demo_ga.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga.py#l16)
```python
from sko.ga import ga

ga = ga(func=schaffer, n_dim=2, size_pop=50, max_iter=800, prob_mut=0.001, lb=[-1, -1], ub=[1, 1], precision=1e-7)
best_x, best_y = ga.run()
print('best_x:', best_x, '\n', 'best_y:', best_y)
```

-> demo code: [examples/demo_ga.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga.py#l22)
```python
import pandas as pd
import matplotlib.pyplot as plt

y_history = pd.dataframe(ga.all_history_y)
fig, ax = plt.subplots(2, 1)
ax[0].plot(y_history.index, y_history.values, '.', color='red')
y_history.min(axis=1).cummin().plot(kind='line')
plt.show()
```

![figure_1-1](https://img1.github.io/heuristic_algorithm/ga_1.png)

### 2.2 genetic algorithm for tsp(travelling salesman problem)
just import the `ga_tsp`, it overloads the `crossover`, `mutation` to solve the tsp

**step1**: define your problem. prepare your points coordinate and the distance matrix.  
here i generate the data randomly as a demo:  
-> demo code: [examples/demo_ga_tsp.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_tsp.py#l1)
```python
import numpy as np
from scipy import spatial
import matplotlib.pyplot as plt

num_points = 50

points_coordinate = np.random.rand(num_points, 2)  # generate coordinate of points
distance_matrix = spatial.distance.cdist(points_coordinate, points_coordinate, metric='euclidean')


def cal_total_distance(routine):
    '''the objective function. input routine, return total distance.
    cal_total_distance(np.arange(num_points))
    '''
    num_points, = routine.shape
    return sum([distance_matrix[routine[i % num_points], routine[(i + 1) % num_points]] for i in range(num_points)])


```

**step2**: do ga  
-> demo code: [examples/demo_ga_tsp.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_tsp.py#l19)
```python

from sko.ga import ga_tsp

ga_tsp = ga_tsp(func=cal_total_distance, n_dim=num_points, size_pop=50, max_iter=500, prob_mut=1)
best_points, best_distance = ga_tsp.run()

```

**step3**: plot the result:  
-> demo code: [examples/demo_ga_tsp.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_tsp.py#l26)
```python
fig, ax = plt.subplots(1, 2)
best_points_ = np.concatenate([best_points, [best_points[0]]])
best_points_coordinate = points_coordinate[best_points_, :]
ax[0].plot(best_points_coordinate[:, 0], best_points_coordinate[:, 1], 'o-r')
ax[1].plot(ga_tsp.generation_best_y)
plt.show()
```

![ga_tps](https://img1.github.io/heuristic_algorithm/ga_tsp.png)


## 3. pso(particle swarm optimization)

### 3.1 pso
**step1**: define your problem:  
-> demo code: [examples/demo_pso.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_pso.py#l1)
```python
def demo_func(x):
    x1, x2, x3 = x
    return x1 ** 2 + (x2 - 0.05) ** 2 + x3 ** 2


```

**step2**: do pso  
-> demo code: [examples/demo_pso.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_pso.py#l6)
```python
from sko.pso import pso

pso = pso(func=demo_func, n_dim=3, pop=40, max_iter=150, lb=[0, -1, 0.5], ub=[1, 1, 1], w=0.8, c1=0.5, c2=0.5)
pso.run()
print('best_x is ', pso.gbest_x, 'best_y is', pso.gbest_y)

```

**step3**: plot the result  
-> demo code: [examples/demo_pso.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_pso.py#l13)
```python
import matplotlib.pyplot as plt

plt.plot(pso.gbest_y_hist)
plt.show()
```


![pso_tps](https://img1.github.io/heuristic_algorithm/pso.png)

### 3.2 pso with nonlinear constraint

if you need nolinear constraint like `(x[0] - 1) ** 2 + (x[1] - 0) ** 2 - 0.5 ** 2<=0`  
codes are like this:
```python
constraint_ueq = (
    lambda x: (x[0] - 1) ** 2 + (x[1] - 0) ** 2 - 0.5 ** 2
    ,
)
pso = pso(func=demo_func, n_dim=2, pop=40, max_iter=max_iter, lb=[-2, -2], ub=[2, 2]
          , constraint_ueq=constraint_ueq)
```

note that, you can add more then one nonlinear constraint. just add it to `constraint_ueq`

more over, we have an animation:  
![pso_ani](https://img1.github.io/heuristic_algorithm/pso.gif)  
‚Üë**see [examples/demo_pso_ani.py](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_pso_ani.py)**


## 4. sa(simulated annealing)
### 4.1 sa for multiple function
**step1**: define your problem  
-> demo code: [examples/demo_sa.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa.py#l1)
```python
demo_func = lambda x: x[0] ** 2 + (x[1] - 0.05) ** 2 + x[2] ** 2

```
**step2**: do sa  
-> demo code: [examples/demo_sa.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa.py#l3)
```python
from sko.sa import sa

sa = sa(func=demo_func, x0=[1, 1, 1], t_max=1, t_min=1e-9, l=300, max_stay_counter=150)
best_x, best_y = sa.run()
print('best_x:', best_x, 'best_y', best_y)

```

**step3**: plot the result  
-> demo code: [examples/demo_sa.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa.py#l10)
```python
import matplotlib.pyplot as plt
import pandas as pd

plt.plot(pd.dataframe(sa.best_y_history).cummin(axis=0))
plt.show()

```
![sa](https://img1.github.io/heuristic_algorithm/sa.png)


moreover, scikit-opt provide 3 types of simulated annealing: fast, boltzmann, cauchy. see [more sa](https://scikit-opt.github.io/scikit-opt/#/en/more_sa)
### 4.2 sa for tsp
**step1**: oh, yes, define your problems. to boring to copy this step.  

**step2**: do sa for tsp  
-> demo code: [examples/demo_sa_tsp.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa_tsp.py#l21)
```python
from sko.sa import sa_tsp

sa_tsp = sa_tsp(func=cal_total_distance, x0=range(num_points), t_max=100, t_min=1, l=10 * num_points)

best_points, best_distance = sa_tsp.run()
print(best_points, best_distance, cal_total_distance(best_points))
```

**step3**: plot the result  
-> demo code: [examples/demo_sa_tsp.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa_tsp.py#l28)
```python
from matplotlib.ticker import formatstrformatter

fig, ax = plt.subplots(1, 2)

best_points_ = np.concatenate([best_points, [best_points[0]]])
best_points_coordinate = points_coordinate[best_points_, :]
ax[0].plot(sa_tsp.best_y_history)
ax[0].set_xlabel(""iteration"")
ax[0].set_ylabel(""distance"")
ax[1].plot(best_points_coordinate[:, 0], best_points_coordinate[:, 1],
           marker='o', markerfacecolor='b', color='c', linestyle='-')
ax[1].xaxis.set_major_formatter(formatstrformatter('%.3f'))
ax[1].yaxis.set_major_formatter(formatstrformatter('%.3f'))
ax[1].set_xlabel(""longitude"")
ax[1].set_ylabel(""latitude"")
plt.show()

```
![sa](https://img1.github.io/heuristic_algorithm/sa_tsp.png)


more: plot the animation:  

![sa](https://img1.github.io/heuristic_algorithm/sa_tsp1.gif)  
‚Üë**see [examples/demo_sa_tsp.py](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa_tsp.py)**




## 5. aca (ant colony algorithm) for tsp
-> demo code: [examples/demo_aca_tsp.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_aca_tsp.py#l17)
```python
from sko.aca import aca_tsp

aca = aca_tsp(func=cal_total_distance, n_dim=num_points,
              size_pop=50, max_iter=200,
              distance_matrix=distance_matrix)

best_x, best_y = aca.run()

```

![aca](https://img1.github.io/heuristic_algorithm/aca_tsp.png)


## 6. immune algorithm (ia)
-> demo code: [examples/demo_ia.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ia.py#l6)
```python

from sko.ia import ia_tsp

ia_tsp = ia_tsp(func=cal_total_distance, n_dim=num_points, size_pop=500, max_iter=800, prob_mut=0.2,
                t=0.7, alpha=0.95)
best_points, best_distance = ia_tsp.run()
print('best routine:', best_points, 'best_distance:', best_distance)

```

![ia](https://img1.github.io/heuristic_algorithm/ia2.png)

## 7. artificial fish swarm algorithm (afsa)
-> demo code: [examples/demo_afsa.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_afsa.py#l1)
```python
def func(x):
    x1, x2 = x
    return 1 / x1 ** 2 + x1 ** 2 + 1 / x2 ** 2 + x2 ** 2


from sko.afsa import afsa

afsa = afsa(func, n_dim=2, size_pop=50, max_iter=300,
            max_try_num=100, step=0.5, visual=0.3,
            q=0.98, delta=0.5)
best_x, best_y = afsa.run()
print(best_x, best_y)
```


# projects using scikit-opt

- [yu, j., he, y., yan, q., & kang, x. (2021). specview: malware spectrum visualization framework with singular spectrum transformation. ieee transactions on information forensics and security, 16, 5093-5107.](https://ieeexplore.ieee.org/abstract/document/9607026/)
- [zhen, h., zhai, h., ma, w., zhao, l., weng, y., xu, y., ... & he, x. (2021). design and tests of reinforcement-learning-based optimal power flow solution generator. energy reports.](https://www.sciencedirect.com/science/article/pii/s2352484721012737)
- [heinrich, k., zschech, p., janiesch, c., & bonin, m. (2021). process data properties matter: introducing gated convolutional neural networks (gcnn) and key-value-predict attention networks (kvp) for next event prediction with deep learning. decision support systems, 143, 113494.](https://www.sciencedirect.com/science/article/pii/s016792362100004x)
- [tang, h. k., & goh, s. k. (2021). a novel non-population-based meta-heuristic optimizer inspired by the philosophy of yi jing. arxiv preprint arxiv:2104.08564.](https://arxiv.org/abs/2104.08564)
- [wu, g., li, l., li, x., chen, y., chen, z., qiao, b., ... & xia, l. (2021). graph embedding based real-time social event matching for ebsns recommendation. world wide web, 1-22.](https://link.springer.com/article/10.1007/s11280-021-00934-y)
- [pan, x., zhang, z., zhang, h., wen, z., ye, w., yang, y., ... & zhao, x. (2021). a fast and robust mixture gases identification and concentration detection algorithm based on attention mechanism equipped recurrent neural network with double loss function. sensors and actuators b: chemical, 342, 129982.](https://www.sciencedirect.com/science/article/abs/pii/s0925400521005517)
- [castella balcell, m. (2021). optimization of the station keeping system for the windcrete floating offshore wind turbine.](https://upcommons.upc.edu/handle/2117/350262)
- [zhai, b., wang, y., wang, w., & wu, b. (2021). optimal variable speed limit control strategy on freeway segments under fog conditions. arxiv preprint arxiv:2107.14406.](https://arxiv.org/abs/2107.14406)
- [yap, x. h. (2021). multi-label classification on locally-linear data: application to chemical toxicity prediction.](https://etd.ohiolink.edu/apexprod/rws_olink/r/1501/10?clear=10&p10_accession_num=wright162901936395651)
- [gebhard, l. (2021). expansion planning of low-voltage grids using ant colony optimization ausbauplanung von niederspannungsnetzen mithilfe eines ameisenalgorithmus.](https://ad-publications.cs.uni-freiburg.de/theses/master_lukas_gebhard_2021.pdf)
- [ma, x., zhou, h., & li, z. (2021). optimal design for interdependencies between hydrogen and power systems. ieee transactions on industry applications.](https://ieeexplore.ieee.org/abstract/document/9585654)
- [de curso, t. d. c. (2021). estudo do modelo johansen-ledoit-sornette de bolhas financeiras.](https://d1wqtxts1xzle7.cloudfront.net/67649721/tcc_thibor_final-with-cover-page-v2.pdf?expires=1639140872&signature=ldzovsago0mlmlvsqjnzpllrhlyt5wdidmbjm1ywog5bsx6apyre9ahuwfnfnc96uvam573wihmev08qlk2vhrcqs1d0buenbt5fworuq6ptdomsxmpbb-lgtu9etimb4sbyvcqb-x3c7hh0ec1fojz040gxjpwdali3e1tdocgrnoabzmgniyx6akfizaaxmiqev3418~870bh4ioqxoapie6-23lcol-32t~fsjsorenolukcosv6uhpourkgsrufay-c2hbuwp36ij7coh0jsto1e45dvgvqndvshz7tmei~0upgh-a8mwzq9h2elcbcn~unq8ycxoa4tukfpcw__&key-pair-id=apkajlohf5ggslrbv4za)
- [wu, t., liu, j., liu, j., huang, z., wu, h., zhang, c., ... & zhang, g. (2021). a novel ai-based framework for aoi-optimal trajectory planning in uav-assisted wireless sensor networks. ieee transactions on wireless communications.](https://ieeexplore.ieee.org/abstract/document/9543607)
- [liu, h., wen, z., & cai, w. (2021, august). fastpso: towards efficient swarm intelligence algorithm on gpus. in 50th international conference on parallel processing (pp. 1-10).](https://dl.acm.org/doi/abs/10.1145/3472456.3472474)
- [mahbub, r. (2020). algorithms and optimization techniques for solving tsp.](https://raiyanmahbub.com/images/research_paper.pdf)
- [li, j., chen, t., lim, k., chen, l., khan, s. a., xie, j., & wang, x. (2019). deep learning accelerated gold nanocluster synthesis. advanced intelligent systems, 1(3), 1900029.](https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.201900029)
"
"deepface","# deepface

<div align=""center"">

[![pypi downloads](https://static.pepy.tech/personalized-badge/deepface?period=total&units=international_system&left_color=grey&right_color=blue&left_text=pypi%20downloads)](https://pepy.tech/project/deepface)
[![conda downloads](https://img.shields.io/conda/dn/conda-forge/deepface?color=green&label=conda%20downloads)](https://anaconda.org/conda-forge/deepface)
[![stars](https://img.shields.io/github/stars/serengil/deepface?color=yellow&style=flat)](https://github.com/serengil/deepface/stargazers)
[![license](http://img.shields.io/:license-mit-green.svg?style=flat)](https://github.com/serengil/deepface/blob/master/license)
[![support me on patreon](https://img.shields.io/endpoint.svg?url=https%3a%2f%2fshieldsio-patreon.vercel.app%2fapi%3fusername%3dserengil%26type%3dpatrons&style=flat)](https://www.patreon.com/serengil?repo=deepface)
[![github sponsors](https://img.shields.io/github/sponsors/serengil?logo=github&color=lightgray)](https://github.com/sponsors/serengil)

[![doi](http://img.shields.io/:doi-10.1109/asyu50717.2020.9259802-blue.svg?style=flat)](https://doi.org/10.1109/asyu50717.2020.9259802)
[![doi](http://img.shields.io/:doi-10.1109/iceet53442.2021.9659697-blue.svg?style=flat)](https://doi.org/10.1109/iceet53442.2021.9659697)

</div>

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-icon-labeled.png"" width=""200"" height=""240""></p>

deepface is a lightweight [face recognition](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and facial attribute analysis ([age](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [gender](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [emotion](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) and [race](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/)) framework for python. it is a hybrid face recognition framework wrapping **state-of-the-art** models: [`vgg-face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/), [`google facenet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/), [`openface`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/), [`facebook deepface`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/), [`deepid`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/), [`arcface`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/), [`dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/) and `sface`.

experiments show that human beings have 97.53% accuracy on facial recognition tasks whereas those models already reached and passed that accuracy level.

## installation [![pypi](https://img.shields.io/pypi/v/deepface.svg)](https://pypi.org/project/deepface/) [![conda](https://img.shields.io/conda/vn/conda-forge/deepface.svg)](https://anaconda.org/conda-forge/deepface)

the easiest way to install deepface is to download it from [`pypi`](https://pypi.org/project/deepface/). it's going to install the library itself and its prerequisites as well.

```shell
$ pip install deepface
```

secondly, deepface is also available at [`conda`](https://anaconda.org/conda-forge/deepface). you can alternatively install the package via conda.

```shell
$ conda install -c conda-forge deepface
```

thirdly, you can install deepface from its source code.

```shell
$ git clone https://github.com/serengil/deepface.git
$ cd deepface
$ pip install -e .
```

then you will be able to import the library and use its functionalities.

```python
from deepface import deepface
```

**facial recognition** - [`demo`](https://youtu.be/wnuvyqp4h44)

a modern [**face recognition pipeline**](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) consists of 5 common stages: [detect](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/), [align](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/), [normalize](https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/), [represent](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and [verify](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/). while deepface handles all these common stages in the background, you don‚Äôt need to acquire in-depth knowledge about all the processes behind it. you can just call its verification, find or analysis function with a single line of code.

**face verification** - [`demo`](https://youtu.be/krcvkncophe)

this function verifies face pairs as same person or different persons. it expects exact image paths as inputs. passing numpy or base64 encoded images is also welcome. then, it is going to return a dictionary and you should check just its verified key.

```python
result = deepface.verify(img1_path = ""img1.jpg"", img2_path = ""img2.jpg"")
```

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-1.jpg"" width=""95%"" height=""95%""></p>

verification function can also handle many faces in the face pairs. in this case, the most similar faces will be compared.

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/verify-many-faces.jpg"" width=""95%"" height=""95%""></p>

**face recognition** - [`demo`](https://youtu.be/hrjp-estm_s)

[face recognition](https://sefiks.com/2020/05/25/large-scale-face-recognition-for-deep-learning/) requires applying face verification many times. herein, deepface has an out-of-the-box find function to handle this action. it's going to look for the identity of input image in the database path and it will return list of pandas data frame as output. result is going to be the size of faces appearing in the source image. besides, target images in the database can have many faces as well.


```python
dfs = deepface.find(img_path = ""img1.jpg"", db_path = ""c:/workspace/my_db"")
```

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg"" width=""95%"" height=""95%""></p>

**embeddings**

face recognition models basically represent facial images as multi-dimensional vectors. sometimes, you need those embedding vectors directly. deepface comes with a dedicated representation function. represent function returns a list of embeddings. result is going to be the size of faces appearing in the image path.

```python
embedding_objs = deepface.represent(img_path = ""img.jpg"")
```

this function returns an array as embedding. the size of the embedding array would be different based on the model name. for instance, vgg-face is the default model and it represents facial images as 2622 dimensional vectors.

```python
embedding = embedding_objs[0][""embedding""]
assert isinstance(embedding, list)
assert model_name = ""vgg-face"" and len(embedding) == 2622
```

here, embedding is also [plotted](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) with 2622 slots horizontally. each slot is corresponding to a dimension value in the embedding vector and dimension value is explained in the colorbar on the right. similar to 2d barcodes, vertical dimension stores no information in the illustration.

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/embedding.jpg"" width=""95%"" height=""95%""></p>

**face recognition models** - [`demo`](https://youtu.be/i_mowvhbldi)

deepface is a **hybrid** face recognition package. it currently wraps many **state-of-the-art** face recognition models: [`vgg-face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) , [`google facenet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/), [`openface`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/), [`facebook deepface`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/), [`deepid`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/), [`arcface`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/), [`dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/) and `sface`. the default configuration uses vgg-face model.

```python
models = [
  ""vgg-face"", 
  ""facenet"", 
  ""facenet512"", 
  ""openface"", 
  ""deepface"", 
  ""deepid"", 
  ""arcface"", 
  ""dlib"", 
  ""sface"",
]

#face verification
result = deepface.verify(img1_path = ""img1.jpg"", 
      img2_path = ""img2.jpg"", 
      model_name = models[0]
)

#face recognition
dfs = deepface.find(img_path = ""img1.jpg"",
      db_path = ""c:/workspace/my_db"", 
      model_name = models[1]
)

#embeddings
embedding_objs = deepface.represent(img_path = ""img.jpg"", 
      model_name = models[2]
)
```

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/model-portfolio-v8.jpg"" width=""95%"" height=""95%""></p>

facenet, vgg-face, arcface and dlib are [overperforming](https://youtu.be/i_mowvhbldi) ones based on experiments. you can find out the scores of those models below on both [labeled faces in the wild](https://sefiks.com/2020/08/27/labeled-faces-in-the-wild-for-face-recognition/) and youtube faces in the wild data sets declared by its creators.

| model | lfw score | ytf score |
| ---   | --- | --- |
| facenet512 | 99.65% | - |
| sface | 99.60% | - |
| arcface | 99.41% | - |
| dlib | 99.38 % | - |
| facenet | 99.20% | - |
| vgg-face | 98.78% | 97.40% |
| *human-beings* | *97.53%* | - |
| openface | 93.80% | - |
| deepid | - | 97.05% |

**similarity**

face recognition models are regular [convolutional neural networks](https://sefiks.com/2018/03/23/convolutional-autoencoder-clustering-images-with-neural-networks/) and they are responsible to represent faces as vectors. we expect that a face pair of same person should be [more similar](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/) than a face pair of different persons.

similarity could be calculated by different metrics such as [cosine similarity](https://sefiks.com/2018/08/13/cosine-similarity-in-machine-learning/), euclidean distance and l2 form. the default configuration uses cosine similarity.

```python
metrics = [""cosine"", ""euclidean"", ""euclidean_l2""]

#face verification
result = deepface.verify(img1_path = ""img1.jpg"", 
          img2_path = ""img2.jpg"", 
          distance_metric = metrics[1]
)

#face recognition
dfs = deepface.find(img_path = ""img1.jpg"", 
          db_path = ""c:/workspace/my_db"", 
          distance_metric = metrics[2]
)
```

euclidean l2 form [seems](https://youtu.be/i_mowvhbldi) to be more stable than cosine and regular euclidean distance based on experiments.

**facial attribute analysis** - [`demo`](https://youtu.be/gt2uen85bda)

deepface also comes with a strong facial attribute analysis module including [`age`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [`gender`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [`facial expression`](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) (including angry, fear, neutral, sad, disgust, happy and surprise) and [`race`](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/) (including asian, white, middle eastern, indian, latino and black) predictions. result is going to be the size of faces appearing in the source image.

```python
objs = deepface.analyze(img_path = ""img4.jpg"", 
        actions = ['age', 'gender', 'race', 'emotion']
)
```

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg"" width=""95%"" height=""95%""></p>

age model got ¬± 4.65 mae; gender model got 97.44% accuracy, 96.29% precision and 95.05% recall as mentioned in its [tutorial](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/).


**face detectors** - [`demo`](https://youtu.be/gz2p2hj2h5k)

face detection and alignment are important early stages of a modern face recognition pipeline. experiments show that just alignment increases the face recognition accuracy almost 1%. [`opencv`](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/), [`ssd`](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/), [`dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/),  [`mtcnn`](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/), [`retinaface`](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) and [`mediapipe`](https://sefiks.com/2022/01/14/deep-face-detection-with-mediapipe/) detectors are wrapped in deepface.

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-portfolio-v3.jpg"" width=""95%"" height=""95%""></p>

all deepface functions accept an optional detector backend input argument. you can switch among those detectors with this argument. opencv is the default detector.

```python
backends = [
  'opencv', 
  'ssd', 
  'dlib', 
  'mtcnn', 
  'retinaface', 
  'mediapipe'
]

#face verification
obj = deepface.verify(img1_path = ""img1.jpg"", 
        img2_path = ""img2.jpg"", 
        detector_backend = backends[0]
)

#face recognition
dfs = deepface.find(img_path = ""img.jpg"", 
        db_path = ""my_db"", 
        detector_backend = backends[1]
)

#embeddings
embedding_objs = deepface.represent(img_path = ""img.jpg"", 
        detector_backend = backends[2]
)

#facial analysis
demographies = deepface.analyze(img_path = ""img4.jpg"", 
        detector_backend = backends[3]
)

#face detection and alignment
face_objs = deepface.extract_faces(img_path = ""img.jpg"", 
        target_size = (224, 224), 
        detector_backend = backends[4]
)
```

face recognition models are actually cnn models and they expect standard sized inputs. so, resizing is required before representation. to avoid deformation, deepface adds black padding pixels according to the target size argument after detection and alignment.

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-detectors-v3.jpg"" width=""90%"" height=""90%""></p>

[retinaface](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) and [mtcnn](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) seem to overperform in detection and alignment stages but they are much slower. if the speed of your pipeline is more important, then you should use opencv or ssd. on the other hand, if you consider the accuracy, then you should use retinaface or mtcnn.

the performance of retinaface is very satisfactory even in the crowd as seen in the following illustration. besides, it comes with an incredible facial landmark detection performance. highlighted red points show some facial landmarks such as eyes, nose and mouth. that's why, alignment score of retinaface is high as well.

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/retinaface-results.jpeg"" width=""90%"" height=""90%"">
<br><em>the yellow angels - fenerbahce women's volleyball team</em>
</p>

you can find out more about retinaface on this [repo](https://github.com/serengil/retinaface).

**real time analysis** - [`demo`](https://youtu.be/-c9ssjcx6wi)

you can run deepface for real time videos as well. stream function will access your webcam and apply both face recognition and facial attribute analysis. the function starts to analyze a frame if it can focus a face sequentially 5 frames. then, it shows results 5 seconds.

```python
deepface.stream(db_path = ""c:/user/sefik/desktop/database"")
```

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg"" width=""90%"" height=""90%""></p>

even though face recognition is based on one-shot learning, you can use multiple face pictures of a person as well. you should rearrange your directory structure as illustrated below.

```bash
user
‚îú‚îÄ‚îÄ database
‚îÇ   ‚îú‚îÄ‚îÄ alice
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alice1.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alice2.jpg
‚îÇ   ‚îú‚îÄ‚îÄ bob
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bob.jpg
```

**api** - [`demo`](https://youtu.be/hekcq6u9xmi)

deepface serves an api as well. you can clone [`/api`](https://github.com/serengil/deepface/tree/master/api) folder and run the api via gunicorn server. this will get a rest service up. in this way, you can call deepface from an external system such as mobile app or web.

```shell
cd scripts
./service.sh
```

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-api.jpg"" width=""90%"" height=""90%""></p>

face recognition, facial attribute analysis and vector representation functions are covered in the api. you are expected to call these functions as http post methods. default service endpoints will be `http://localhost:5000/verify` for face recognition, `http://localhost:5000/analyze` for facial attribute analysis, and `http://localhost:5000/represent` for vector representation. you can pass input images as exact image paths on your environment, base64 encoded strings or images on web. [here](https://github.com/serengil/deepface/tree/master/api), you can find a postman project to find out how these methods should be called.

**dockerized service**

you can deploy the deepface api on a kubernetes cluster with docker. the following [shell script](https://github.com/serengil/deepface/blob/master/scripts/dockerize.sh) will serve deepface on `localhost:5000`. you need to re-configure the [dockerfile](https://github.com/serengil/deepface/blob/master/dockerfile) if you want to change the port. then, even if you do not have a development environment, you will be able to consume deepface services such as verify and analyze. you can also access the inside of the docker image to run deepface related commands. please follow the instructions in the [shell script](https://github.com/serengil/deepface/blob/master/scripts/dockerize.sh).

```shell
cd scripts
./dockerize.sh
```

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-dockerized-v2.jpg"" width=""50%"" height=""50%""></p>

**command line interface**

deepface comes with a command line interface as well. you are able to access its functions in command line as shown below. the command deepface expects the function name as 1st argument and function arguments thereafter.

```shell
#face verification
$ deepface verify -img1_path tests/dataset/img1.jpg -img2_path tests/dataset/img2.jpg

#facial analysis
$ deepface analyze -img_path tests/dataset/img1.jpg
```

you can also run these commands if you are running deepface with docker. please follow the instructions in the [shell script](https://github.com/serengil/deepface/blob/master/scripts/dockerize.sh#l17).

**tech stack** - [`vlog`](https://youtu.be/r8fhsl7u3ee), [`tutorial`](https://sefiks.com/2021/03/31/tech-stack-recommendations-for-face-recognition/)

face recognition models represent facial images as vector embeddings. the idea behind facial recognition is that vectors should be more similar for same person than different persons. the question is that where and how to store facial embeddings in a large scale system. tech stack is vast to store vector embeddings. to determine the right tool, you should consider your task such as face verification or face recognition, priority such as speed or confidence, and also data size.

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/tech-stack-v2.jpg"" width=""90%"" height=""90%""></p>

## derived applications

you can use deepface not just for facial recognition tasks. it's very common to use deepface for entertainment purposes. for instance, celebrity look-alike prediction and parental look-alike prediction tasks can be done with deepface!

**parental look-alike prediction** - [`vlog`](https://youtu.be/nza4tmi9vhe), [`tutorial`](https://sefiks.com/2022/12/22/decide-whom-your-child-looks-like-with-facial-recognition-mommy-or-daddy/)

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/parental-look-alike-v2.jpg"" width=""90%"" height=""90%""></p>

**celebrity look-alike prediction** - [`vlog`](https://youtu.be/jaxken-kieo), [`tutorial`](https://sefiks.com/2019/05/05/celebrity-look-alike-face-recognition-with-deep-learning-in-keras/)

<p align=""center""><img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/look-alike-v3.jpg"" width=""90%"" height=""90%""></p>

## contribution [![tests](https://github.com/serengil/deepface/actions/workflows/tests.yml/badge.svg)](https://github.com/serengil/deepface/actions/workflows/tests.yml)

pull requests are more than welcome! you should run the unit tests locally by running [`test/unit_tests.py`](https://github.com/serengil/deepface/blob/master/tests/unit_tests.py) before creating a pr. once a pr sent, github test workflow will be run automatically and unit test results will be available in [github actions](https://github.com/serengil/deepface/actions) before approval. besides, workflow will evaluate the code with pylint as well.

## support

there are many ways to support a project - starring‚≠êÔ∏è the github repo is just one üôè

you can also support this work on [patreon](https://www.patreon.com/serengil?repo=deepface) or [github sponsors](https://github.com/sponsors/serengil).

<a href=""https://www.patreon.com/serengil?repo=deepface"">
<img src=""https://raw.githubusercontent.com/serengil/deepface/master/icon/patreon.png"" width=""30%"" height=""30%"">
</a>

## citation

please cite deepface in your publications if it helps your research. here are its bibtex entries:

if you use deepface for facial recogntion purposes, please cite the this publication.

```bibtex
@inproceedings{serengil2020lightface,
  title        = {lightface: a hybrid deep face recognition framework},
  author       = {serengil, sefik ilkin and ozpinar, alper},
  booktitle    = {2020 innovations in intelligent systems and applications conference (asyu)},
  pages        = {23-27},
  year         = {2020},
  doi          = {10.1109/asyu50717.2020.9259802},
  url          = {https://doi.org/10.1109/asyu50717.2020.9259802},
  organization = {ieee}
}
```

if you use deepface for facial attribute analysis purposes such as age, gender, emotion or ethnicity prediction, please cite the this publication.

```bibtex
@inproceedings{serengil2021lightface,
  title        = {hyperextended lightface: a facial attribute analysis framework},
  author       = {serengil, sefik ilkin and ozpinar, alper},
  booktitle    = {2021 international conference on engineering and emerging technologies (iceet)},
  pages        = {1-4},
  year         = {2021},
  doi          = {10.1109/iceet53442.2021.9659697},
  url          = {https://doi.org/10.1109/iceet53442.2021.9659697},
  organization = {ieee}
}
```

also, if you use deepface in your github projects, please add deepface in the `requirements.txt`.

## licence

deepface is licensed under the mit license - see [`license`](https://github.com/serengil/deepface/blob/master/license) for more details. however, the library wraps some external face recognition models: [vgg-face](http://www.robots.ox.ac.uk/~vgg/software/vgg_face/), [facenet](https://github.com/davidsandberg/facenet/blob/master/license.md), [openface](https://github.com/iwantooxxoox/keras-openface/blob/master/license), [deepface](https://github.com/swghosh/deepface), [deepid](https://github.com/ruoyiran/deepid/blob/master/license.md), [arcface](https://github.com/leondgarse/keras_insightface/blob/master/license), [dlib](https://github.com/davisking/dlib/blob/master/dlib/license.txt), and [sface](https://github.com/opencv/opencv_zoo/blob/master/models/face_recognition_sface/license). besides, age, gender and race / ethnicity models are based on vgg-face. licence types will be inherited if you are going to use those models. please check the license types of those models for production purposes.

deepface [logo](https://thenounproject.com/term/face-recognition/2965879/) is created by [adrien coquet](https://thenounproject.com/coquet_adrien/) and it is licensed under [creative commons: by attribution 3.0 license](https://creativecommons.org/licenses/by/3.0/).
"
"Detectron","**detectron is deprecated. please see [detectron2](https://github.com/facebookresearch/detectron2), a ground-up rewrite of detectron in pytorch.**

# detectron

detectron is facebook ai research's software system that implements state-of-the-art object detection algorithms, including [mask r-cnn](https://arxiv.org/abs/1703.06870). it is written in python and powered by the [caffe2](https://github.com/caffe2/caffe2) deep learning framework.

at fair, detectron has enabled numerous research projects, including: [feature pyramid networks for object detection](https://arxiv.org/abs/1612.03144), [mask r-cnn](https://arxiv.org/abs/1703.06870), [detecting and recognizing human-object interactions](https://arxiv.org/abs/1704.07333), [focal loss for dense object detection](https://arxiv.org/abs/1708.02002), [non-local neural networks](https://arxiv.org/abs/1711.07971), [learning to segment every thing](https://arxiv.org/abs/1711.10370), [data distillation: towards omni-supervised learning](https://arxiv.org/abs/1712.04440), [densepose: dense human pose estimation in the wild](https://arxiv.org/abs/1802.00434), and [group normalization](https://arxiv.org/abs/1803.08494).

<div align=""center"">
  <img src=""demo/output/33823288584_1d21cf0a26_k_example_output.jpg"" width=""700px"" />
  <p>example mask r-cnn output.</p>
</div>

## introduction

the goal of detectron is to provide a high-quality, high-performance
codebase for object detection *research*. it is designed to be flexible in order
to support rapid implementation and evaluation of novel research. detectron
includes implementations of the following object detection algorithms:

- [mask r-cnn](https://arxiv.org/abs/1703.06870) -- *marr prize at iccv 2017*
- [retinanet](https://arxiv.org/abs/1708.02002) -- *best student paper award at iccv 2017*
- [faster r-cnn](https://arxiv.org/abs/1506.01497)
- [rpn](https://arxiv.org/abs/1506.01497)
- [fast r-cnn](https://arxiv.org/abs/1504.08083)
- [r-fcn](https://arxiv.org/abs/1605.06409)

using the following backbone network architectures:

- [resnext{50,101,152}](https://arxiv.org/abs/1611.05431)
- [resnet{50,101,152}](https://arxiv.org/abs/1512.03385)
- [feature pyramid networks](https://arxiv.org/abs/1612.03144) (with resnet/resnext)
- [vgg16](https://arxiv.org/abs/1409.1556)

additional backbone architectures may be easily implemented. for more details about these models, please see [references](#references) below.

## update

- 4/2018: support group normalization - see [`gn/readme.md`](./projects/gn/readme.md)

## license

detectron is released under the [apache 2.0 license](https://github.com/facebookresearch/detectron/blob/master/license). see the [notice](https://github.com/facebookresearch/detectron/blob/master/notice) file for additional details.

## citing detectron

if you use detectron in your research or wish to refer to the baseline results published in the [model zoo](model_zoo.md), please use the following bibtex entry.

```
@misc{detectron2018,
  author =       {ross girshick and ilija radosavovic and georgia gkioxari and
                  piotr doll\'{a}r and kaiming he},
  title =        {detectron},
  howpublished = {\url{https://github.com/facebookresearch/detectron}},
  year =         {2018}
}
```

## model zoo and baselines

we provide a large set of baseline results and trained models available for download in the [detectron model zoo](model_zoo.md).

## installation

please find installation instructions for caffe2 and detectron in [`install.md`](install.md).

## quick start: using detectron

after installation, please see [`getting_started.md`](getting_started.md) for brief tutorials covering inference and training with detectron.

## getting help

to start, please check the [troubleshooting](install.md#troubleshooting) section of our installation instructions as well as our [faq](faq.md). if you couldn't find help there, try searching our github issues. we intend the issues page to be a forum in which the community collectively troubleshoots problems.

if bugs are found, **we appreciate pull requests** (including adding q&a's to `faq.md` and improving our installation instructions and troubleshooting documents). please see [contributing.md](contributing.md) for more information about contributing to detectron.

## references

- [data distillation: towards omni-supervised learning](https://arxiv.org/abs/1712.04440).
  ilija radosavovic, piotr doll√°r, ross girshick, georgia gkioxari, and kaiming he.
  tech report, arxiv, dec. 2017.
- [learning to segment every thing](https://arxiv.org/abs/1711.10370).
  ronghang hu, piotr doll√°r, kaiming he, trevor darrell, and ross girshick.
  tech report, arxiv, nov. 2017.
- [non-local neural networks](https://arxiv.org/abs/1711.07971).
  xiaolong wang, ross girshick, abhinav gupta, and kaiming he.
  tech report, arxiv, nov. 2017.
- [mask r-cnn](https://arxiv.org/abs/1703.06870).
  kaiming he, georgia gkioxari, piotr doll√°r, and ross girshick.
  ieee international conference on computer vision (iccv), 2017.
- [focal loss for dense object detection](https://arxiv.org/abs/1708.02002).
  tsung-yi lin, priya goyal, ross girshick, kaiming he, and piotr doll√°r.
  ieee international conference on computer vision (iccv), 2017.
- [accurate, large minibatch sgd: training imagenet in 1 hour](https://arxiv.org/abs/1706.02677).
  priya goyal, piotr doll√°r, ross girshick, pieter noordhuis, lukasz wesolowski, aapo kyrola, andrew tulloch, yangqing jia, and kaiming he.
  tech report, arxiv, june 2017.
- [detecting and recognizing human-object interactions](https://arxiv.org/abs/1704.07333).
  georgia gkioxari, ross girshick, piotr doll√°r, and kaiming he.
  tech report, arxiv, apr. 2017.
- [feature pyramid networks for object detection](https://arxiv.org/abs/1612.03144).
  tsung-yi lin, piotr doll√°r, ross girshick, kaiming he, bharath hariharan, and serge belongie.
  ieee conference on computer vision and pattern recognition (cvpr), 2017.
- [aggregated residual transformations for deep neural networks](https://arxiv.org/abs/1611.05431).
  saining xie, ross girshick, piotr doll√°r, zhuowen tu, and kaiming he.
  ieee conference on computer vision and pattern recognition (cvpr), 2017.
- [r-fcn: object detection via region-based fully convolutional networks](http://arxiv.org/abs/1605.06409).
  jifeng dai, yi li, kaiming he, and jian sun.
  conference on neural information processing systems (nips), 2016.
- [deep residual learning for image recognition](http://arxiv.org/abs/1512.03385).
  kaiming he, xiangyu zhang, shaoqing ren, and jian sun.
  ieee conference on computer vision and pattern recognition (cvpr), 2016.
- [faster r-cnn: towards real-time object detection with region proposal networks](http://arxiv.org/abs/1506.01497)
  shaoqing ren, kaiming he, ross girshick, and jian sun.
  conference on neural information processing systems (nips), 2015.
- [fast r-cnn](http://arxiv.org/abs/1504.08083).
  ross girshick.
  ieee international conference on computer vision (iccv), 2015.
"
"Self-supervised learning","<div align=""center"">

<img src=""docs/source/_images/logos/bolts_logo.png"" width=""400px"">

**deep learning components for extending pytorch lightning**

______________________________________________________________________

<p align=""center"">
  <a href=""#install"">installation</a> ‚Ä¢
  <a href=""https://lightning-bolts.readthedocs.io/en/latest/"">latest docs</a> ‚Ä¢
  <a href=""https://lightning-bolts.readthedocs.io/en/stable/"">stable docs</a> ‚Ä¢
  <a href=""#what-is-bolts"">about</a> ‚Ä¢
  <a href=""#team"">community</a> ‚Ä¢
  <a href=""https://www.pytorchlightning.ai/"">website</a> ‚Ä¢
  <a href=""https://www.grid.ai/"">grid ai</a> ‚Ä¢
  <a href=""#license"">license</a>
</p>

[![pypi status](https://badge.fury.io/py/lightning-bolts.svg)](https://badge.fury.io/py/lightning-bolts)
[![pypi status](https://pepy.tech/badge/lightning-bolts)](https://pepy.tech/project/lightning-bolts)
[![build status](https://dev.azure.com/lightning-ai/lightning%20bolts/_apis/build/status/lightning-ai.lightning-bolts?branchname=master)](https://dev.azure.com/lightning-ai/lightning%20bolts/_build?definitionid=31&_a=summary&repositoryfilter=13&branchfilter=4923%2c4923)
[![codecov](https://codecov.io/gh/lightning-ai/lightning-bolts/branch/master/graph/badge.svg?token=o8p0qhvj90)](https://codecov.io/gh/lightning-ai/lightning-bolts)

[![documentation status](https://readthedocs.org/projects/lightning-bolts/badge/?version=latest)](https://lightning-bolts.readthedocs.io/en/latest/)
[![slack](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://www.pytorchlightning.ai/community)
[![license](https://img.shields.io/badge/license-apache%202.0-blue.svg)](https://github.com/pytorchlightning/lightning-bolts/blob/master/license)
[![doi](https://zenodo.org/badge/250025410.svg)](https://zenodo.org/badge/latestdoi/250025410)

</div>

______________________________________________________________________

## getting started

pip / conda

```bash
pip install lightning-bolts
```

<details>
  <summary>other installations</summary>

install bleeding-edge (no guarantees)

```bash
pip install git+https://github.com/pytorchlightning/lightning-bolts.git@master --upgrade
```

to install all optional dependencies

```bash
pip install lightning-bolts[""extra""]
```

</details>

## what is bolts

bolts provides a variety of components to extend pytorch lightning such as callbacks & datasets, for applied research and production.

## news

- sept 22: [leverage sparsity for faster inference with lightning flash and sparseml](https://devblog.pytorchlightning.ai/leverage-sparsity-for-faster-inference-with-lightning-flash-and-sparseml-cdda1165622b)
- aug 26: [fine-tune transformers faster with lightning flash and torch ort](https://devblog.pytorchlightning.ai/fine-tune-transformers-faster-with-lightning-flash-and-torch-ort-ec2d53789dc3)

#### example 1: accelerate lightning training with the torch ort callback

torch ort converts your model into an optimized onnx graph, speeding up training & inference when using nvidia or amd gpus. see the [documentation](https://lightning-bolts.readthedocs.io/en/latest/callbacks/torch_ort.html) for more details.

```python
from pytorch_lightning import lightningmodule, trainer
import torchvision.models as models
from pl_bolts.callbacks import ortcallback


class visionmodel(lightningmodule):
    def __init__(self):
        super().__init__()
        self.model = models.vgg19_bn(pretrained=true)

    ...


model = visionmodel()
trainer = trainer(gpus=1, callbacks=ortcallback())
trainer.fit(model)
```

#### example 2: introduce sparsity with the sparsemlcallback to accelerate inference

we can introduce sparsity during fine-tuning with [sparseml](https://github.com/neuralmagic/sparseml), which ultimately allows us to leverage the [deepsparse](https://github.com/neuralmagic/deepsparse) engine to see performance improvements at inference time.

```python
from pytorch_lightning import lightningmodule, trainer
import torchvision.models as models
from pl_bolts.callbacks import sparsemlcallback


class visionmodel(lightningmodule):
    def __init__(self):
        super().__init__()
        self.model = models.vgg19_bn(pretrained=true)

    ...


model = visionmodel()
trainer = trainer(gpus=1, callbacks=sparsemlcallback(recipe_path=""recipe.yaml""))
trainer.fit(model)
```

## are specific research implementations supported?

we'd like to encourage users to contribute general components that will help a broad range of problems, however components that help specifics domains will also be welcomed!

for example a callback to help train ssl models would be a great contribution, however the next greatest ssl model from your latest paper would be a good contribution to [lightning flash](https://github.com/pytorchlightning/lightning-flash).

use [lightning flash](https://github.com/pytorchlightning/lightning-flash) to train, predict and serve state-of-the-art models for applied research. we suggest looking at our [vissl](https://lightning-flash.readthedocs.io/en/latest/integrations/vissl.html) flash integration for ssl based tasks.

## contribute!

bolts is supported by the pytorch lightning team and the pytorch lightning community!

join our slack and/or read our [contributing](./.github/contributing.md) guidelines to get help becoming a contributor!

______________________________________________________________________

## license

please observe the apache 2.0 license that is listed in this repository.
in addition the lightning framework is patent pending.
"
"Deep High-Resolution-Net","# deep high-resolution representation learning for human pose estimation (cvpr 2019)
## news
- [2021/04/12] welcome to check out our recent work on bottom-up pose estimation (cvpr 2021) [hrnet-dekr](https://github.com/hrnet/dekr)!
- [2020/07/05] [a very nice blog](https://towardsdatascience.com/overview-of-human-pose-estimation-neural-networks-hrnet-higherhrnet-architectures-and-faq-1954b2f8b249) from towards data science introducing hrnet and higherhrnet for human pose estimation.
- [2020/03/13] a longer version is accepted by tpami: [deep high-resolution representation learning for visual recognition](https://arxiv.org/pdf/1908.07919.pdf). it includes more hrnet applications, and the codes are available: [semantic segmentation](https://github.com/hrnet/hrnet-semantic-segmentation),  [objection detection](https://github.com/hrnet/hrnet-object-detection),  [facial landmark detection](https://github.com/hrnet/hrnet-facial-landmark-detection), and [image classification](https://github.com/hrnet/hrnet-image-classification).
- [2020/02/01] we have added demo code for hrnet. thanks [alex simes](https://github.com/alex9311). 
- visualization code for showing the pose estimation results. thanks depu!
- [2019/08/27] higherhrnet is now on [arxiv](https://arxiv.org/abs/1908.10357), which is a bottom-up approach for human pose estimation powerd by hrnet. we will also release code and models at [higher-hrnet-human-pose-estimation](https://github.com/hrnet/higher-hrnet-human-pose-estimation), stay tuned!
- our new work [high-resolution representations for labeling pixels and regions](https://arxiv.org/abs/1904.04514) is available at [hrnet](https://github.com/hrnet). our hrnet has been applied to a wide range of vision tasks, such as [image classification](https://github.com/hrnet/hrnet-image-classification), [objection detection](https://github.com/hrnet/hrnet-object-detection), [semantic segmentation](https://github.com/hrnet/hrnet-semantic-segmentation) and [facial landmark](https://github.com/hrnet/hrnet-facial-landmark-detection).

## introduction
this is an official pytorch implementation of [*deep high-resolution representation learning for human pose estimation*](https://arxiv.org/abs/1902.09212). 
in this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. most existing methods **recover high-resolution representations from low-resolution representations** produced by a high-to-low resolution network. instead, our proposed network **maintains high-resolution representations** through the whole process.
we start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks **in parallel**. we conduct **repeated multi-scale fusions** such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. as a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. we empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the coco keypoint detection dataset and the mpii human pose dataset. </br>

![illustrating the architecture of the proposed hrnet](/figures/hrnet.png)
## main results
### results on mpii val
| arch               | head | shoulder | elbow | wrist |  hip | knee | ankle | mean | mean@0.1 |
|--------------------|------|----------|-------|-------|------|------|-------|------|----------|
| pose_resnet_50     | 96.4 |     95.3 |  89.0 |  83.2 | 88.4 | 84.0 |  79.6 | 88.5 |     34.0 |
| pose_resnet_101    | 96.9 |     95.9 |  89.5 |  84.4 | 88.4 | 84.5 |  80.7 | 89.1 |     34.0 |
| pose_resnet_152    | 97.0 |     95.9 |  90.0 |  85.0 | 89.2 | 85.3 |  81.3 | 89.6 |     35.0 |
| **pose_hrnet_w32** | 97.1 |     95.9 |  90.3 |  86.4 | 89.1 | 87.1 |  83.3 | 90.3 |     37.7 |

### note:
- flip test is used.
- input size is 256x256
- pose_resnet_[50,101,152] is our previous work of [*simple baselines for human pose estimation and tracking*](http://openaccess.thecvf.com/content_eccv_2018/html/bin_xiao_simple_baselines_for_eccv_2018_paper.html)

### results on coco val2017 with detector having human ap of 56.4 on coco val2017 dataset
| arch               | input size | #params | gflops |    ap | ap .5 | ap .75 | ap (m) | ap (l) |    ar | ar .5 | ar .75 | ar (m) | ar (l) |
|--------------------|------------|---------|--------|-------|-------|--------|--------|--------|-------|-------|--------|--------|--------|
| pose_resnet_50     |    256x192 | 34.0m   |    8.9 | 0.704 | 0.886 |  0.783 |  0.671 |  0.772 | 0.763 | 0.929 |  0.834 |  0.721 |  0.824 |
| pose_resnet_50     |    384x288 | 34.0m   |   20.0 | 0.722 | 0.893 |  0.789 |  0.681 |  0.797 | 0.776 | 0.932 |  0.838 |  0.728 |  0.846 |
| pose_resnet_101    |    256x192 | 53.0m   |   12.4 | 0.714 | 0.893 |  0.793 |  0.681 |  0.781 | 0.771 | 0.934 |  0.840 |  0.730 |  0.832 |
| pose_resnet_101    |    384x288 | 53.0m   |   27.9 | 0.736 | 0.896 |  0.803 |  0.699 |  0.811 | 0.791 | 0.936 |  0.851 |  0.745 |  0.858 |
| pose_resnet_152    |    256x192 | 68.6m   |   15.7 | 0.720 | 0.893 |  0.798 |  0.687 |  0.789 | 0.778 | 0.934 |  0.846 |  0.736 |  0.839 |
| pose_resnet_152    |    384x288 | 68.6m   |   35.3 | 0.743 | 0.896 |  0.811 |  0.705 |  0.816 | 0.797 | 0.937 |  0.858 |  0.751 |  0.863 |
| **pose_hrnet_w32** |    256x192 | 28.5m   |    7.1 | 0.744 | 0.905 |  0.819 |  0.708 |  0.810 | 0.798 | 0.942 |  0.865 |  0.757 |  0.858 |
| **pose_hrnet_w32** |    384x288 | 28.5m   |   16.0 | 0.758 | 0.906 |  0.825 |  0.720 |  0.827 | 0.809 | 0.943 |  0.869 |  0.767 |  0.871 |
| **pose_hrnet_w48** |    256x192 | 63.6m   |   14.6 | 0.751 | 0.906 |  0.822 |  0.715 |  0.818 | 0.804 | 0.943 |  0.867 |  0.762 |  0.864 |
| **pose_hrnet_w48** |    384x288 | 63.6m   |   32.9 | 0.763 | 0.908 |  0.829 |  0.723 |  0.834 | 0.812 | 0.942 |  0.871 |  0.767 |  0.876 |

### note:
- flip test is used.
- person detector has person ap of 56.4 on coco val2017 dataset.
- pose_resnet_[50,101,152] is our previous work of [*simple baselines for human pose estimation and tracking*](http://openaccess.thecvf.com/content_eccv_2018/html/bin_xiao_simple_baselines_for_eccv_2018_paper.html).
- gflops is for convolution and linear layers only.


### results on coco test-dev2017 with detector having human ap of 60.9 on coco test-dev2017 dataset
| arch               | input size | #params | gflops |    ap | ap .5 | ap .75 | ap (m) | ap (l) |    ar | ar .5 | ar .75 | ar (m) | ar (l) |
|--------------------|------------|---------|--------|-------|-------|--------|--------|--------|-------|-------|--------|--------|--------|
| pose_resnet_152    |    384x288 | 68.6m   |   35.3 | 0.737 | 0.919 |  0.828 |  0.713 |  0.800 | 0.790 | 0.952 |  0.856 |  0.748 |  0.849 |
| **pose_hrnet_w48** |    384x288 | 63.6m   |   32.9 | 0.755 | 0.925 |  0.833 |  0.719 |  0.815 | 0.805 | 0.957 |  0.874 |  0.763 |  0.863 |
| **pose_hrnet_w48\*** |    384x288 | 63.6m   |   32.9 | 0.770 | 0.927 |  0.845 |  0.734 |  0.831 | 0.820 | 0.960 |  0.886 |  0.778 |  0.877 |

### note:
- flip test is used.
- person detector has person ap of 60.9 on coco test-dev2017 dataset.
- pose_resnet_152 is our previous work of [*simple baselines for human pose estimation and tracking*](http://openaccess.thecvf.com/content_eccv_2018/html/bin_xiao_simple_baselines_for_eccv_2018_paper.html).
- gflops is for convolution and linear layers only.
- pose_hrnet_w48\* means using additional data from [ai challenger](https://challenger.ai/dataset/keypoint) for training.

## environment
the code is developed using python 3.6 on ubuntu 16.04. nvidia gpus are needed. the code is developed and tested using 4 nvidia p100 gpu cards. other platforms or gpu cards are not fully tested.

## quick start
### installation
1. install pytorch >= v1.0.0 following [official instruction](https://pytorch.org/).
   **note that if you use pytorch's version < v1.0.0, you should following the instruction at <https://github.com/microsoft/human-pose-estimation.pytorch> to disable cudnn's implementations of batchnorm layer. we encourage you to use higher pytorch's version(>=v1.0.0)**
2. clone this repo, and we'll call the directory that you cloned as ${pose_root}.
3. install dependencies:
   ```
   pip install -r requirements.txt
   ```
4. make libs:
   ```
   cd ${pose_root}/lib
   make
   ```
5. install [cocoapi](https://github.com/cocodataset/cocoapi):
   ```
   # cocoapi=/path/to/clone/cocoapi
   git clone https://github.com/cocodataset/cocoapi.git $cocoapi
   cd $cocoapi/pythonapi
   # install into global site-packages
   make install
   # alternatively, if you do not have permissions or prefer
   # not to install the coco api into global site-packages
   python3 setup.py install --user
   ```
   note that instructions like # cocoapi=/path/to/install/cocoapi indicate that you should pick a path where you'd like to have the software cloned and then set an environment variable (cocoapi in this case) accordingly.
4. init output(training model output directory) and log(tensorboard log directory) directory:

   ```
   mkdir output 
   mkdir log
   ```

   your directory tree should look like this:

   ```
   ${pose_root}
   ‚îú‚îÄ‚îÄ data
   ‚îú‚îÄ‚îÄ experiments
   ‚îú‚îÄ‚îÄ lib
   ‚îú‚îÄ‚îÄ log
   ‚îú‚îÄ‚îÄ models
   ‚îú‚îÄ‚îÄ output
   ‚îú‚îÄ‚îÄ tools 
   ‚îú‚îÄ‚îÄ readme.md
   ‚îî‚îÄ‚îÄ requirements.txt
   ```

6. download pretrained models from our model zoo([googledrive](https://drive.google.com/drive/folders/1hotihvbyixsm5ygdpbuuj7o_tzv4oxjc?usp=sharing) or [onedrive](https://1drv.ms/f/s!ahixjn_j-blw231mh2krnmlq5kkq))
   ```
   ${pose_root}
    `-- models
        `-- pytorch
            |-- imagenet
            |   |-- hrnet_w32-36af842e.pth
            |   |-- hrnet_w48-8ef0771d.pth
            |   |-- resnet50-19c8e357.pth
            |   |-- resnet101-5d3b4d8f.pth
            |   `-- resnet152-b121ed2d.pth
            |-- pose_coco
            |   |-- pose_hrnet_w32_256x192.pth
            |   |-- pose_hrnet_w32_384x288.pth
            |   |-- pose_hrnet_w48_256x192.pth
            |   |-- pose_hrnet_w48_384x288.pth
            |   |-- pose_resnet_101_256x192.pth
            |   |-- pose_resnet_101_384x288.pth
            |   |-- pose_resnet_152_256x192.pth
            |   |-- pose_resnet_152_384x288.pth
            |   |-- pose_resnet_50_256x192.pth
            |   `-- pose_resnet_50_384x288.pth
            `-- pose_mpii
                |-- pose_hrnet_w32_256x256.pth
                |-- pose_hrnet_w48_256x256.pth
                |-- pose_resnet_101_256x256.pth
                |-- pose_resnet_152_256x256.pth
                `-- pose_resnet_50_256x256.pth

   ```
   
### data preparation
**for mpii data**, please download from [mpii human pose dataset](http://human-pose.mpi-inf.mpg.de/). the original annotation files are in matlab format. we have converted them into json format, you also need to download them from [onedrive](https://1drv.ms/f/s!ahixjn_j-blw00sqrairnetmevu4) or [googledrive](https://drive.google.com/drive/folders/1en_vqmstnsxmdldxa6qpqeydqulnms3a?usp=sharing).
extract them under {pose_root}/data, and make them look like this:
```
${pose_root}
|-- data
`-- |-- mpii
    `-- |-- annot
        |   |-- gt_valid.mat
        |   |-- test.json
        |   |-- train.json
        |   |-- trainval.json
        |   `-- valid.json
        `-- images
            |-- 000001163.jpg
            |-- 000003072.jpg
```

**for coco data**, please download from [coco download](http://cocodataset.org/#download), 2017 train/val is needed for coco keypoints training and validation. we also provide person detection result of coco val2017 and test-dev2017 to reproduce our multi-person pose estimation results. please download from [onedrive](https://1drv.ms/f/s!ahixjn_j-blwzzdxoz5befl8swm-) or [googledrive](https://drive.google.com/drive/folders/1frudnudxe9fjqcrz2bnf_tkmlo0nb_dk?usp=sharing).
download and extract them under {pose_root}/data, and make them look like this:
```
${pose_root}
|-- data
`-- |-- coco
    `-- |-- annotations
        |   |-- person_keypoints_train2017.json
        |   `-- person_keypoints_val2017.json
        |-- person_detection_results
        |   |-- coco_val2017_detections_ap_h_56_person.json
        |   |-- coco_test-dev2017_detections_ap_h_609_person.json
        `-- images
            |-- train2017
            |   |-- 000000000009.jpg
            |   |-- 000000000025.jpg
            |   |-- 000000000030.jpg
            |   |-- ... 
            `-- val2017
                |-- 000000000139.jpg
                |-- 000000000285.jpg
                |-- 000000000632.jpg
                |-- ... 
```

### training and testing

#### testing on mpii dataset using model zoo's models([googledrive](https://drive.google.com/drive/folders/1hotihvbyixsm5ygdpbuuj7o_tzv4oxjc?usp=sharing) or [onedrive](https://1drv.ms/f/s!ahixjn_j-blw231mh2krnmlq5kkq))
 

```
python tools/test.py \
    --cfg experiments/mpii/hrnet/w32_256x256_adam_lr1e-3.yaml \
    test.model_file models/pytorch/pose_mpii/pose_hrnet_w32_256x256.pth
```

#### training on mpii dataset

```
python tools/train.py \
    --cfg experiments/mpii/hrnet/w32_256x256_adam_lr1e-3.yaml
```

#### testing on coco val2017 dataset using model zoo's models([googledrive](https://drive.google.com/drive/folders/1hotihvbyixsm5ygdpbuuj7o_tzv4oxjc?usp=sharing) or [onedrive](https://1drv.ms/f/s!ahixjn_j-blw231mh2krnmlq5kkq))
 

```
python tools/test.py \
    --cfg experiments/coco/hrnet/w32_256x192_adam_lr1e-3.yaml \
    test.model_file models/pytorch/pose_coco/pose_hrnet_w32_256x192.pth \
    test.use_gt_bbox false
```

#### training on coco train2017 dataset

```
python tools/train.py \
    --cfg experiments/coco/hrnet/w32_256x192_adam_lr1e-3.yaml \
```

### visualization

#### visualizing predictions on coco val

```
python visualization/plot_coco.py \
    --prediction output/coco/w48_384x288_adam_lr1e-3/results/keypoints_val2017_results_0.json \
    --save-path visualization/results

```


<img src=""figures\visualization\coco\score_610_id_2685_000000002685.png"" height=""215""><img src=""figures\visualization\coco\score_710_id_153229_000000153229.png"" height=""215""><img src=""figures\visualization\coco\score_755_id_343561_000000343561.png"" height=""215"">

<img src=""figures\visualization\coco\score_755_id_559842_000000559842.png"" height=""209""><img src=""figures\visualization\coco\score_770_id_6954_000000006954.png"" height=""209""><img src=""figures\visualization\coco\score_919_id_53626_000000053626.png"" height=""209"">

### other applications
many other dense prediction tasks, such as segmentation, face alignment and object detection, etc. have been benefited by hrnet. more information can be found at [high-resolution networks](https://github.com/hrnet).

### other implementation
[mmpose](https://github.com/open-mmlab/mmpose) </br>
[modelscope (‰∏≠ÊñáÔºâ](https://modelscope.cn/models/damo/cv_hrnetv2w32_body-2d-keypoints_image/summary)</br>
[timm](https://huggingface.co/docs/timm/main/en/models/hrnet)


### citation
if you use our code or models in your research, please cite with:
```
@inproceedings{sun2019deep,
  title={deep high-resolution representation learning for human pose estimation},
  author={sun, ke and xiao, bin and liu, dong and wang, jingdong},
  booktitle={cvpr},
  year={2019}
}

@inproceedings{xiao2018simple,
    author={xiao, bin and wu, haiping and wei, yichen},
    title={simple baselines for human pose estimation and tracking},
    booktitle = {european conference on computer vision (eccv)},
    year = {2018}
}

@article{wangscjdzlmtwlx19,
  title={deep high-resolution representation learning for visual recognition},
  author={jingdong wang and ke sun and tianheng cheng and 
          borui jiang and chaorui deng and yang zhao and dong liu and yadong mu and 
          mingkui tan and xinggang wang and wenyu liu and bin xiao},
  journal   = {tpami}
  year={2019}
}
```
"
"Lucent","![](https://github.com/greentfrapp/lucent/raw/master/images/lucent_header.jpg)

# lucent

<!--*it's still magic even if you know how it's done. gnu terry pratchett*-->

[![travis build status](https://img.shields.io/travis/greentfrapp/lucent.svg)](https://travis-ci.org/greentfrapp/lucent)
[![code coverage](https://img.shields.io/coveralls/github/greentfrapp/lucent.svg)](https://coveralls.io/github/greentfrapp/lucent)

*pytorch + lucid = lucent*

the wonderful [lucid](https://github.com/tensorflow/lucid) library adapted for the wonderful pytorch!

**lucent is not affiliated with lucid or openai's clarity team, although we would love to be!**
credit is due to the original lucid authors, we merely adapted the code for pytorch and we take the blame for all issues and bugs found here.

# usage

lucent is still in pre-alpha phase and can be installed locally with the following command:

```
pip install torch-lucent
```

in the spirit of lucid, get up and running with lucent immediately, thanks to google's [colab](https://colab.research.google.com/notebooks/welcome.ipynb)! 

you can also clone this repository and run the notebooks locally with [jupyter](http://jupyter.org/install.html).

## quickstart

```
import torch

from lucent.optvis import render
from lucent.modelzoo import inceptionv1

device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
model = inceptionv1(pretrained=true)
model.to(device).eval()

render.render_vis(model, ""mixed4a:476"")
```

## tutorials

<a href=""https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/tutorial.ipynb"">
<img src=""https://github.com/greentfrapp/lucent-notebooks/raw/master/images/tutorial_card.jpg"" width=""500"" alt=""""></img></a>

<a href=""https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/modelzoo.ipynb""><img src=""https://github.com/greentfrapp/lucent-notebooks/raw/master/images/modelzoo_card.jpg"" width=""500"" alt=""""></img></a>

## other notebooks

here, we have tried to recreate some of the lucid notebooks! you can also check out the [lucent-notebooks](https://github.com/greentfrapp/lucent-notebooks) repo to clone all the notebooks.

<a href=""https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/diversity.ipynb""><img src=""https://github.com/greentfrapp/lucent-notebooks/raw/master/images/diversity_card.jpg"" width=""500"" alt=""""></img></a>

<a href=""https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/neuron_interaction.ipynb""><img src=""https://github.com/greentfrapp/lucent-notebooks/raw/master/images/neuron_interaction_card.jpg"" width=""500"" alt=""""></img></a>

<a href=""https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/feature_inversion.ipynb"">
<img src=""https://github.com/greentfrapp/lucent-notebooks/raw/master/images/feature_inversion_card.jpg"" width=""500"" alt=""""></img>
</a>

<a href=""https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/style_transfer.ipynb"">
<img src=""https://github.com/greentfrapp/lucent-notebooks/raw/master/images/style_transfer_card.jpg"" width=""500"" alt=""""></img>
</a>

<a href=""https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/activation_grids.ipynb"">
<img src=""https://github.com/greentfrapp/lucent-notebooks/raw/master/images/activation_grids_card.jpg"" width=""500"" alt=""""></img>
</a>

# recommended readings

* [feature visualization](https://distill.pub/2017/feature-visualization/)
* [the building blocks of interpretability](https://distill.pub/2018/building-blocks/)
* [using artiÔ¨Åcial intelligence to augment human intelligence](https://distill.pub/2017/aia/)
* [visualizing representations: deep learning and human beings](http://colah.github.io/posts/2015-01-visualizing-representations/)
* [differentiable image parameterizations](https://distill.pub/2018/differentiable-parameterizations/)
* [activation atlas](https://distill.pub/2019/activation-atlas/)

## related talks
* [lessons from a year of distill ml research](https://www.youtube.com/watch?v=jlzsguzaiyy) (shan carter, openvisconf)
* [machine learning for visualization](https://www.youtube.com/watch?v=6n-kcyn0zxu) (ian johnson, openvisconf)

# slack

check out `#proj-lucid` and `#circuits` on the [distill slack](http://slack.distill.pub)!

# additional information

## license and disclaimer

you may use this software under the apache 2.0 license. see [license](https://github.com/greentfrapp/lucent/blob/master/license).
"
"lightly","
![lightly logo](docs/logos/lightly_logo_crop.png)


![github](https://img.shields.io/github/license/lightly-ai/lightly)
![unit tests](https://github.com/lightly-ai/lightly/workflows/unit%20tests/badge.svg)
![codecov](https://codecov.io/gh/lightly-ai/lightly/branch/master/graph/badge.svg?token=1neavrok3w)

lightly is a computer vision framework for self-supervised learning.

> we, at [lightly](https://www.lightly.ai), are passionate engineers who want to make deep learning more efficient. that's why - together with our community - we want to popularize the use of self-supervised methods to understand and curate raw image data. our solution can be applied before any data annotation step and the learned representations can be used to visualize and analyze datasets. this allows to select the best core set of samples for model training through advanced filtering.

- [homepage](https://www.lightly.ai)
- [web-app](https://app.lightly.ai)
- [documentation](https://docs.lightly.ai/self-supervised-learning/)
- [github](https://github.com/lightly-ai/lightly)
- [discord](https://discord.gg/xvnjw94)

### features

lightly offers features like

- modular framework which exposes low-level building blocks such as loss functions
- support for multi-gpu training using pytorch lightning
- easy to use and written in a pytorch like style
- supports custom backbone models for self-supervised pre-training

#### supported models

you can [find sample code for all the supported models here.](https://docs.lightly.ai/self-supervised-learning/examples/models.html)
we provide pytorch, pytorch lightning and pytorch lightning distributed examples for each of the models
to kickstart your project.

some of our supported models:

- [barlow twins, 2021](https://arxiv.org/abs/2103.03230)
- [byol, 2020](https://arxiv.org/abs/2006.07733)
- [dcl & dclw, 2021](https://arxiv.org/abs/2110.06848)
- [dino, 2021](https://arxiv.org/abs/2104.14294)
- [mae, 2021](https://arxiv.org/abs/2111.06377)
- [msn, 2022](https://arxiv.org/abs/2204.07141)
- [moco, 2019](https://arxiv.org/abs/1911.05722)
- [nnclr, 2021](https://arxiv.org/abs/2104.14548)
- [simclr, 2020](https://arxiv.org/abs/2002.05709)
- [simmim, 2021](https://arxiv.org/abs/2111.09886)
- [simsiam, 2021](https://arxiv.org/abs/2011.10566)
- [smog, 2022](https://arxiv.org/abs/2207.06167)
- [swav, 2020](https://arxiv.org/abs/2006.09882)
- [tico, 2022](https://arxiv.org/abs/2206.10698)
- [vicreg, 2022](https://arxiv.org/abs/2105.04906)
- [vicregl, 2022](https://arxiv.org/abs/2210.01571)


### tutorials

want to jump to the tutorials and see lightly in action?

- [train moco on cifar-10](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_moco_memory_bank.html)
- [train simclr on clothing data](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html)
- [train simsiam on satellite images](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simsiam_esa.html)
- [use lightly with custom augmentations](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_custom_augmentations.html)
- [pre-train a detectron2 backbone with lightly](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_pretrain_detectron2.html)

tutorials of using the lightly packge together with the lightly platform:

- [active learning using yolov7 and comma10k](https://docs.lightly.ai/docs/active-learning-yolov7)
- [active learning with the nvidia tlt](https://github.com/lightly-ai/nvidiatltactivelearning)

community and partner projects:

- [on-device deep learning with lightly on an arm microcontroller](https://github.com/arm-software/endpointai/tree/master/proofofconcepts/vision/openmvmaskdefaults)

## quick start

lightly requires **python 3.6+** but we recommend using **python 3.7+**.
we recommend installing lightly in a **linux** or **osx** environment.

### dependencies

- hydra-core>=1.0.0
- numpy>=1.18.1
- pytorch_lightning>=1.5
- requests>=2.23.0
- torchvision
- tqdm

### installation
you can install lightly and its dependencies from pypi with:
```
pip3 install lightly
```

we strongly recommend that you install lightly in a dedicated virtualenv, to avoid conflicting with your system packages.


### lightly in action

with lightly, you can use the latest self-supervised learning methods in a modular
way using the full power of pytorch. experiment with different backbones,
models, and loss functions. the framework has been designed to be easy to use
from the ground up. [find more examples in our docs](https://docs.lightly.ai/self-supervised-learning/examples/models.html).

```python
import torch
import torchvision
import lightly.models as models
import lightly.loss as loss
import lightly.data as data

# the collate function applies random transforms to the input images
collate_fn = data.imagecollatefunction(input_size=32, cj_prob=0.5)

# create a dataset from your image folder
dataset = data.lightlydataset(input_dir='./my/cute/cats/dataset/')

# build a pytorch dataloader
dataloader = torch.utils.data.dataloader(
    dataset,                # pass the dataset to the dataloader
    batch_size=128,         # a large batch size helps with the learning
    shuffle=true,           # shuffling is important!
    collate_fn=collate_fn)  # apply transformations to the input images


# create a pytorch module for the simclr model
class simclr(nn.module):
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        self.projection_head = models.modules.simclrprojectionhead(
          input_dim=512, 
          hidden_dim=512,
          output_dim=128
        )

    def forward(self, x):
        x = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(x)
        return z

# use a resnet backbone
resnet = torchvision.models.resnet18()
backbone = nn.sequential(*list(resnet.children())[:-1])

# build the simclr model
model = simclr(backbone)

# lightly exposes building blocks such as loss functions
criterion = loss.ntxentloss(temperature=0.5)

# get a pytorch optimizer
optimizer = torch.optim.sgd(model.parameters(), lr=1e-0, weight_decay=1e-5)
```

you can easily use another model like simsiam by swapping the model and the
loss function.
```python
# pytorch module for the simsiam model
class simsiam(nn.module):
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        self.projection_head = simsiamprojectionhead(512, 512, 128)
        self.prediction_head = simsiampredictionhead(128, 64, 128)

    def forward(self, x):
        f = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(f)
        p = self.prediction_head(z)
        z = z.detach()
        return z, p

model = simsiam(backbone)

# use the simsiam loss function
criterion = loss.negativecosinesimilarity()
```
you can [find a more complete example for simsiam here.](https://docs.lightly.ai/self-supervised-learning/examples/simsiam.html)


use pytorch lightning to train the model:

```python
trainer = pl.trainer(max_epochs=max_epochs, gpus=1)
trainer.fit(
    model,
    dataloader
)
```

or train the model on 4 gpus:
```python

# use distributed version of loss functions
criterion = ntxentloss(gather_distributed=true)

trainer = pl.trainer(
    max_epochs=max_epochs, 
    gpus=4, 
    distributed_backend='ddp'
)
trainer.fit(
    model,
    dataloader
)
```

we provide proper multi-gpu training with distributed gather and synchronized batchnorm.

[have a look at our docs regarding distributed training](https://docs.lightly.ai/self-supervised-learning/getting_started/distributed_training.html)



### benchmarks

currently implemented models and their accuracy on cifar10 and imagenette. all models have been evaluated using knn. we report the max test accuracy over the epochs as well as the maximum gpu memory consumption. all models in this benchmark use the same augmentations as well as the same resnet-18 backbone. training precision is set to fp32 and sgd is used as an optimizer with cosinelr.
one epoch on cifar10 takes ~35 seconds on a v100 gpu. [learn more about the cifar10 and imagenette benchmark here](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html)

#### imagenette

| model        | epochs | batch size | test accuracy |
|--------------|--------|------------|---------------|
| barlowtwins  |    800 |        256 |         0.789 |
| byol         |    800 |        256 |         0.851 |
| dcl          |    800 |        256 |         0.816 |
| dclw         |    800 |        256 |         0.827 |
| dino (res18) |    800 |        256 |         0.881 |
| moco         |    800 |        256 |         0.832 |
| nnclr        |    800 |        256 |         0.848 |
| simclr       |    800 |        256 |         0.858 |
| simsiam      |    800 |        256 |         0.852 |
| swav         |    800 |        256 |         0.899 |


#### cifar10

| model         | epochs | batch size | test accuracy |
|---------------|--------|------------|---------------|
| barlowtwins   |    800 |        512 |         0.857 |
| byol          |    800 |        512 |         0.911 |
| dcl           |    800 |        512 |         0.873 |
| dclw          |    800 |        512 |         0.873 | 
| dino          |    800 |        512 |         0.884 |
| moco          |    800 |        512 |         0.900 |
| nnclr         |    800 |        512 |         0.896 |
| simclr        |    800 |        512 |         0.875 |
| simsiam       |    800 |        512 |         0.906 |
| swav          |    800 |        512 |         0.881 |

## terminology

below you can see a schematic overview of the different concepts present in the lightly python package. the terms in bold are explained in more detail in our [documentation](https://docs.lightly.ai/self-supervised-learning/).

<img src=""/docs/source/getting_started/images/lightly_overview.png"" alt=""overview of the lightly pip package""/></a>


### next steps
head to the [documentation](https://docs.lightly.ai) and see the things you can achieve with lightly!


## development

to install dev dependencies (for example to contribute to the framework)
you can use the following command:
```
pip3 install -e "".[dev]""
```

for more information about how to contribute have a look [here](contributing.md).

### running tests

unit tests are within the [tests folder](tests/) and we recommend running them using 
[pytest](https://docs.pytest.org/en/stable/).
there are two test configurations available. by default, only a subset will be run.
this is faster and should take less than a minute. you can run it using
```
python -m pytest -s -v
```

to run all tests (including the slow ones) you can use the following command.
```
python -m pytest -s -v --runslow
```

### code linting
we provide a [pylint](https://github.com/pycqa/pylint) config following the
[google python style guide](https://github.com/google/styleguide/blob/gh-pages/pyguide.md).

you can run the linter from your terminal either on a folder
```
pylint lightly/
```
or on a specific file
```
pylint lightly/core.py
```


## further reading

**self-supervised learning:**
- [a simple framework for contrastive learning of visual representations (2020)](https://arxiv.org/abs/2002.05709)
- [momentum contrast for unsupervised visual representation learning (2020)](https://arxiv.org/abs/1911.05722)
- [unsupervised learning of visual features by contrasting cluster assignments (2020)](https://arxiv.org/abs/2006.09882)
- [what should not be contrastive in contrastive learning (2020)](https://arxiv.org/abs/2008.05659)

## faq

- why should i care about self-supervised learning? aren't pre-trained models from imagenet much better for transfer learning?
  - self-supervised learning has become increasingly popular among scientists over the last year because the learned representations perform extraordinarily well on downstream tasks. this means that they capture the important information in an image better than other types of pre-trained models. by training a self-supervised model on *your* dataset, you can make sure that the representations have all the necessary information about your images.

- how can i contribute?
  - create an issue if you encounter bugs or have ideas for features we should implement. you can also add your own code by forking this repository and creating a pr. more details about how to contribute with code is in our [contribution guide](contributing.md).

- is this framework for free?
  - yes, this framework completely free to use and we provide the code. we believe that
  we need to make training deep learning models more data efficient to achieve widespread adoption. one step to achieve this goal is by leveraging self-supervised learning. the company behind lightly commited to keep this framework open-source.

- if this framework is free, how is the company behind lightly making money?
  - training self-supervised models is only one part of our solution. [the company behind lightly](https://lightly.ai/) focuses on processing and analyzing embeddings created by self-supervised models. 
  by building, what we call a self-supervised active learning loop we help companies understand and work with their data more efficiently. this framework acts as an interface
  for our platform to easily upload and download datasets, embeddings and models. whereas 
  the platform will cost for additional features this frameworks will always remain free of charge (even for commercial use).


## lightly in research

- [decoupled contrastive learning](https://arxiv.org/abs/2110.06848)
- [dpcl: constrative representation learning with differential privacy](https://assets.researchsquare.com/files/rs-1516950/v1_covered.pdf?c=1654486158)
- [self-supervised learning methods for label-efficient dental caries classification](https://www.mdpi.com/2075-4418/12/5/1237)
- [solo-learn: a library of self-supervised methods for visual representation learning](https://www.jmlr.org/papers/volume23/21-1155/21-1155.pdf)


## bibtex
if you want to cite the framework feel free to use this:

```bibtex
@article{susmelj2020lightly,
  title={lightly},
  author={igor susmelj and matthias heller and philipp wirth and jeremy prescott and malte ebner et al.},
  journal={github. note: https://github.com/lightly-ai/lightly},
  year={2020}
}
```
"
"IoT Owl","# iot owl
<img alt=""owl"" src=""owl2.png"" width=""200""></img>  
iot owl is light face detection and recognition system made for small iot devices like raspberry pi.

## versions
heavy
- with mask detection
- without mask detection 

more in the future

## how does it work?
heavy version:
1. raspberry pi analyzes every video frame streamed from the camera
2. if program detects faces in the frame collects next 10  frames (in the default config, you can change this value) and choice best one then crop face from the best frame
3. \[optional\] detects mask on the face
4.  sends cropped face to microsoft api to encode face and get information about it (for example: hair color, emotions, whether the mask is put correctly, all available options below)
5.  sends returned token with detected face to recognition person
6.  check information about student in local database by returned from cloud person id

![diagram](diagram.png)



## requirements
minimal for heavy version:  
* python 3 interperter  
* camera (can be wireless)  
* space on disk (this value depends on how many users we want to recognize and how much information about them, we want to store, additionally is highly possible that you have already installed some of this libraries)
	* 1mb - database with information about users
	* 30mb - models if we want to detect persons with masks 
	* 10mb - cvlib
	* 22mb - matplotlib
	* 200mb - opencv
	* 1200mb - tensorflow  
	total: 1463mb
 
 
recommended: 
* internet connection (faster internet = faster face recognition)<br><br>

## benchmarks
```diff
heavy version:
________________________
- download: 150mbps 
+ upload: 140mbps
0.6s ~ 0.9s

________________________
- download: 134mbps 
+ upload: 105mbps
0.7s ~ 1.2s

________________________
- download: 12mbps 
+ upload: 4mbps
1.2s ~ 1.9s
```

## how¬†to¬†use
setup:  
1.¬†you **have to** set¬†in¬†configuration¬†file:
- [microsoft¬†api¬†key](https://azure.microsoft.com/en-us/services/cognitive-services/face/)
- microsoft¬†endpoint¬†links¬†with¬†parameters  
- ip¬†of¬†the¬†camera¬†or¬†number¬†if¬†it's¬†connected¬†directly¬†to¬†pc¬†(default¬†is¬†""0"")  
2. download¬†requirements¬†from¬†""requirements.txt""
3. in¬†main¬†file import:
 - `facedetection.win_face_detection`
 - `os`
 - `sys`

4.¬†add¬†program¬†to¬†path¬†by: `sys.path.append(os.getcwd())`
5.¬†create¬†an¬†object¬†of¬†the¬†class,¬†run¬†""run""¬†function¬†and¬†pass¬†to¬†it¬†function¬†which¬†will¬†be¬†run¬†every¬†time¬†when¬†face¬†will¬†be¬†detected 



## example

```py
import facedetection.ms_face_detection
import os
import sys
sys.path.append(os.getcwd())


def analyzestudent(detected_persons = []):
 print(str(data))

  

def experimental():
 print(""start"")
 test = facedetection.ms_face_detection.apifacedetection()
 test.run(analyzestudent)


if __name__ == ""__main__"":
 experimental()

```

## example output
```json
[
    [
        {
            ""confidence"": 0.5445673,
            ""faceattributes"": {
                ""accessories"": [],
                ""emotion"": {
                    ""anger"": 0.0,
                    ""contempt"": 0.001,
                    ""disgust"": 0.0,
                    ""fear"": 0.0,
                    ""happiness"": 0.902,
                    ""neutral"": 0.098,
                    ""sadness"": 0.0,
                    ""surprise"": 0.0
                },
                ""facialhair"": {
                    ""beard"": 0.1,
                    ""moustache"": 0.1,
                    ""sideburns"": 0.1
                },
                ""glasses"": ""noglasses"",
                ""smile"": 0.902
            },
            ""faceid"": ""0d56aee7-946a-4450-b1b5-563b5266b129"",
            ""facerectangle"": {
                ""height"": 138,
                ""left"": 44,
                ""top"": 60,
                ""width"": 138
            },
            ""recognitionmodel"": ""recognition_01"",
            ""userdata"": ""\""filip\""\""poplewski\""\""3it\""\""10:11:2003\""""
        }
    ]
]
```
```
example greeting:
[""good morning ['filip']""]
````

```json
[
    [
        {
            ""confidence"": 0.78242,
            ""faceattributes"": {
                ""headpose"": {
                    ""pitch"": -2.0,
                    ""roll"": -2.1,
                    ""yaw"": 18.7
                },
                ""mask"": {
                    ""noseandmouthcovered"": false,
                    ""type"": ""othermaskorocclusion""
                }
            },
            ""faceid"": ""66388a5c-ef86-484e-8319-b7010d782a92"",
            ""facerectangle"": {
                ""height"": 202,
                ""left"": 41,
                ""top"": 52,
                ""width"": 148
            },
            ""recognitionmodel"": ""recognition_04""
        }
    ]
]

```
  

## argument¬†passed¬†to¬†given¬†function

apifacedetection¬†will¬†run¬†provided¬†as¬†argument¬†function¬†every¬†time¬†when¬†it¬†detect¬†face¬†at¬†the¬†frame.  
apifacedetection¬†pass¬†json¬†with¬†all¬†collected¬†data¬†about¬†person¬†in¬†the¬†image¬†to¬†given¬†function¬†¬†variable¬†named¬†""detected_persons"".  
examples¬†of¬†all¬†three¬†json's¬†you¬†can¬†find¬†in¬†""response.txt""

## debugging¬†and¬†configuration

if¬†you¬†want¬†to¬†configure¬†face¬†detection¬†to¬†your¬†camera¬†you¬†can¬†run¬†version¬†made¬†for¬†debugging.¬†everything¬†what¬†you¬†need¬†to¬†do¬†is¬†change:  

`import¬†facedetection.ms_face_detection`
to `import¬†facedetection.debug_ms_face_detection`  
  
 and `facedetection.ms_face_detection.apifacedetection()`
to `facedetection.debug_ms_face_detection.apifacedetection()`

it will display window with:
- camera view,
- detected face
- cropped face
- response 
- time to next face recognition
- face quality in percents


## issue?
if¬†you¬†have¬†any¬†questions¬†or¬†you need help in implementation write¬†to me :)  
email:¬†¬†¬†filip.poplewski@protonmail.com 
"
"Pattern","pattern
=======

[![build status](http://img.shields.io/travis/clips/pattern/master.svg?style=flat)](https://travis-ci.org/clips/pattern/branches)
[![coverage](https://img.shields.io/coveralls/clips/pattern/master.svg?style=flat)](https://coveralls.io/github/clips/pattern?branch=master)
[![pypi version](http://img.shields.io/pypi/v/pattern.svg?style=flat)](https://pypi.python.org/pypi/pattern)
[![license](https://img.shields.io/badge/license-bsd%203--clause-green.svg?style=flat)](https://github.com/clips/pattern/blob/master/license.txt)

pattern is a web mining module for python. it has tools for:

 * data mining: web services (google, twitter, wikipedia), web crawler, html dom parser
 * natural language processing: part-of-speech taggers, n-gram search, sentiment analysis, wordnet
 * machine learning: vector space model, clustering, classification (knn, svm, perceptron)
 * network analysis: graph centrality and visualization.

it is well documented, thoroughly tested with 350+ unit tests and comes bundled with 50+ examples. the source code is licensed under bsd.

![example workflow](https://raw.githubusercontent.com/clips/pattern/master/docs/g/pattern_schema.gif)

example
-------

this example trains a classifier on adjectives mined from twitter using python 3. first, tweets that contain hashtag #win or #fail are collected. for example: *""$20 tip off a sweet little old lady today #win""*. the word part-of-speech tags are then parsed, keeping only adjectives. each tweet is transformed to a vector, a dictionary of adjective ‚Üí count items, labeled `win` or `fail`. the classifier uses the vectors to learn which other tweets look more like `win` or more like `fail`.

```python
from pattern.web import twitter
from pattern.en import tag
from pattern.vector import knn, count

twitter, knn = twitter(), knn()

for i in range(1, 3):
    for tweet in twitter.search('#win or #fail', start=i, count=100):
        s = tweet.text.lower()
        p = '#win' in s and 'win' or 'fail'
        v = tag(s)
        v = [word for word, pos in v if pos == 'jj'] # jj = adjective
        v = count(v) # {'sweet': 1}
        if v:
            knn.train(v, type=p)

print(knn.classify('sweet potato burger'))
print(knn.classify('stupid autocorrect'))
```

installation
------------

pattern supports python 2.7 and python 3.6. to install pattern so that it is available in all your scripts, unzip the download and from the command line do:
```bash
cd pattern-3.6
python setup.py install
```

if you have pip, you can automatically download and install from the [pypi repository](https://pypi.python.org/pypi/pattern):
```bash
pip install pattern
```

if none of the above works, you can make python aware of the module in three ways:
- put the pattern folder in the same folder as your script.
- put the pattern folder in the standard location for modules so it is available to all scripts:
  * `c:\python36\lib\site-packages\` (windows),
  * `/library/python/3.6/site-packages/` (mac os x),
  * `/usr/lib/python3.6/site-packages/` (unix).
- add the location of the module to `sys.path` in your script, before importing it:

```python
module = '/users/tom/desktop/pattern'
import sys; if module not in sys.path: sys.path.append(module)
from pattern.en import parsetree
```

documentation
-------------

for documentation and examples see the [user documentation](https://github.com/clips/pattern/wiki).

version
-------

3.6

license
-------

**bsd**, see `license.txt` for further details.

reference
---------

de smedt, t., daelemans, w. (2012). pattern for python. *journal of machine learning research, 13*, 2031‚Äì2035.

contribute
----------

the source code is hosted on github and contributions or donations are welcomed.

bundled dependencies
--------------------

pattern is bundled with the following data sets, algorithms and python packages:

- **brill tagger**, eric brill
- **brill tagger for dutch**, jeroen geertzen
- **brill tagger for german**, gerold schneider & martin volk
- **brill tagger for spanish**, trained on wikicorpus (samuel reese & gemma boleda et al.)
- **brill tagger for french**, trained on lefff (beno√Æt sagot & lionel cl√©ment et al.)
- **brill tagger for italian**, mined from wiktionary
- **english pluralization**, damian conway
- **spanish verb inflection**, fred jehle
- **french verb inflection**, bob salita
- **graph javascript framework**, aslak hellesoy & dave hoover
- **libsvm**, chih-chung chang & chih-jen lin
- **liblinear**, rong-en fan et al.
- **networkx centrality**, aric hagberg, dan schult & pieter swart
- **spelling corrector**, peter norvig

acknowledgements
----------------

**authors:**

- tom de smedt (tom@organisms.be)
- walter daelemans (walter.daelemans@ua.ac.be)

**contributors (chronological):**

- frederik de bleser
- jason wiener
- daniel friesen
- jeroen geertzen
- thomas crombez
- ken williams
- peteris erins
- rajesh nair
- f. de smedt
- radim ≈ôeh≈Ø≈ôek
- tom loredo
- john debovis
- thomas sileo
- gerold schneider
- martin volk
- samuel joseph
- shubhanshu mishra
- robert elwell
- fred jehle
- antoine mazi√®res + fabelier.org
- r√©mi de zoeten + closealert.nl
- kenneth koch
- jens grivolla
- fabio marfia
- steven loria
- colin molter + tevizz.com
- peter bull
- maurizio sambati
- dan fu
- salvatore di dio
- vincent van asch
- frederik elwert
"
"spammy","spammy
======

|pypi version| |build status| |python versions| |percentagecov| |requirements status| |license| 

.. figure:: http://i.imgur.com/w83tsal.png
    :alt:

:author: `tasdik rahman <http://tasdikrahman.me>`__
:latest version: 1.0.3

.. contents::
    :backlinks: none

.. sectnum::


overview
--------

`spammy <https://github.com/tasdikrahman/spammy>`__ : spam filtering at your service

    `spammy <https://github.com/tasdikrahman/spammy>`__ powers the web app https://plino.herokuapp.com

features
--------

- train the classifier on your own dataset to classify your emails into spam or ham
- dead simple to use. see `usage <#example>`__
- blazingly fast once the classifier is trained. (see `benchmarks <#benchmarks>`__)
- custom exceptions raised so that when you miss something, spammy tells you where did you go wrong in a graceful way
- written in uncomplicated ``python``
- built on top of the giant shoulders of `nltk <http://nltk.org>`__

example
-------
`[back to top] <#overview>`__

- your data directory structure should be something similar to

.. code:: bash

    $ tree /home/tasdik/dropbox/projects/spammy/examples/test_dataset
    /home/tasdik/dropbox/projects/spammy/examples/test_dataset
    ‚îú‚îÄ‚îÄ ham
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 5458.2001-04-25.kaminski.ham.txt
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 5459.2001-04-25.kaminski.ham.txt
    ‚îÇ¬†¬† ...
    ‚îÇ¬†¬† ...
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 5851.2001-05-22.kaminski.ham.txt
    ‚îî‚îÄ‚îÄ spam
        ‚îú‚îÄ‚îÄ 4136.2005-07-05.sa_and_hp.spam.txt
        ‚îú‚îÄ‚îÄ 4137.2005-07-05.sa_and_hp.spam.txt
        ...
        ...
        ‚îî‚îÄ‚îÄ 5269.2005-07-19.sa_and_hp.spam.txt


**example**

.. code:: python

    >>> import os
    >>> from spammy import spammy
    >>>
    >>> directory = '/home/tasdik/dropbox/projects/spamfilter/data/corpus3'
    >>>
    >>> # directory structure
    >>> os.listdir(directory)
    ['spam', 'summary.txt', 'ham']
    >>> os.listdir(os.path.join(directory, 'spam'))[:3]
    ['4257.2005-04-06.bg.spam.txt', '0724.2004-09-21.bg.spam.txt', '2835.2005-01-19.bg.spam.txt']
    >>>
    >>> # spammy object created
    >>> cl = spammy(directory, limit=100)
    >>> cl.train()
    >>>
    >>> spam_text = \
    ... """"""
    ... my dear friend,
    ... 
    ... how are you and your family? i hope you all are fine.
    ... 
    ... my dear i know that this mail will come to you as a surprise, but it's for my 
    ... urgent need for a foreign partner that made me to contact you for your sincere
    ... genuine assistance my name is mr.herman hirdiramani, i am a banker by 
    ... profession currently holding the post of director auditing department in 
    ... the islamic development bank(isdb)here in ouagadougou, burkina faso.
    ... 
    ... i got your email information through the burkina's chamber of commerce 
    ... and industry on foreign business relations here in ouagadougou burkina faso 
    ... i haven'disclose this deal to any body i hope that you will not expose or 
    ... betray this trust and confident that i am about to repose on you for the 
    ... mutual benefit of our both families.
    ... 
    ... i need your urgent assistance in transferring the sum of eight million,
    ... four hundred and fifty thousand united states dollars ($8,450,000:00) into
    ... your account within 14 working banking days this money has been dormant for 
    ... years in our bank without claim due to the owner of this fund died along with 
    ... his entire family and his supposed next of kin in an underground train crash 
    ... since years ago. for your further informations please visit 
    ... (http://news.bbc.co.uk/2/hi/5141542.stm)
    ... """"""
    >>> cl.classify(spam_text)
    'spam'
    >>>


.. figure:: http://i.imgur.com/l8moq2u.jpg
    :alt:

accuracy of the classifier
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: python

    >>> from spammy import spammy
    >>> directory = '/home/tasdik/dropbox/projects/spammy/examples/training_dataset'
    >>> cl = spammy(directory, limit=300)  # training on only 300 spam and ham files
    >>> cl.train()
    >>> data_dir = '/home/tasdik/dropbox/projects/spammy/examples/test_dataset'
    >>>
    >>> cl.accuracy(directory=data_dir, label='spam', limit=300)
    0.9554794520547946
    >>> cl.accuracy(directory=data_dir, label='ham', limit=300)
    0.9033333333333333
    >>> 

**note**: 

- more examples can be found over in the `examples directory <https://github.com/tasdikrahman/spammy/tree/master/examples>`__

installation
------------
`[back to top] <#overview>`__

.. figure:: http://hd.wallpaperswide.com/thumbs/shut_up_and_take_my_money-t2.jpg
    :alt:

**note**: spammy currently supports only ``python2``

**install the dependencies first**

.. code:: bash

    $ pip install nltk==3.2.1, beautifulsoup4==4.4.1


to install use pip:

.. code:: bash

    $ pip install spammy

or if you don't have ``pip``use ``easy_install``

.. code:: bash

    $ easy_install spammy

or build it yourself (only if you must):


.. code:: bash

    $ git clone https://github.com/tasdikrahman/spammy.git
    $ python setup.py install

upgrading
~~~~~~~~~

to upgrade the package, 

.. code:: bash

    $ pip install -u spammy

installation behind a proxy
~~~~~~~~~~~~~~~~~~~~~~~~~~~

if you are behind a proxy, then this should work
    
.. code:: bash

    $ pip --proxy [username:password@]domain_name:port install spammy

benchmarks
----------
`[back to top] <#overview>`__

spammy is blazingly fast once trained

don't believe me? have a look

.. code:: python

    >>> import timeit
    >>> from spammy import spammy
    >>>
    >>> directory = '/home/tasdik/dropbox/projects/spamfilter/data/corpus3'
    >>> cl = spammy(directory, limit=100)
    >>> cl.train()
    >>> spam_text_2 = \
    ... """"""
    ... international monetary fund (imf)
    ... dept: world debt reconciliation agencies.
    ... advise: your outstanding payment notification
    ...  
    ... attention
    ... a power of attorney was forwarded to our office this morning by two gentle men,
    ... one of them is an american national and he is mr david deane by name while the
    ... other person is mr... jack morgan by name a canadian national.
    ... this gentleman claimed to be your representative, and this power of attorney 
    ... stated that you are dead; they brought an account to replace your information 
    ... in other to claim your fund of (us$9.7m) which is now lying dormant and unclaimed,
    ...  below is the new account they have submitted:
    ...                     bank.-hsbc canada
    ...                     vancouver, canada
    ...                     account no. 2984-0008-66
    ...  
    ... be further informed that this power of attorney also stated that you suffered.
    ... """"""
    >>>
    >>> def classify_timeit():
    ...    result = cl.classify(spam_text_2)
    ... 
    >>> timeit.repeat(classify_timeit, number=5)
    [0.1810469627380371, 0.16121697425842285, 0.16121196746826172]
    >>>


contributing
------------
`[back to top] <#overview>`__

refer `contributing <https://github.com/tasdikrahman/spammy/tree/master/contributing.rst>`__ page for details

roadmap
~~~~~~~

- include more algorithms for increased accuracy
- ``python3`` support

licensing
---------
`[back to top] <#overview>`__

spammy is built by `tasdik rahman <http://tasdikrahman.me>`__ and licensed under gplv3.

    spammy
    copyright (c) 2016  tasdik rahman(prodicus@outlook.com)

    this program is free software: you can redistribute it and/or modify
    it under the terms of the gnu general public license as published by
    the free software foundation, either version 3 of the license, or
    (at your option) any later version.

    this program is distributed in the hope that it will be useful,
    but without any warranty; without even the implied warranty of
    merchantability or fitness for a particular purpose.  see the
    gnu general public license for more details.

    you should have received a copy of the gnu general public license
    along with this program.  if not, see <http://www.gnu.org/licenses/>.

you can find a full copy of the license file `here <https://github.com/tasdikrahman/spammy/tree/master/license.txt>`__

credits
-------
`[back to top] <#overview>`__

if you'd like give me credit somewhere on your blog or tweet a shout out to `@tasdikrahman <https://twitter.com/tasdikrahman>`__, well hey, i'll take it.

donation
--------

if you have found my little bits of software of any use to you, you can help me pay my internet bills :)

|paypal badge|

|instamojo|

|gratipay|

|patreon|

.. |pypi version| image:: https://img.shields.io/pypi/v/spammy.svg
   :target: https://pypi.python.org/pypi/spammy
.. |build status| image:: https://travis-ci.org/tasdikrahman/spammy.svg?branch=master
    :target: https://travis-ci.org/tasdikrahman/spammy
.. |license| image:: https://img.shields.io/pypi/l/spammy.svg
   :target: https://pypi.python.org/pypi/spammy
.. |python versions| image:: https://img.shields.io/pypi/pyversions/spammy.svg
    :target: https://pypi.python.org/pypi/spammy
.. |grade| image:: https://api.codacy.com/project/badge/grade/c61c09b6c4ca4580b1f24c03ce3ad8e2
    :target: https://www.codacy.com/app/tasdik95/spammy
.. |percentagecov| image:: https://api.codacy.com/project/badge/coverage/e2cb32eae16242f795f498d40d0d8984
    :target: https://www.codacy.com/app/tasdik95/spammy
.. |requirements status| image:: https://requires.io/github/tasdikrahman/spammy/requirements.svg?branch=master
     :target: https://requires.io/github/tasdikrahman/spammy/requirements/?branch=master
     :alt: requirements status
.. |paypal badge| image:: https://www.paypalobjects.com/webstatic/mktg/logo/am_mc_vs_dc_ae.jpg
   :target: https://www.paypal.me/tasdik
.. |gratipay| image:: https://cdn.rawgit.com/gratipay/gratipay-badge/2.3.0/dist/gratipay.png
   :target: https://gratipay.com/tasdikrahman/
.. |instamojo| image:: https://www.soldermall.com/images/pic-online-payment.jpg
   :target: https://www.instamojo.com/@tasdikrahman
.. |patreon| image:: http://i.imgur.com/icwpfos.png
   :target: https://www.patreon.com/tasdikrahman/

"
"KoNLPy","konlpy
======

.. image:: https://badges.gitter.im/join%20chat.svg
   :alt: join the chat at https://gitter.im/konlpy/konlpy
   :target: https://gitter.im/konlpy/konlpy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

.. image:: https://img.shields.io/travis/konlpy/konlpy.svg
    :target: https://travis-ci.org/konlpy/konlpy
    :alt: build status

.. image:: https://readthedocs.org/projects/konlpy/badge/?version=latest
    :target: https://readthedocs.org/projects/konlpy/?badge=latest
    :alt: documentation status

.. image:: https://img.shields.io/coveralls/konlpy/konlpy.svg
    :target: https://coveralls.io/r/konlpy/konlpy
    :alt: coverage status

.. image:: https://img.shields.io/pypi/status/konlpy.svg
    :target: https://pypi.python.org/pypi/konlpy/
    :alt: development status

.. image:: https://img.shields.io/badge/licence-gpl-blue.svg
    :target: http://www.gnu.org/copyleft/gpl.html
    :alt: license


ÌïúÍµ≠Ïñ¥ ÏûêÏó∞Ïñ¥Ï≤òÎ¶¨Î•º Ìï† Ïàò ÏûàÎäî ÌååÏù¥Ïç¨ Ìå®ÌÇ§ÏßÄÏûÖÎãàÎã§.

konlpy is a python package for natural language processing of the korean language. 

- english documentation: http://konlpy.org/en/latest
- ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú: http://konlpy.org/ko/latest

links
------

- konlpy was rewrapped using py4j by `steven han <https://github.com/nazgul33>`_: https://github.com/nazgul33/konlpy
"
"Rosetta","rosetta
====

tools for data science with a focus on text processing.

* focuses on ""medium data"", i.e. data too big to fit into memory but too small to necessitate the use of a cluster.
* integrates with existing scientific python stack as well as select outside tools.

examples
--------

* see the `examples/` directory.  
* the [docs](http://pythonhosted.org/rosetta/#examples) contain plots of example output.


packages
--------

### `cmdutils` 
* unix-like command line utilities.  filters (read from stdin/write to stdout) for files.
* focus on stream processing and csv files.

### `parallel` 
* wrappers for python multiprocessing that add ease of use
* memory-friendly multiprocessing

### `text`
* stream text from disk to formats used in common ml processes
* write processed text to sparse formats
* helpers for ml tools (e.g. vowpal wabbit, gensim, etc...)
* other general utilities

### `workflow`
* high-level wrappers that have helped with our workflow and provide additional examples of code use

### `modeling`
* general ml modeling utilities

install
-------
check out the master branch from the [rosettarepo][rosettarepo].  then, (so long as you have `pip`).
    
    cd rosetta
    make
    make test
    
if you update the source, you can do

    make reinstall
    make test

the above `make` targets use `pip`, so you can of course do `pip uninstall` at any time.

getting the source (above) is the preferred method since the code changes often, but if you don't use git you can download a tagged release (tarball) [here](https://github.com/columbia-applied-data-science/rosetta/releases).  then

    pip install rosetta-x.x.x.tar.gz

development
-----------

### code

you can get the latest sources with

    git clone git://github.com/columbia-applied-data-science/rosetta

### contributing

feel free to contribute a bug report or a request by opening an [issue](https://github.com/columbia-applied-data-science/rosetta/issues)

the preferred method to contribute is to fork and send a pull request.  before doing this, read [contributing.md](contributing.md)

dependencies
------------

* major dependencies on *pandas* and *numpy*.
* minor dependencies on *gensim* and *statsmodels*.
* some examples need *scikit-learn*.
* minor dependencies on *docx*
* minor dependencies on the unix utilities *pdftotext* and *catdoc*

testing
-------
from the base repo directory, `rosetta/`, you can run all tests with

    make test

documentation
-------------

documentation for releases is hosted at [pypi](http://pythonhosted.org/rosetta).  this does not auto-update.


history
-------
*rosetta* refers to the [rosetta stone](http://en.wikipedia.org/wiki/rosetta_stone), the ancient egyptian tablet discovered just over 200 years ago. the tablet contained fragmented text in three different languages and the uncovering of its meaning is considered an essential key to our understanding of ancient egyptian civilization. we would like this project to provide individuals the necessary tools to process and unearth insight in the ever-growing volumes of textual data of today.

[rosettarepo]: https://github.com/columbia-applied-data-science/rosetta
"
"PySS3","<img src=""https://raw.githubusercontent.com/sergioburdisso/pyss3/master/docs/_static/ss3_logo_banner.png"" alt=""pyss3 logo"" title=""pyss3"" height=""150"" />

[![documentation status](https://readthedocs.org/projects/pyss3/badge/?version=latest)](http://pyss3.readthedocs.io/en/latest/?badge=latest)
[![build status](https://api.travis-ci.com/sergioburdisso/pyss3.svg?branch=master)](https://app.travis-ci.com/github/sergioburdisso/pyss3)
[![codecov](https://codecov.io/gh/sergioburdisso/pyss3/branch/master/graph/badge.svg)](https://codecov.io/gh/sergioburdisso/pyss3)
[![requirements status](https://requires.io/github/sergioburdisso/pyss3/requirements.svg?branch=master)](https://requires.io/github/sergioburdisso/pyss3/requirements/?branch=master)
[![pypi version](https://badge.fury.io/py/pyss3.svg)](https://badge.fury.io/py/pyss3)
[![downloads](https://pepy.tech/badge/pyss3)](https://pepy.tech/project/pyss3)
[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sergioburdisso/pyss3/master?filepath=examples)

---

# a python package implementing a new simple and interpretable model for text classification

:sushi: **online live demos:** http://tworld.io/ss3/ :icecream::ice_cream::cake:

<br>

the ss3 text classifier is a novel and simple supervised machine learning model for text classification which is interpretable, that is, it has the **ability to naturally (self)explain its rationale**. it was originally introduced in section 3 of the paper _[""a text classification framework for simple and effective early depression detection over social media streams""](https://dx.doi.org/10.1016/j.eswa.2019.05.023)_ ([arxiv preprint](https://arxiv.org/abs/1905.08772)).
this simple model obtained the best and 2nd-best results, consecutively, in the last three editions of the [clef's erisk lab](https://erisk.irlab.org/) among all participating models [[burdisso *et al.* 2019](http://ceur-ws.org/vol-2380/paper_103.pdf); [loyola *et al.* 2021](http://ceur-ws.org/vol-2936/paper-81.pdf)].
given its white-box nature, it allows researchers and practitioners to deploy interpretable (i.e. self-explainable) and therefore more reliable, models for text classification (which could be especially useful for those working with classification problems by which people's lives could be somehow affected).

**note:** this package also incorporates different variations of the original model, such as the one introduced in _[""t-ss3: a text classifier with dynamic n-grams for early risk detection over text streams""](https://doi.org/10.1016/j.patrec.2020.07.001)_ ([arxiv preprint](https://arxiv.org/abs/1911.06147)) which allows ss3 to recognize important variable-length word n-grams ""on the fly"".

## what is pyss3?

[pyss3](https://github.com/sergioburdisso/pyss3) is a python package that allows you to work with ss3 in a very straightforward, interactive and visual way. in addition to the implementation of the ss3 classifier, pyss3 comes with a set of tools to help you developing your machine learning models in a clearer and faster way. these tools let you analyze, monitor and understand your models by allowing you to see what they have actually learned and why. to achieve this, pyss3 provides you with 3  main components: the ``ss3`` class, the ``live_test`` class, and the ``evaluation`` class, as pointed out below.


### :point_right: the ``ss3`` class

which implements the classifier using a clear api. for instance, let's first load [one of the tutorial](https://pyss3.rtfd.io/en/latest/tutorials/movie-review.html)'s dataset:

```python
from pyss3.util import dataset

url = ""https://github.com/sergioburdisso/pyss3/raw/master/examples/datasets/movie_review.zip""

x_train, y_train = dataset.load_from_url(url, ""train"")
x_test, y_test = dataset.load_from_url(url, ""test"")
```

now let's train our first ss3 model! note that the api is very similar to that of `sklearn`'s models:

````python
from pyss3 import ss3

clf = ss3()
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
````

also, this class provides a handful of other useful methods, such as, for instance, [``extract_insight()``](https://pyss3.rtfd.io/en/latest/api/index.html#pyss3.ss3.extract_insight) to [extract the text fragments involved in the classification decision](https://pyss3.readthedocs.io/en/latest/tutorials/extract-insight.html) (allowing you to better understand the rationale behind the model‚Äôs predictions) or [``classify_multilabel()``](https://pyss3.rtfd.io/en/latest/api/index.html#pyss3.ss3.classify_multilabel) to provide [multi-label classification](https://en.wikipedia.org/wiki/multi-label_classification) support: 

````python
doc = ""liverpool ceo peter moore on building a global fanbase""

# standard ""single-label"" classification
label = clf.classify_label(doc) # 'business'

# multi-label classification
labels = clf.classify_multilabel(doc)  # ['business', 'sports']
````

### :point_right: the ``live_test`` class

which allows you to interactively test your model and visually see the reasons behind classification decisions, **with just one line of code**:
```python
from pyss3.server import live_test

clf = ss3()
clf.fit(x_train, y_train)

live_test.run(clf, x_test, y_test) # <- this one! cool uh? :)
```
as shown in the image below, this will open up, locally, an interactive tool in your browser which you can use to (live) test your models with the documents given in `x_test` (or typing in your own!). this will allow you to visualize and understand what your model is actually learning.

![img](https://raw.githubusercontent.com/sergioburdisso/pyss3/master/docs/_static/ss3_live_test.gif)

for example, we have uploaded two of these live tests online for you to try out: [""movie review (sentiment analysis)""](http://tworld.io/ss3/live_test_online/#30305) and [""topic categorization""](http://tworld.io/ss3/live_test_online/#30303), both were obtained following the [tutorials](https://pyss3.readthedocs.io/en/latest/user_guide/getting-started.html#tutorials).

### :point_right: and last but not least, the ``evaluation`` class

this is probably one of the most useful components of pyss3. as the name suggests, this class provides the user easy-to-use methods for model evaluation and hyperparameter optimization, like, for example, the [``test``](https://pyss3.readthedocs.io/en/latest/api/index.html#pyss3.util.evaluation.test), [``kfold_cross_validation``](https://pyss3.readthedocs.io/en/latest/api/index.html#pyss3.util.evaluation.kfold_cross_validation), [``grid_search``](https://pyss3.readthedocs.io/en/latest/api/index.html#pyss3.util.evaluation.grid_search), and [``plot``](https://pyss3.readthedocs.io/en/latest/api/index.html#pyss3.util.evaluation.plot) methods for performing tests, stratified k-fold cross validations, grid searches for hyperparameter optimization, and visualizing evaluation results using an interactive 3d plot, respectively. probably one of its most important features is the ability to automatically (and permanently) record the history of evaluations that you've performed. this will save you a lot of time and will allow you to interactively visualize and analyze your classifier performance in terms of its different hyper-parameters values (and select the best model according to your needs). for instance, let's perform a grid search with a 4-fold cross-validation on the three [hyperparameters](https://pyss3.rtfd.io/en/latest/user_guide/ss3-classifier.html#hyperparameters), smoothness(`s`), significance(`l`), and sanction(`p`):

```python
from pyss3.util import evaluation

best_s, best_l, best_p, _ = evaluation.grid_search(
    clf, x_train, y_train,
    s=[0.2, 0.32, 0.44, 0.56, 0.68, 0.8],
    l=[0.1, 0.48, 0.86, 1.24, 1.62, 2],
    p=[0.5, 0.8, 1.1, 1.4, 1.7, 2],
    k_fold=4
)
```
in this illustrative example, `s`, `l`, and `p` will take those 6 different values each, and once the search is over, this function will return (by default) the hyperparameter values that obtained the best accuracy.
now, we could also use the ``plot`` function to analyze the results obtained in our grid search using the interactive 3d evaluation plot:

```python
evaluation.plot()
```

![img](https://raw.githubusercontent.com/sergioburdisso/pyss3/master/docs/_static/plot_evaluations.gif)

in this 3d plot, each point represents an experiment/evaluation performed using that particular combination of values (`s`, `l`, and `p`). also, these points are painted proportional to how good the performance was according to the selected metric; the plot will update ""on the fly"" when the user select a different evaluation metric (accuracy, precision, recall, f1, etc.). additionally, when the cursor is moved over a data point, useful information is shown (including a ""compact"" representation of the confusion matrix obtained in that experiment). finally, it is worth mentioning that, before showing the 3d plots, pyss3 creates a single and portable html file in your project folder containing the interactive plots. this allows users to store, send or upload the plots to another place using this single html file. for example, we have uploaded two of these files for you to see: [""sentiment analysis (movie reviews)""](https://pyss3.readthedocs.io/en/latest/_static/ss3_model_evaluation[movie_review_3grams].html) and [""topic categorization""](https://pyss3.readthedocs.io/en/latest/_static/ss3_model_evaluation[topic_categorization_3grams].html), both evaluation plots were also obtained following the [tutorials](https://pyss3.readthedocs.io/en/latest/user_guide/getting-started.html#tutorials).


## want to give pyss3 a shot? :eyeglasses: :coffee:

just go to the [getting started](https://pyss3.readthedocs.io/en/latest/user_guide/getting-started.html) page :d

### installation


simply use:
```console
pip install pyss3
```

## want to contribute to this open source project? :sparkles::octocat::sparkles:

thanks for your interest in the project, you're ![awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)!!
any kind of help is very welcome (code, bug reports, content, data, documentation, design, examples, ideas, feedback, etc.),  issues and/or pull requests are welcome for any level of improvement, from a small typo to new features, help us make pyss3 better :+1:

remember that you can use the ""edit"" button ('pencil' icon) up the top to [edit any file of this repo directly on github](https://help.github.com/en/github/managing-files-in-a-repository/editing-files-in-your-repository).

finally, in case you're planning to create a **new pull request**, for committing to this repo, we follow the ""seven rules of a great git commit message"" from [""how to write a git commit message""](https://chris.beams.io/posts/git-commit/), so make sure your commits follow them as well.

(if you need any further information, please, **do not hesitate** to contact me - sergio.burdisso@gmail.com)

### contributors :muscle::sunglasses::+1:

thanks goes to these awesome people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

<!-- all-contributors-list:start - do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tr>
    <td align=""center""><a href=""http://angermeir.me/""><img src=""https://avatars3.githubusercontent.com/u/16398152?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>florian angermeir</b></sub></a><br /><a href=""https://github.com/sergioburdisso/pyss3/commits?author=angrymeir"" title=""code"">üíª</a> <a href=""#ideas-angrymeir"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""#data-angrymeir"" title=""data"">üî£</a></td>
    <td align=""center""><a href=""https://www.linkedin.com/in/muneebvaiyani/""><img src=""https://avatars3.githubusercontent.com/u/36028992?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>muneeb vaiyani</b></sub></a><br /><a href=""#ideas-vaiyani"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""#data-vaiyani"" title=""data"">üî£</a></td>
    <td align=""center""><a href=""https://www.saurabhbora.com""><img src=""https://avatars2.githubusercontent.com/u/29205181?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>saurabh bora</b></sub></a><br /><a href=""#ideas-enthussb"" title=""ideas, planning, & feedback"">ü§î</a></td>
    <td align=""center""><a href=""https://hbaniecki.com""><img src=""https://avatars.githubusercontent.com/u/32574004?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>hubert baniecki</b></sub></a><br /><a href=""#ideas-hbaniecki"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""https://github.com/sergioburdisso/pyss3/commits?author=hbaniecki"" title=""documentation"">üìñ</a></td>
  </tr>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- all-contributors-list:end -->

this project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. contributions of any kind welcome!

## further readings :scroll:


[full documentation](https://pyss3.readthedocs.io)

[api documentation](https://pyss3.readthedocs.io/en/latest/api/)

[paper preprint](https://arxiv.org/abs/1912.09322)
"
"PyStanfordDependencies","pystanforddependencies
======================

.. image:: https://travis-ci.org/dmcc/pystanforddependencies.svg?branch=master
    :target: https://travis-ci.org/dmcc/pystanforddependencies

.. image:: https://badge.fury.io/py/pystanforddependencies.png
   :target: https://badge.fury.io/py/pystanforddependencies

.. image:: https://coveralls.io/repos/dmcc/pystanforddependencies/badge.png?branch=master
   :target: https://coveralls.io/r/dmcc/pystanforddependencies?branch=master

python interface for converting `penn treebank
<http://www.cis.upenn.edu/~treebank/>`_ trees to `universal
dependencies <http://universaldependencies.github.io/docs/>`_
and `stanford dependencies
<http://nlp.stanford.edu/software/stanford-dependencies.shtml>`_.

example usage
-------------
start by getting a ``stanforddependencies`` instance with
``stanforddependencies.get_instance()``::

    >>> import stanforddependencies
    >>> sd = stanforddependencies.get_instance(backend='subprocess')

``get_instance()`` takes several options. ``backend`` can currently
be ``subprocess`` or ``jpype`` (see below). if you have an existing
`stanford corenlp <http://nlp.stanford.edu/software/corenlp.shtml>`_ or
`stanford parser <http://nlp.stanford.edu/software/lex-parser.shtml>`_
jar file, use the ``jar_filename`` parameter to point to the full path of
the jar file. otherwise, pystanforddependencies will download a jar file
for you and store it in locally (``~/.local/share/pystanforddeps``). you
can request a specific version with the ``version`` flag, e.g.,
``version='3.4.1'``. to convert trees, use the ``convert_trees()`` or
``convert_tree()`` method (note that by default, ``convert_trees()`` can
be considerably faster if you're doing batch conversion). these return
a sentence (list of ``token`` objects) or a list of sentences (list of
list of ``token`` objects) respectively::

    >>> sent = sd.convert_tree('(s1 (np (dt some) (jj blue) (nn moose)))')
    >>> for token in sent:
    ...     print token
    ...
    token(index=1, form='some', cpos='dt', pos='dt', head=3, deprel='det')
    token(index=2, form='blue', cpos='jj', pos='jj', head=3, deprel='amod')
    token(index=3, form='moose', cpos='nn', pos='nn', head=0, deprel='root')

this tells you that ``moose`` is the head of the sentence and is
modified by ``some`` (with a ``det`` = determiner relation) and ``blue``
(with an ``amod`` = adjective modifier relation). fields on ``token``
objects are readable as attributes. see docs for additional options in
``convert_tree()`` and ``convert_trees()``.

visualization
-------------

if you have the `asciitree <https://pypi.python.org/pypi/asciitree>`_
package, you can use a prettier ascii formatter::

    >>> print sent.as_asciitree()
     moose [root]
      +-- some [det]
      +-- blue [amod]

if you have python 2.7 or later, you can use `graphviz
<http://graphviz.org/>`_ to render your graphs. you'll need the `python
graphviz <https://pypi.python.org/pypi/graphviz>`_ package to call
``as_dotgraph()``::

    >>> dotgraph = sent.as_dotgraph()
    >>> print dotgraph
    digraph {
            0 [label=root]
            1 [label=some]
                    3 -> 1 [label=det]
            2 [label=blue]
                    3 -> 2 [label=amod]
            3 [label=moose]
                    0 -> 3 [label=root]
    }
    >>> dotgraph.render('moose') # renders a pdf by default
    'moose.pdf'
    >>> dotgraph.format = 'svg'
    >>> dotgraph.render('moose')
    'moose.svg'

the python `xdot <https://pypi.python.org/pypi/xdot>`_
package provides an interactive visualization::

    >>> import xdot
    >>> window = xdot.dotwindow()
    >>> window.set_dotcode(dotgraph.source)

both ``as_asciitree()`` and ``as_dotgraph()`` allow customization.
see the docs for additional options.

backends
--------
currently pystanforddependencies includes two backends:

- ``subprocess`` (works anywhere with a ``java`` binary, but more
  overhead so batched conversions with ``convert_trees()`` are
  recommended)
- ``jpype`` (requires `jpype1 <https://pypi.python.org/pypi/jpype1>`_,
  faster than the subprocess backend, also includes access to the stanford
  corenlp lemmatizer)

by default, pystanforddependencies will attempt to use the ``jpype``
backend. if ``jpype`` isn't available or crashes on startup,
pystanforddependencies will fallback to ``subprocess`` with a warning.

universal dependencies status
-----------------------------
pystanforddependencies supports most features in `universal dependencies
<http://universaldependencies.github.io/docs/>`_ (see `issue #10
<https://github.com/dmcc/pystanforddependencies/issues/10>`_ for the
most up to date status). pystanforddependencies output matches universal
dependencies in terms of structure and dependency labels, but universal
pos tags and features are missing. currently, pystanforddependencies will
output universal dependencies by default (unless you're using stanford
corenlp 3.5.1 or earlier).

related projects
----------------
- `clearnlp-converter <https://pypi.python.org/pypi/clearnlp-converter/>`_
  (uses `clearnlp <http://www.clearnlp.com/>`_ instead of `stanford
  corenlp <http://nlp.stanford.edu/software/corenlp.shtml>`_ for
  dependency conversion)

more information
----------------
licensed under `apache 2.0 <http://www.apache.org/licenses/license-2.0>`_.

written by david mcclosky (`homepage
<http://nlp.stanford.edu/~mcclosky/>`_, `code <http://github.com/dmcc>`_)

bug reports and feature requests: `github issue tracker
<http://github.com/dmcc/pystanforddependencies/issues>`_

release summaries
-----------------
- 0.3.1 (2015.11.02): better collapsed universal handling, bugfixes
- 0.3.0 (2015.10.09): support copy nodes, more input checking/debugging
  help, example ``convert.py`` program
- 0.2.0 (2015.08.02): universal dependencies support (mostly),
  python 3 support (fully), minor api updates
- 0.1.7 (2015.06.13): bugfixes for ``jpype``, handle version mismatches
  in ibm java
- 0.1.6 (2015.02.12): support for ``graphviz`` formatting, corenlp 3.5.1,
  better windows portability
- 0.1.5 (2015.01.10): support for ascii tree formatting
- 0.1.4 (2015.01.07): fix ``ccprocessed`` support
- 0.1.3 (2015.01.03): bugfixes, coveralls integration, refactoring
- 0.1.2 (2015.01.02): better conll structures, test suite and travis ci
  support, bugfixes
- 0.1.1 (2014.12.15): more docs, fewer bugs
- 0.1 (2014.12.14): initial release
"
"jellyfish","# overview

**jellyfish** is a library for approximate & phonetic matching of strings.

source: [https://github.com/jamesturk/jellyfish](https://github.com/jamesturk/jellyfish)

documentation: [https://jamesturk.github.io/jellyfish/](https://jamesturk.github.io/jellyfish/)

issues: [https://github.com/jamesturk/jellyfish/issues](https://github.com/jamesturk/jellyfish/issues)

[![pypi badge](https://badge.fury.io/py/jellyfish.svg)](https://badge.fury.io/py/jellyfish)
[![test badge](https://github.com/jamesturk/jellyfish/workflows/python%20package/badge.svg)](https://github.com/jamesturk/jellyfish/actions?query=workflow%3a%22python+package)
[![coveralls](https://coveralls.io/repos/jamesturk/jellyfish/badge.png?branch=master)](https://coveralls.io/r/jamesturk/jellyfish)


## included algorithms

string comparison:

* levenshtein distance
* damerau-levenshtein distance
* jaro distance
* jaro-winkler distance
* match rating approach comparison
* hamming distance

phonetic encoding:

* american soundex
* metaphone
* nysiis (new york state identification and intelligence system)
* match rating codex

## example usage

``` python
>>> import jellyfish
>>> jellyfish.levenshtein_distance(u'jellyfish', u'smellyfish')
2
>>> jellyfish.jaro_distance(u'jellyfish', u'smellyfish')
0.89629629629629637
>>> jellyfish.damerau_levenshtein_distance(u'jellyfish', u'jellyfihs')
1

>>> jellyfish.metaphone(u'jellyfish')
'jlfx'
>>> jellyfish.soundex(u'jellyfish')
'j412'
>>> jellyfish.nysiis(u'jellyfish')
'jalyf'
>>> jellyfish.match_rating_codex(u'jellyfish')
'jllfsh'
```
"
"stanford-corenlp-python","# python interface to stanford core nlp tools v3.4.1

this is a python wrapper for stanford university's nlp group's java-based [corenlp tools](http://nlp.stanford.edu/software/corenlp.shtml).  it can either be imported as a module or run as a json-rpc server. because it uses many large trained models (requiring 3gb ram on 64-bit machines and usually a few minutes loading time), most applications will probably want to run it as a server.


   * python interface to stanford corenlp tools: tagging, phrase-structure parsing, dependency parsing, [named-entity recognition](http://en.wikipedia.org/wiki/named-entity_recognition), and [coreference resolution](http://en.wikipedia.org/wiki/coreference).
   * runs an json-rpc server that wraps the java server and outputs json.
   * outputs parse trees which can be used by [nltk](http://nltk.googlecode.com/svn/trunk/doc/howto/tree.html).


it depends on [pexpect](http://www.noah.org/wiki/pexpect) and includes and uses code from [jsonrpc](http://www.simple-is-better.org/rpc/) and [python-progressbar](http://code.google.com/p/python-progressbar/).

it runs the stanford corenlp jar in a separate process, communicates with the java process using its command-line interface, and makes assumptions about the output of the parser in order to parse it into a python dict object and transfer it using json.  the parser will break if the output changes significantly, but it has been tested on **core nlp tools version 3.4.1** released 2014-08-27.

## download and usage

to use this program you must [download](http://nlp.stanford.edu/software/corenlp.shtml#download) and unpack the compressed file containing stanford's corenlp package.  by default, `corenlp.py` looks for the stanford core nlp folder as a subdirectory of where the script is being run.  in other words:

	sudo pip install pexpect unidecode
	git clone git://github.com/dasmith/stanford-corenlp-python.git
	cd stanford-corenlp-python
	wget http://nlp.stanford.edu/software/stanford-corenlp-full-2014-08-27.zip
	unzip stanford-corenlp-full-2014-08-27.zip

then launch the server:

    python corenlp.py

optionally, you can specify a host or port:

    python corenlp.py -h 0.0.0.0 -p 3456

that will run a public json-rpc server on port 3456.

assuming you are running on port 8080, the code in `client.py` shows an example parse: 

    import jsonrpc
    from simplejson import loads
    server = jsonrpc.serverproxy(jsonrpc.jsonrpc20(),
                                 jsonrpc.transporttcpip(addr=(""127.0.0.1"", 8080)))

    result = loads(server.parse(""hello world.  it is so beautiful""))
    print ""result"", result

that returns a dictionary containing the keys `sentences` and `coref`. the key `sentences` contains a list of dictionaries for each sentence, which contain `parsetree`, `text`, `tuples` containing the dependencies, and `words`, containing information about parts of speech, recognized named-entities, etc:

	{u'sentences': [{u'parsetree': u'(root (s (vp (np (intj (uh hello)) (np (nn world)))) (. !)))',
	                 u'text': u'hello world!',
	                 u'tuples': [[u'dep', u'world', u'hello'],
	                             [u'root', u'root', u'world']],
	                 u'words': [[u'hello',
	                             {u'characteroffsetbegin': u'0',
	                              u'characteroffsetend': u'5',
	                              u'lemma': u'hello',
	                              u'namedentitytag': u'o',
	                              u'partofspeech': u'uh'}],
	                            [u'world',
	                             {u'characteroffsetbegin': u'6',
	                              u'characteroffsetend': u'11',
	                              u'lemma': u'world',
	                              u'namedentitytag': u'o',
	                              u'partofspeech': u'nn'}],
	                            [u'!',
	                             {u'characteroffsetbegin': u'11',
	                              u'characteroffsetend': u'12',
	                              u'lemma': u'!',
	                              u'namedentitytag': u'o',
	                              u'partofspeech': u'.'}]]},
	                {u'parsetree': u'(root (s (np (prp it)) (vp (vbz is) (adjp (rb so) (jj beautiful))) (. .)))',
	                 u'text': u'it is so beautiful.',
	                 u'tuples': [[u'nsubj', u'beautiful', u'it'],
	                             [u'cop', u'beautiful', u'is'],
	                             [u'advmod', u'beautiful', u'so'],
	                             [u'root', u'root', u'beautiful']],
	                 u'words': [[u'it',
	                             {u'characteroffsetbegin': u'14',
	                              u'characteroffsetend': u'16',
	                              u'lemma': u'it',
	                              u'namedentitytag': u'o',
	                              u'partofspeech': u'prp'}],
	                            [u'is',
	                             {u'characteroffsetbegin': u'17',
	                              u'characteroffsetend': u'19',
	                              u'lemma': u'be',
	                              u'namedentitytag': u'o',
	                              u'partofspeech': u'vbz'}],
	                            [u'so',
	                             {u'characteroffsetbegin': u'20',
	                              u'characteroffsetend': u'22',
	                              u'lemma': u'so',
	                              u'namedentitytag': u'o',
	                              u'partofspeech': u'rb'}],
	                            [u'beautiful',
	                             {u'characteroffsetbegin': u'23',
	                              u'characteroffsetend': u'32',
	                              u'lemma': u'beautiful',
	                              u'namedentitytag': u'o',
	                              u'partofspeech': u'jj'}],
	                            [u'.',
	                             {u'characteroffsetbegin': u'32',
	                              u'characteroffsetend': u'33',
	                              u'lemma': u'.',
	                              u'namedentitytag': u'o',
	                              u'partofspeech': u'.'}]]}],
	u'coref': [[[[u'it', 1, 0, 0, 1], [u'hello world', 0, 1, 0, 2]]]]}
    
to use it in a regular script (useful for debugging), load the module instead:

    from corenlp import *
    corenlp = stanfordcorenlp()  # wait a few minutes...
    corenlp.parse(""parse this sentence."")

the server, `stanfordcorenlp()`, takes an optional argument `corenlp_path` which specifies the path to the jar files.  the default value is `stanfordcorenlp(corenlp_path=""./stanford-corenlp-full-2014-08-27/"")`.

## coreference resolution

the library supports [coreference resolution](http://en.wikipedia.org/wiki/coreference), which means pronouns can be ""dereferenced.""  if an entry in the `coref` list is, `[u'hello world', 0, 1, 0, 2]`, the numbers mean:

  * 0 = the reference appears in the 0th sentence (e.g. ""hello world"")
  * 1 = the 2nd token, ""world"", is the [headword](http://en.wikipedia.org/wiki/head_%28linguistics%29) of that sentence
  * 0 = 'hello world' begins at the 0th token in the sentence
  * 2 = 'hello world' ends before the 2nd token in the sentence.

<!--


## adding wordnet

note: wordnet doesn't seem to be supported using this approach.  looks like you'll need java.

download wordnet-3.0 prolog:  http://wordnetcode.princeton.edu/3.0/wnprolog-3.0.tar.gz
tar xvfz wnprolog-3.0.tar.gz 

-->


## questions 

**stanford corenlp tools require a large amount of free memory**.  java 5+ uses about 50% more ram on 64-bit machines than 32-bit machines.  32-bit machine users can lower the memory requirements by changing `-xmx3g` to `-xmx2g` or even less.
if pexpect timesout while loading models, check to make sure you have enough memory and can run the server alone without your kernel killing the java process:

	java -cp stanford-corenlp-2014-08-27.jar:stanford-corenlp-3.4.1-models.jar:xom.jar:joda-time.jar -xmx3g edu.stanford.nlp.pipeline.stanfordcorenlp -props default.properties

you can reach me, dustin smith, by sending a message on github or through email (contact information is available [on my webpage](http://web.media.mit.edu/~dustin)).


# license & contributors

this is free and open source software and has benefited from the contribution and feedback of others.  like stanford's corenlp tools, it is covered under the [gnu general public license v2 +](http://www.gnu.org/licenses/gpl-2.0.html), which in short means that modifications to this program must maintain the same free and open source distribution policy.

i gratefully welcome bug fixes and new features.  if you have forked this repository, please submit a [pull request](https://help.github.com/articles/using-pull-requests/) so others can benefit from your contributions.  this project has already benefited from contributions from these members of the open source community:

  * [emilio monti](https://github.com/emilmont)
  * [justin cheng](https://github.com/jcccf) 
  * abhaya agarwal

*thank you!*

## related projects

maintainers of the core nlp library at stanford keep an [updated list of wrappers and extensions](http://nlp.stanford.edu/software/corenlp.shtml#extensions).  see brendan o'connor's [stanford_corenlp_pywrapper](https://github.com/brendano/stanford_corenlp_pywrapper) for a different approach more suited to batch processing.
"
"CLTK","|circleci| |pypi| |twitter| |discord|


.. |circleci| image:: https://circleci.com/gh/cltk/cltk/tree/master.svg?style=svg
   :target: https://circleci.com/gh/cltk/cltk/tree/master

.. |rtd| image:: https://img.shields.io/readthedocs/cltk
   :target: http://docs.cltk.org/

.. |codecov| image:: https://codecov.io/gh/cltk/cltk/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/cltk/cltk

.. |pypi| image:: https://img.shields.io/pypi/v/cltk
   :target: https://pypi.org/project/cltk/

.. |zenodo| image:: https://zenodo.org/badge/doi/10.5281/zenodo.3445585.svg
   :target: https://doi.org/10.5281/zenodo.3445585

.. |binder| image:: https://mybinder.org/badge_logo.svg
   :target: https://mybinder.org/v2/gh/cltk/tutorials/master

.. |twitter| image:: https://img.shields.io/twitter/url?style=social&url=https%3a%2f%2ftwitter.com%2fcltkorg&label=follow%20%40cltkorg
   :target: https://twitter.com/cltkorg
   
.. |discord| image:: https://img.shields.io/discord/974033391542480936
   :target: https://discord.gg/atudjqx7cg
 
the classical language toolkit (cltk) is a python library offering natural language processing (nlp) for pre-modern languages.


installation
============

for the cltk's latest version:

.. code-block:: bash

   $ pip install cltk

for more information, see `installation docs <https://docs.cltk.org/en/latest/installation.html>`_ or, to install from source, `development <https://docs.cltk.org/en/latest/development.html>`_.

pre-1.0 software remains available on the `branch v0.1.x <https://github.com/cltk/cltk/tree/v0.1.x>`_ and docs at `<https://legacy.cltk.org>`_. install it with ``pip install ""cltk<1.0""``.


documentation
=============

documentation at `<https://docs.cltk.org>`_.


citation
========

when using the cltk, please cite `the following publication <https://aclanthology.org/2021.acl-demo.3>`_, including the doi:

   johnson, kyle p., patrick j. burns, john stewart, todd cook, cl√©ment besnier, and william j. b.  mattingly. ""the classical language toolkit: an nlp framework for pre-modern languages."" in *proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing: system demonstrations*, pp. 20-29. 2021. 10.18653/v1/2021.acl-demo.3


the complete bibtex entry:

.. code-block:: bibtex

   @inproceedings{johnson-etal-2021-classical,
       title = ""the {c}lassical {l}anguage {t}oolkit: {a}n {nlp} framework for pre-modern languages"",
       author = ""johnson, kyle p.  and
         burns, patrick j.  and
         stewart, john  and
         cook, todd  and
         besnier, cl{\'e}ment  and
         mattingly, william j. b."",
       booktitle = ""proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing: system demonstrations"",
       month = aug,
       year = ""2021"",
       address = ""online"",
       publisher = ""association for computational linguistics"",
       url = ""https://aclanthology.org/2021.acl-demo.3"",
       doi = ""10.18653/v1/2021.acl-demo.3"",
       pages = ""20--29"",
       abstract = ""this paper announces version 1.0 of the classical language toolkit (cltk), an nlp framework for pre-modern languages. the vast majority of nlp, its algorithms and software, is created with assumptions particular to living languages, thus neglecting certain important characteristics of largely non-spoken historical languages. further, scholars of pre-modern languages often have different goals than those of living-language researchers. to fill this void, the cltk adapts ideas from several leading nlp frameworks to create a novel software architecture that satisfies the unique needs of pre-modern languages and their researchers. its centerpiece is a modular processing pipeline that balances the competing demands of algorithmic diversity with pre-configured defaults. the cltk currently provides pipelines, including models, for almost 20 languages."",
   }


license
=======

.. |year| date:: %y

copyright (c) 2014-|year| kyle p. johnson under the `mit license <https://github.com/cltk/cltk/blob/master/license>`_.
"
"Rasa","<h1 align=""center"">rasa open source</h1>

<div align=""center"">

[![join the chat on rasa community forum](https://img.shields.io/badge/forum-join%20discussions-brightgreen.svg)](https://forum.rasa.com/?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![pypi version](https://badge.fury.io/py/rasa.svg)](https://badge.fury.io/py/rasa)
[![supported python versions](https://img.shields.io/pypi/pyversions/rasa.svg)](https://pypi.python.org/pypi/rasa)
[![build status](https://github.com/rasahq/rasa/workflows/continuous%20integration/badge.svg)](https://github.com/rasahq/rasa/actions)
[![coverage status](https://api.codeclimate.com/v1/badges/756dc6fea1d5d3e127f7/test_coverage)](https://codeclimate.com/github/rasahq/rasa/)
[![documentation status](https://img.shields.io/badge/docs-stable-brightgreen.svg)](https://rasa.com/docs)
![documentation build](https://img.shields.io/netlify/d2e447e4-5a5e-4dc7-be5d-7c04ae7ff706?label=documentation%20build)
[![fossa status](https://app.fossa.com/api/projects/custom%2b8141%2fgit%40github.com%3arasahq%2frasa.git.svg?type=shield)](https://app.fossa.com/projects/custom%2b8141%2fgit%40github.com%3arasahq%2frasa.git?ref=badge_shield)
[![prs welcome](https://img.shields.io/badge/prs-welcome-brightgreen.svg?style=flat-square)](https://github.com/orgs/rasahq/projects/23)

</div>

<hr />

üí° **we're migrating issues to jira** üí°

starting january 2023, issues for rasa open source are located in
[this jira board](https://rasa-open-source.atlassian.net/browse/oss). you can browse issues without being logged in;
if you want to create issues, you'll need to create a jira account.

<hr />

<img align=""right"" height=""255"" src=""https://www.rasa.com/assets/img/sara/sara-open-source-2.0.png"" alt=""an image of sara, the rasa mascot bird, holding a flag that reads open source with one wing, and a wrench in the other"" title=""rasa open source"">

rasa is an open source machine learning framework to automate text and voice-based conversations. with rasa, you can build contextual assistants on:
- facebook messenger
- slack
- google hangouts
- webex teams
- microsoft bot framework
- rocket.chat
- mattermost
- telegram
- twilio
- your own custom conversational channels

or voice assistants as:
- alexa skills
- google home actions

rasa helps you build contextual assistants capable of having layered conversations with
lots of back-and-forth. in order for a human to have a meaningful exchange with a contextual
assistant, the assistant needs to be able to use context to build on things that were previously
discussed ‚Äì rasa enables you to build assistants that can do this in a scalable way.

there's a lot more background information in this
[blog post](https://medium.com/rasa-blog/a-new-approach-to-conversational-software-2e64a5d05f2a).

---
- ü§î [learn more about rasa](https://rasa.community/)

- ü§ì [read the docs](https://rasa.com/docs/rasa/)

- üòÅ [install rasa](https://rasa.com/docs/rasa/installation/environment-set-up)

- üöÄ [dive deeper in the learning center](https://learning.rasa.com/)

- ü§ó [contribute](#how-to-contribute)

- ‚ùì [get enterprise-grade support](https://rasa.com/support/)

- üè¢ [explore the features of our commercial platform](https://rasa.com/product/rasa-platform/)

- üìö [learn more about research papers that leverage rasa](https://scholar.google.com/scholar?oi=bibs&hl=en&authuser=1&cites=16243802403383697687,353275993797024115,14567308604105196228,9067977709825839723,9855847065463746011&as_sdt=5)



---
## where to get help

there is extensive documentation in the [rasa docs](https://rasa.com/docs/rasa).
make sure to select the correct version so you are looking at
the docs for the version you installed.

please use [rasa community forum](https://forum.rasa.com) for quick answers to
questions.

### readme contents:
- [how to contribute](#how-to-contribute)
- [development internals](#development-internals)
- [releases](#releases)
- [license](#license)

### how to contribute
we are very happy to receive and merge your contributions into this repository!

to contribute via pull request, follow these steps:

1. create an issue describing the feature you want to work on (or
   have a look at the [contributor board](https://github.com/orgs/rasahq/projects/23))
2. write your code, tests and documentation, and format them with ``black``
3. create a pull request describing your changes

for more detailed instructions on how to contribute code, check out these [code contributor guidelines](contributing.md).

you can find more information about how to contribute to rasa (in lots of
different ways!) [on our website.](http://rasa.community).

your pull request will be reviewed by a maintainer, who will get
back to you about any necessary changes or questions. you will
also be asked to sign a
[contributor license agreement](https://cla-assistant.io/rasahq/rasa).


## development internals

### installing poetry

rasa uses poetry for packaging and dependency management. if you want to build it from source,
you have to install poetry first. please follow
[the official guide](https://python-poetry.org/docs/#installation) to see all possible options.

### managing environments

the official [poetry guide](https://python-poetry.org/docs/managing-environments/) suggests to use
[pyenv](https://github.com/pyenv/pyenv) or any other similar tool to easily switch between python versions.
this is how it can be done:

```bash
pyenv install 3.7.9
pyenv local 3.7.9  # activate python 3.7.9 for the current project
```
*note*: if you have trouble installing a specific version of python on your system
it might be worth trying other supported versions.

by default, poetry will try to use the currently activated python version to create the virtual environment
for the current project automatically. you can also create and activate a virtual environment manually ‚Äî in this
case, poetry should pick it up and use it to install the dependencies. for example:

```bash
python -m venv .venv
source .venv/bin/activate
```

you can make sure that the environment is picked up by executing

```bash
poetry env info
```

### building from source

to install dependencies and `rasa` itself in editable mode execute

```bash
make install
```

*note for macos users*: under macos big sur we've seen some compiler issues for
dependencies. using `export system_version_compat=1` before the installation helped.


#### installing optional dependencies

in order to install rasa's optional dependencies, you need to run:

```bash
make install-full
```

*note for macos users*: the command `make install-full` could result in a failure while installing `tokenizers`
(issue described in depth [here](https://github.com/huggingface/tokenizers/issues/1050)).

in order to resolve it, you must follow these steps to install a rust compiler:
```bash
brew install rustup
rustup-init
```

after initialising the rust compiler, you should restart the console and check its installation:
```bash
rustc --version
```

in case the path variable had not been automatically setup, run:
```bash
export path=""$home/.cargo/bin:$path""
```


### running and changing the documentation

first of all, install all the required dependencies:

```bash
make install install-docs
```

after the installation has finished, you can run and view the documentation
locally using:

```bash
make livedocs
```

it should open a new tab with the local version of the docs in your browser;
if not, visit http://localhost:3000 in your browser.
you can now change the docs locally and the web page will automatically reload
and apply your changes.

### running the tests

in order to run the tests, make sure that you have the development requirements installed:

```bash
make prepare-tests-ubuntu # only on ubuntu and debian based systems
make prepare-tests-macos  # only on macos
```

then, run the tests:

```bash
make test
```

they can also be run at multiple jobs to save some time:

```bash
jobs=[n] make test
```

where `[n]` is the number of jobs desired. if omitted, `[n]` will be automatically chosen by pytest.


### running the integration tests

in order to run the integration tests, make sure that you have the development requirements installed:

```bash
make prepare-tests-ubuntu # only on ubuntu and debian based systems
make prepare-tests-macos  # only on macos
```

then, you'll need to start services with the following command which uses
[docker compose](https://docs.docker.com/compose/install/):

```bash
make run-integration-containers
```

finally, you can run the integration tests like this:

```bash
make test-integration
```


### resolving merge conflicts

poetry doesn't include any solution that can help to resolve merge conflicts in
the lock file `poetry.lock` by default.
however, there is a great tool called [poetry-merge-lock](https://poetry-merge-lock.readthedocs.io/en/latest/).
here is how you can install it:

```bash
pip install poetry-merge-lock
```

just execute this command to resolve merge conflicts in `poetry.lock` automatically:

```bash
poetry-merge-lock
```

### build a docker image locally

in order to build a docker image on your local machine execute the following command:

```bash
make build-docker
```

the docker image is available on your local machine as `rasa:localdev`.

### code style

to ensure a standardized code style we use the formatter [black](https://github.com/ambv/black).
to ensure our type annotations are correct we use the type checker [pytype](https://github.com/google/pytype).
if your code is not formatted properly or doesn't type check, github will fail to build.

#### formatting

if you want to automatically format your code on every commit, you can use [pre-commit](https://pre-commit.com/).
just install it via `pip install pre-commit` and execute `pre-commit install` in the root folder.
this will add a hook to the repository, which reformats files on every commit.

if you want to set it up manually, install black via `poetry install`.
to reformat files execute
```
make formatter
```

#### type checking

if you want to check types on the codebase, install `mypy` using `poetry install`.
to check the types execute
```
make types
```

### deploying documentation updates

we use `docusaurus v2` to build docs for tagged versions and for the `main` branch.
the static site that gets built is pushed to the `documentation` branch of this repo.

we host the site on netlify. on `main` branch builds (see `.github/workflows/documentation.yml`), we push the built docs to
the `documentation` branch. netlify automatically re-deploys the docs pages whenever there is a change to that branch.

## releases
rasa has implemented robust policies governing version naming, as well as release pace for major, minor, and patch releases.

the values for a given version number (major.minor.patch) are incremented as follows:
- major version for incompatible api changes or other breaking changes.
- minor version for functionality added in a backward compatible manner.
- patch version for backward compatible bug fixes.

the following table describes the version types and their expected *release cadence*:

| version type |                                                                  description                                                                  |  target cadence |
|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| major        | for significant changes, or when any backward-incompatible changes are introduced to the api or data model.                                   | every 1 - 2 yrs |
| minor        | for when new backward-compatible functionality is introduced, a minor feature is introduced, or when a set of smaller features is rolled out. | +/- quarterly   |
| patch        | for backward-compatible bug fixes that fix incorrect behavior.                                                                                | as needed       |

while this table represents our target release frequency, we reserve the right to modify it based on changing market conditions and technical requirements.

### maintenance policy
our end of life policy defines how long a given release is considered supported, as well as how long a release is
considered to be still in active development or maintenance.

the maintentance duration and end of life for every release are shown on our website as part of the [product release and maintenance policy](https://rasa.com/rasa-product-release-and-maintenance-policy/).

### cutting a major / minor release
#### a week before release day

1. **make sure the [milestone](https://github.com/rasahq/rasa/milestones) already exists and is scheduled for the
correct date.**
2. **take a look at the issues & prs that are in the milestone**: does it look about right for the release highlights
we are planning to ship? does it look like anything is missing? don't worry about being aware of every pr that should
be in, but it's useful to take a moment to evaluate what's assigned to the milestone.
3. **post a message on the engineering slack channel**, letting the team know you'll be the one cutting the upcoming
release, as well as:
    1. providing the link to the appropriate milestone
    2. reminding everyone to go over their issues and prs and please assign them to the milestone
    3. reminding everyone of the scheduled date for the release

#### a day before release day

1. **go over the milestone and evaluate the status of any pr merging that's happening. follow up with people on their
bugs and fixes.** if the release introduces new bugs or regressions that can't be fixed in time, we should discuss on
slack about this and take a decision on how to move forward. if the issue is not ready to be merged in time, we remove the issue / pr from the milestone and notify the pr owner and the product manager on slack about it. the pr / issue owners are responsible for
communicating any issues which might be release relevant. postponing the release should be considered as an edge case scenario.

#### release day! üöÄ

1. **at the start of the day, post a small message on slack announcing release day!** communicate you'll be handling
the release, and the time you're aiming to start releasing (again, no later than 4pm, as issues may arise and
cause delays). this message should be posted early in the morning and before moving forward with any of the steps of the release,
   in order to give enough time to people to check their prs and issues. that way they can plan any remaining work. a template of the slack message can be found [here](https://rasa-hq.slack.com/archives/c36ss4n8m/p1613032208137500?thread_ts=1612876410.068400&cid=c36ss4n8m).
   the release time should be communicated transparently so that others can plan potentially necessary steps accordingly. if there are bigger changes this should be communicated.
2. make sure the milestone is empty (everything has been either merged or moved to the next milestone)
3. once everything in the milestone is taken care of, post a small message on slack communicating you are about to
start the release process (in case anything is missing).
4. **you may now do the release by following the instructions outlined in the
[rasa open source readme](#steps-to-release-a-new-version) !**

#### after a major release

after a major release has been completed, please follow [these instructions to complete the documentation update](./docs/readme.md#manual-steps-after-a-new-version).

### steps to release a new version
releasing a new version is quite simple, as the packages are build and distributed by github actions.

*release steps*:
1. make sure all dependencies are up to date (**especially rasa sdk**)
    - for rasa sdk, except in the case of a patch release, that means first creating a [new rasa sdk release](https://github.com/rasahq/rasa-sdk#steps-to-release-a-new-version) (make sure the version numbers between the new rasa and rasa sdk releases match)
    - once the tag with the new rasa sdk release is pushed and the package appears on [pypi](https://pypi.org/project/rasa-sdk/), the dependency in the rasa repository can be resolved (see below).
2. if this is a minor / major release: make sure all fixes from currently supported minor versions have been merged from their respective release branches (e.g. 3.3.x) back into main.
3. in case of a minor release, create a new branch that corresponds to the new release, e.g.
   ```bash
    git checkout -b 1.2.x
    git push origin 1.2.x
    ```
4. switch to the branch you want to cut the release from (`main` in case of a major, the `<major>.<minor>.x` branch for minors and patches)
    - update the `rasa-sdk` entry in `pyproject.toml` with the new release version and run `poetry update`. this creates a new `poetry.lock` file with all dependencies resolved.
    - commit the changes with `git commit -am ""bump rasa-sdk dependency""` but do not push them. they will be automatically picked up by the following step.
5. if this is a major release, update the list of actively maintained versions [in the readme](#actively-maintained-versions) and in [the docs](./docs/docs/actively-maintained-versions.mdx).
6. run `make release`
7. create a pr against the release branch (e.g. `1.2.x`)
8. once your pr is merged, tag a new release (this should always happen on the release branch), e.g. using
    ```bash
    git checkout 1.2.x
    git pull origin 1.2.x
    git tag 1.2.0 -m ""next release""
    git push origin 1.2.0 --tags
    ```
    github will build this tag and publish the build artifacts.
9. after all the steps are completed and if everything goes well then we should see a message automatically posted in the company's slack (`product` channel) like this [one](https://rasa-hq.slack.com/archives/c7b08q5fx/p1614354499046600)
10. if no message appears in the channel then you can do the following checks:
    - check the workflows in [github actions](https://github.com/rasahq/rasa/actions) and make sure that the merged pr of the current release is completed successfully. to easily find your pr you can use the filters `event: push` and `branch: <version number>` (example on release 2.4 you can see [here](https://github.com/rasahq/rasa/actions/runs/643344876))
    - if the workflow is not completed, then try to re run the workflow in case that solves the problem
    - if the problem persists, check also the log files and try to find the root cause of the issue
    - if you still cannot resolve the error, contact the infrastructure team by providing any helpful information from your investigation
11.  after the message is posted correctly in the `product` channel, check also in the `product-engineering-alerts` channel if there are any alerts related to the rasa open source release like this [one](https://rasa-hq.slack.com/archives/c01585an2np/p1615486087001000)

### cutting a patch release

patch releases are simpler to cut, since they are meant to contain only bugfixes.

**the only things you need to do to cut a patch release are:**

1. notify the engineering team on slack that you are planning to cut a patch, in case someone has an important fix
to add.
2. make sure the bugfix(es) are in the release branch you will use (p.e if you are cutting a `2.0.4` patch, you will
need your fixes to be on the `2.0.x` release branch). all patch releases must come from a `.x` branch!
3. once you're ready to release the rasa open source patch, checkout the branch, run `make release` and follow the
steps + get the pr merged.
4. once the pr is in, pull the `.x` branch again and push the tag!

### actively maintained versions

we're actively maintaining _any minor on our latest major release_ and _the latest minor of the previous major release_.
currently, this means the following minor versions will receive bugfixes updates:
- 2.8
- every minor version on 3.x

## license
licensed under the apache license, version 2.0.
copyright 2022 rasa technologies gmbh. [copy of the license](license.txt).

a list of the licenses of the dependencies of the project can be found at
the bottom of the
[libraries summary](https://libraries.io/github/rasahq/rasa).
"
"Dedupe","# dedupe python library

[![tests passing](https://github.com/dedupeio/dedupe/workflows/tests/badge.svg)](https://github.com/dedupeio/dedupe/actions?query=workflow%3atests)[![codecov](https://codecov.io/gh/dedupeio/dedupe/branch/main/graph/badge.svg?token=aaukurtegh)](https://codecov.io/gh/dedupeio/dedupe)

_dedupe is a python library that uses machine learning to perform fuzzy matching, deduplication and entity resolution quickly on structured data._

__dedupe__ will help you: 

* __remove duplicate entries__ from a spreadsheet of names and addresses
* __link a list__ with customer information to another with order history, even without unique customer ids
* take a database of campaign contributions and __figure out which ones were made by the same person__, even if the names were entered slightly differently for each record

dedupe takes in human training data and comes up with the best rules for your dataset to quickly and automatically find similar records, even with very large databases.

## important links
* documentation: https://docs.dedupe.io/
* repository: https://github.com/dedupeio/dedupe
* issues: https://github.com/dedupeio/dedupe/issues
* mailing list: https://groups.google.com/forum/#!forum/open-source-deduplication
* examples: https://github.com/dedupeio/dedupe-examples

## dedupe library consulting

if you or your organization would like professional assistance in working with the dedupe library, dedupe.io llc offers consulting services. [read more about pricing and available services here](https://dedupe.io/pricing/#consulting).

## tools built with dedupe

### [dedupe.io](https://dedupe.io/)
a cloud service powered by the dedupe library for de-duplicating and finding matches in your data. it provides a step-by-step wizard for uploading your data, setting up a model, training, clustering and reviewing the results.

[dedupe.io](https://dedupe.io/) also supports record linkage across data sources and continuous matching and training through an [api](https://apidocs.dedupe.io/en/latest/).

for more, see the [dedupe.io product site](https://dedupe.io/), [tutorials on how to use it](https://dedupe.io/tutorial/intro-to-dedupe-io.html), and [differences between it and the dedupe library](https://dedupe.io/documentation/should-i-use-dedupeio-or-the-dedupe-python-library.html).

dedupe is well adopted by the python community. check out this [blogpost](https://medium.com/district-data-labs/basics-of-entity-resolution-with-python-and-dedupe-bc87440b64d4),
a youtube video on how to use [dedupe with python](https://youtu.be/mcstwxeurha) and a youtube video on how to apply [dedupe at scale using spark](https://youtu.be/q9hpuymiwje?t=2704).


### [csvdedupe](https://github.com/dedupeio/csvdedupe)
command line tool for de-duplicating and [linking](https://github.com/dedupeio/csvdedupe#csvlink-usage) csv files. read about it on [source knight-mozilla opennews](https://source.opennews.org/en-us/articles/introducing-cvsdedupe/).

## installation

### using dedupe

if you only want to use dedupe, install it this way:

```bash
pip install dedupe
```

familiarize yourself with [dedupe's api](https://docs.dedupe.io/en/latest/api-documentation.html), and get started on your project. need inspiration? have a look at [some examples](https://github.com/dedupeio/dedupe-examples).

### developing dedupe

we recommend using [virtualenv](http://virtualenv.readthedocs.org/en/latest/virtualenv.html) and [virtualenvwrapper](http://virtualenvwrapper.readthedocs.org/en/latest/install.html) for working in a virtualized development environment. [read how to set up virtualenv](http://docs.python-guide.org/en/latest/dev/virtualenvs/).

once you have virtualenvwrapper set up,

```bash
mkvirtualenv dedupe
git clone https://github.com/dedupeio/dedupe.git
cd dedupe
pip install -e . --config-settings editable_mode=compat
pip install -r requirements.txt
```

if these tests pass, then everything should have been installed correctly!

```bash
pytest
```

afterwards, whenever you want to work on dedupe,

```bash
workon dedupe
```

## testing
unit tests of core dedupe functions
```bash
pytest
```

#### test using canonical dataset from bilenko's research
  
using deduplication
```bash
python -m pip install -e ./benchmarks
python benchmarks/benchmarks/canonical.py
```

using record linkage
```bash
python -m pip install -e ./benchmarks
python benchmarks/benchmarks/canonical_matching.py
```


## team

* forest gregg, datamade
* derek eder, datamade

## credits

dedupe is based on mikhail yuryevich bilenko's ph.d. dissertation: [*learnable similarity functions and their application to record linkage and clustering*](http://www.cs.utexas.edu/~ml/papers/marlin-dissertation-06.pdf).

## errors / bugs

if something is not behaving intuitively, it is a bug, and should be reported.
[report it here](https://github.com/dedupeio/dedupe/issues)


## note on patches/pull requests
 
* fork the project.
* make your feature addition or bug fix.
* send us a pull request. bonus points for topic branches.

## copyright

copyright (c) 2022 forest gregg and derek eder. released under the [mit license](https://github.com/dedupeio/dedupe/blob/main/license).

third-party copyright in this distribution is noted where applicable.

## citing dedupe
if you use dedupe in an academic work, please give this citation:

forest gregg and derek eder. 2022. dedupe. https://github.com/dedupeio/dedupe.
"
"Snips NLU","snips nlu
=========

.. image:: https://travis-ci.org/snipsco/snips-nlu.svg?branch=master
   :target: https://travis-ci.org/snipsco/snips-nlu

.. image:: https://ci.appveyor.com/api/projects/status/github/snipsco/snips-nlu?branch=master&svg=true
   :target: https://ci.appveyor.com/project/snipsco/snips-nlu

.. image:: https://img.shields.io/pypi/v/snips-nlu.svg?branch=master
   :target: https://pypi.python.org/pypi/snips-nlu

.. image:: https://img.shields.io/pypi/pyversions/snips-nlu.svg?branch=master
   :target: https://pypi.python.org/pypi/snips-nlu

.. image:: https://codecov.io/gh/snipsco/snips-nlu/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/snipsco/snips-nlu

.. image:: https://img.shields.io/twitter/url/http/shields.io.svg?style=social
   :target: https://twitter.com/intent/tweet?text=extract%20meaning%20from%20text%20with%20snips%20nlu,%20an%20open%20source%20library%20written%20in%20python%20and%20rust&url=https://github.com/snipsco/snips-nlu&via=snips&hashtags=nlu,nlp,machinelearning,python,rustlang


`snips nlu <https://snips-nlu.readthedocs.io>`_ (natural language understanding) is a python library that allows to extract structured information from sentences written in natural language.

summary
-------

- `what is snips nlu about ?`_
- `getting started`_

  - `system requirements`_
  - `installation`_
  - `language resources`_
- `api usage`_

  - `sample code`_
  - `command line interface`_
- `sample datasets`_
- `benchmarks`_
- `documentation`_
- `citing snips nlu`_
- `faq & community`_
- `related content`_
- `how do i contribute ?`_
- `licence`_

what is snips nlu about ?
-------------------------

behind every chatbot and voice assistant lies a common piece of technology: natural language understanding (nlu). anytime a user interacts with an ai using natural language, their words need to be translated into a machine-readable description of what they meant.

the nlu engine first detects what the intention of the user is (a.k.a. `intent`_), then extracts the parameters (called `slots`_) of the query. the developer can then use this to determine the appropriate action or response.


let‚Äôs take an example to illustrate this, and consider the following sentence:

.. code-block:: text

    ""what will be the weather in paris at 9pm?""

properly trained, the snips nlu engine will be able to extract structured data such as:

.. code-block:: json

    {
       ""intent"": {
          ""intentname"": ""searchweatherforecast"",
          ""probability"": 0.95
       },
       ""slots"": [
          {
             ""value"": ""paris"",
             ""entity"": ""locality"",
             ""slotname"": ""forecast_locality""
          },
          {
             ""value"": {
                ""kind"": ""instanttime"",
                ""value"": ""2018-02-08 20:00:00 +00:00""
             },
             ""entity"": ""snips/datetime"",
             ""slotname"": ""forecast_start_datetime""
          }
       ]
    }

in this case, the identified intent is ``searchweatherforecast`` and two slots were extracted, a locality and a datetime. as you can see, snips nlu does an extra step on top of extracting entities: it resolves them. the extracted datetime value has indeed been converted into a handy iso format.

check out our `blog post`_ to get more details about why we built snips nlu and how it works under the hood. we also published a `paper on arxiv`_, presenting the machine learning architecture of the snips voice platform.


getting started
---------------

-------------------
system requirements
-------------------

- python 2.7 or python >= 3.5
- ram: snips nlu will typically use between 100mb and 200mb of ram, depending on the language and the size of the dataset.


------------
installation
------------

.. code-block:: python

    pip install snips-nlu

we currently have pre-built binaries (wheels) for ``snips-nlu`` and its
dependencies for macos (10.11 and later), linux x86_64 and windows.

for any other architecture/os `snips-nlu` can be installed from the source
distribution. to do so, `rust <https://www.rust-lang.org/en-us/install.html>`_
and `setuptools_rust <https://github.com/pyo3/setuptools-rust>`_ must be
installed before running the ``pip install snips-nlu`` command.

------------------
language resources
------------------

snips nlu relies on `external language resources`_ that must be downloaded before the
library can be used. you can fetch resources for a specific language by
running the following command:

.. code-block:: sh

    python -m snips_nlu download en

or simply:

.. code-block:: sh

    snips-nlu download en


the list of supported languages is available at 
`this address <https://snips-nlu.readthedocs.io/en/latest/languages.html>`_.

api usage
---------

----------------------
command line interface
----------------------

the easiest way to test the abilities of this library is through the command line interface.

first, start by training the nlu with one of the `sample datasets`_:

.. code-block:: sh
    
    snips-nlu train path/to/dataset.json path/to/output_trained_engine

where ``path/to/dataset.json`` is the path to the dataset which will be used during training, and ``path/to/output_trained_engine`` is the location where the trained engine should be persisted once the training is done.

after that, you can start parsing sentences interactively by running:

.. code-block:: sh
    
    snips-nlu parse path/to/trained_engine

where ``path/to/trained_engine`` corresponds to the location where you have stored the trained engine during the previous step.


-----------
sample code
-----------

here is a sample code that you can run on your machine after having
installed `snips-nlu`, fetched the english resources and downloaded one of the `sample datasets`_:

.. code-block:: python

    >>> from __future__ import unicode_literals, print_function
    >>> import io
    >>> import json
    >>> from snips_nlu import snipsnluengine
    >>> from snips_nlu.default_configs import config_en
    >>> with io.open(""sample_datasets/lights_dataset.json"") as f:
    ...     sample_dataset = json.load(f)
    >>> nlu_engine = snipsnluengine(config=config_en)
    >>> nlu_engine = nlu_engine.fit(sample_dataset)
    >>> text = ""please turn the light on in the kitchen""
    >>> parsing = nlu_engine.parse(text)
    >>> parsing[""intent""][""intentname""]
    'turnlighton'


what it does is training an nlu engine on a sample weather dataset and parsing
a weather query.

sample datasets
---------------

here is a list of some datasets that can be used to train a snips nlu engine:

- `lights dataset <sample_datasets/lights_dataset.json>`_: ""turn on the lights in the kitchen"", ""set the light to red in the bedroom""
- `beverage dataset <sample_datasets/beverage_dataset.json>`_: ""prepare two cups of cappucino"", ""make me a cup of tea""
- `flights dataset <sample_datasets/flights_dataset.json>`_: ""book me a flight to go to boston this weekend"", ""book me some tickets from istanbul to moscow in three days""

benchmarks
----------

in january 2018, we reproduced an `academic benchmark`_ which was published during the summer 2017. in this article, authors assessed the performance of api.ai (now dialogflow, google), luis.ai (microsoft), ibm watson, and `rasa nlu`_. for fairness, we used an updated version of rasa nlu and compared it to the latest version of snips nlu (both in dark blue).

.. image:: .img/benchmarks.png

in the figure above, `f1 scores`_ of both intent classification and slot filling were computed for several nlu providers, and averaged across the three datasets used in the academic benchmark mentionned before. all the underlying results can be found `here <https://github.com/snipsco/nlu-benchmark/tree/master/2018-01-braum-et-al-extension>`_.


documentation
-------------

to find out how to use snips nlu please refer to the `package documentation <https://snips-nlu.readthedocs.io>`_, it will provide you with a step-by-step guide on how to setup and use this library.

citing snips nlu
----------------

please cite the following paper when using snips nlu:

.. code-block:: bibtex

   @article{coucke2018snips,
     title   = {snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces},
     author  = {coucke, alice and saade, alaa and ball, adrien and bluche, th{\'e}odore and caulier, alexandre and leroy, david and doumouro, cl{\'e}ment and gisselbrecht, thibault and caltagirone, francesco and lavril, thibaut and others},
     journal = {arxiv preprint arxiv:1805.10190},
     pages   = {12--16},
     year    = {2018}
   }

faq & community
---------------

please join the `forum`_ to ask your questions and get feedback from the community.

related content
---------------
* `what is snips about ? <https://snips.ai/>`_
* snips nlu open sourcing `blog post`_
* `snips voice platform paper (arxiv) <https://arxiv.org/abs/1805.10190>`_
* `snips nlu language resources <https://github.com/snipsco/snips-nlu-language-resources>`_
* `bug tracker <https://github.com/snipsco/snips-nlu/issues>`_
* `snips nlu rust <https://github.com/snipsco/snips-nlu-rs>`_: rust inference pipeline implementation and bindings (c, swift, kotlin, python)
* `rustling <https://github.com/snipsco/rustling-ontology>`_: snips nlu builtin entities parser


how do i contribute ?
---------------------

please see the `contribution guidelines <contributing.rst>`_.

licence
-------

this library is provided by `snips <https://www.snips.ai>`_ as open source software. see `license <license>`_ for more information.


geonames licence
----------------

the `snips/city`, `snips/country` and `snips/region` builtin entities rely on
software from geonames, which is made available under a creative commons attribution 4.0
license international. for the license and warranties for geonames please refer to: https://creativecommons.org/licenses/by/4.0/legalcode.


.. _external language resources: https://github.com/snipsco/snips-nlu-language-resources
.. _forum: https://forum.snips.ai/
.. _blog post: https://medium.com/snips-ai/an-introduction-to-snips-nlu-the-open-source-library-behind-snips-embedded-voice-platform-b12b1a60a41a
.. _paper on arxiv: https://arxiv.org/abs/1805.10190
.. _academic benchmark: http://workshop.colips.org/wochat/@sigdial2017/documents/sigdial22.pdf
.. _rasa nlu: https://nlu.rasa.ai/
.. _f1 scores: https://en.wikipedia.org/wiki/f1_score
.. _intent: https://snips-nlu.readthedocs.io/en/latest/data_model.html#intent
.. _slots: https://snips-nlu.readthedocs.io/en/latest/data_model.html#slot
"
"RexMex","![version](https://badge.fury.io/py/rexmex.svg?style=plastic)
[![license](https://img.shields.io/badge/license-apache_2.0-blue.svg)](https://opensource.org/licenses/apache-2.0)
[![repo size](https://img.shields.io/github/repo-size/astrazeneca/rexmex.svg)](https://github.com/astrazeneca/rexmex/archive/master.zip)
[![build badge](https://github.com/astrazeneca/rexmex/workflows/ci/badge.svg)](https://github.com/astrazeneca/rexmex/actions?query=workflow%3aci)
[![codecov](https://codecov.io/gh/astrazeneca/rexmex/branch/main/graph/badge.svg?token=cygaejra0z)](https://codecov.io/gh/astrazeneca/rexmex)

<p align=""center"">
  <img width=""90%"" src=""https://github.com/astrazeneca/rexmex/blob/main/rexmex_small.jpg?raw=true?sanitize=true"" />
</p>

--------------------------------------------------------------------------------

**rexmex** is recommender system evaluation metric library.

please look at the **[documentation](https://rexmex.readthedocs.io/en/latest/)** and **[external resources](https://rexmex.readthedocs.io/en/latest/notes/resources.html)**.

**rexmex** consists of utilities for recommender system evaluation. first, it provides a comprehensive collection of metrics for the evaluation of recommender systems. second, it includes a variety of methods for reporting and plotting the performance results. implemented metrics cover a range of well-known metrics and newly proposed metrics from data mining ([icdm](http://icdm2019.bigke.org/), [cikm](http://www.cikm2019.net/), [kdd](https://www.kdd.org/kdd2020/)) conferences and prominent journals.

**citing**

if you find *rexmex* useful in your research, please consider adding the following citation:

```bibtex
@inproceedings{rexmex,
       title = {{rexmex: a general purpose recommender metrics library for fair evaluation.}},
       author = {benedek rozemberczki and sebastian nilsson and piotr grabowski and charles tapley hoyt and gavin edwards},
       year = {2021},
}
```
--------------------------------------------------------------------------------

**an introductory example**

the following example loads a synthetic dataset which has the mandatory `y_true` and `y_score` keys.  the dataset has binary labels and predictied probability scores. we read the dataset and define a defult `classificationmetric` instance for the evaluation of the predictions. using this metric set we create a score card and get the predictive performance metrics.

```python
from rexmex import classificationmetricset, datasetreader, scorecard

reader = datasetreader()
scores = reader.read_dataset()

metric_set = classificationmetricset()

score_card = scorecard(metric_set)

report = score_card.get_performance_metrics(scores[""y_true""], scores[""y_score""])
```

--------------------------------------------------------------------------------

**an advanced example**

the following more advanced example loads the same synthetic dataset which has the `source_id`, `target_id`, `source_group` and `target group` keys besides the mandatory `y_true` and `y_score`.   using the `source_group` key  we group the predictions and return a performance metric report.

```python
from rexmex import classificationmetricset, datasetreader, scorecard

reader = datasetreader()
scores = reader.read_dataset()

metric_set = classificationmetricset()

score_card = scorecard(metric_set)

report = score_card.generate_report(scores, grouping=[""source_group""])
```

--------------------------------------------------------------------------------

**scorecard**

a **rexmex** score card allows the reporting of recommender system performance metrics, plotting the performance metrics and saving those. our framework provides 7 rating, 38 classification, 18 ranking, and 2 coverage metrics.

**metric sets**

metric sets allow the users to calculate a range of evaluation metrics for a label - predicted label vector pair. we provide a general `metricset` class and specialized metric sets with pre-set metrics have the following general categories:

- **ranking**
- **rating**
- **classification**
- **coverage**

--------------------------------------------------------------------------------

**ranking metric set**

* **[normalized distance based performance measure (ndpm)](https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28sici%291097-4571%28199503%2946%3a2%3c133%3a%3aaid-asi6%3e3.0.co%3b2-z)**
* **[discounted cumulative gain (dcg)](https://en.wikipedia.org/wiki/discounted_cumulative_gain)**
* **[normalized discounted cumulative gain (ndcg)](https://en.wikipedia.org/wiki/discounted_cumulative_gain)**
* **[reciprocal rank](https://en.wikipedia.org/wiki/mean_reciprocal_rank)**

<details>
<summary><b>expand to see all ranking metrics in the metric set.</b></summary>

* **[mean reciprocal rank (mrr)](https://en.wikipedia.org/wiki/mean_reciprocal_rank)**
* **[spearmanns rho](https://en.wikipedia.org/wiki/spearman%27s_rank_correlation_coefficient)**
* **[kendall tau](https://en.wikipedia.org/wiki/kendall_rank_correlation_coefficient)**
* **[hits@k](https://en.wikipedia.org/wiki/evaluation_measures_(information_retrieval))**
* **[novelty](https://www.sciencedirect.com/science/article/pii/s163107051930043x)**
* **[average recall @ k](https://en.wikipedia.org/wiki/evaluation_measures_(information_retrieval))**
* **[mean average recall @ k](https://en.wikipedia.org/wiki/evaluation_measures_(information_retrieval))**
* **[average precision @ k](https://en.wikipedia.org/wiki/evaluation_measures_(information_retrieval))**
* **[mean average precision @ k](https://en.wikipedia.org/wiki/evaluation_measures_(information_retrieval))**
* **[personalisation](http://www.mavir.net/docs/tfm-vargas-sandoval.pdf)**
* **[intra list similarity](http://www.mavir.net/docs/tfm-vargas-sandoval.pdf)**

</details>

--------------------------------------------------------------------------------

**rating metric set**

these metrics assume that items are scored explicitly and ratings are predicted by a regression model. 

* **[mean squared error (mse)](https://en.wikipedia.org/wiki/mean_squared_error)**
* **[root mean squared error (rmse)](https://en.wikipedia.org/wiki/mean_squared_error)**
* **[mean absolute error (mae)](https://en.wikipedia.org/wiki/mean_absolute_error)**
* **[mean absolute percentage error (mape)](https://en.wikipedia.org/wiki/mean_absolute_percentage_error)**


<details>
<summary><b>expand to see all rating metrics in the metric set.</b></summary>

* **[symmetric mean absolute percentage error (smape)](https://en.wikipedia.org/wiki/symmetric_mean_absolute_percentage_error)**
* **[pearson correlation](https://en.wikipedia.org/wiki/pearson_correlation_coefficient)**
* **[coefficient of determination](https://en.wikipedia.org/wiki/coefficient_of_determination)**

</details>

--------------------------------------------------------------------------------

**classification metric set**

these metrics assume that the items are scored with raw probabilities (these can be binarized).

* **[precision (or positive predictive value)](https://en.wikipedia.org/wiki/precision_and_recall)**
* **[recall (sensitivity, hit rate, or true positive rate)](https://en.wikipedia.org/wiki/precision_and_recall)**
* **[area under the precision recall curve (auprc)](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210x.13140)**
* **[area under the receiver operating characteristic (auroc)](https://en.wikipedia.org/wiki/receiver_operating_characteristic)**

<details>
<summary><b>expand to see all classification metrics in the metric set.</b></summary>

* **[f-1 score](https://en.wikipedia.org/wiki/f-score)**
* **[average precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)**
* **[specificty (selectivity or true negative rate )](https://en.wikipedia.org/wiki/precision_and_recall)**
* **[matthew's correlation](https://en.wikipedia.org/wiki/precision_and_recall)**
* **[accuracy](https://en.wikipedia.org/wiki/precision_and_recall)**
* **[balanced accuracy](https://en.wikipedia.org/wiki/precision_and_recall)**
* **[fowlkes-mallows index](https://en.wikipedia.org/wiki/precision_and_recall)**

</details>

--------------------------------------------------------------------------------

**coverage metric set**

these metrics measure how well the recommender system covers the available items in the catalog and possible users. 
in other words measure the diversity of predictions.

* **[item coverage](https://www.bgu.ac.il/~shanigu/publications/evaluationmetrics.17.pdf)**
* **[user coverage](https://www.bgu.ac.il/~shanigu/publications/evaluationmetrics.17.pdf)**


--------------------------------------------------------------------------------
**documentation and reporting issues**

head over to our [documentation](https://rexmex.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.

if you notice anything unexpected, please open an [issue](https://github.com/astrazeneca/rexmex/issues) and let us know. if you are missing a specific method, feel free to open a [feature request](https://github.com/astrazeneca/rexmex/issues).
we are motivated to constantly make rexmex even better.

--------------------------------------------------------------------------------

**installation via the command line**

rexmex can be installed with the following command after the repo is cloned.

```sh
$ pip install .
```

use `-e/--editable` when developing.

**installation via pip**

rexmex can be installed with the following pip command.

```sh
$ pip install rexmex
```

as we create new releases frequently, upgrading the package casually might be beneficial.

```sh
$ pip install rexmex --upgrade
```

--------------------------------------------------------------------------------

**running tests**

tests can be run with `tox` with the following:

```sh
$ pip install tox
$ tox -e py
```

--------------------------------------------------------------------------------

**citation**

if you use rexmex in a scientific publication, we would appreciate citations. please see github's built-in citation tool.



--------------------------------------------------------------------------------

**license**

- [apache-2.0 license](https://github.com/az-ai/rexmex/blob/master/license)
"
"ChemicalX","[pypi-image]: https://badge.fury.io/py/chemicalx.svg
[pypi-url]: https://pypi.python.org/pypi/chemicalx
[size-image]: https://img.shields.io/github/repo-size/astrazeneca/chemicalx.svg
[size-url]: https://github.com/astrazeneca/chemicalx/archive/main.zip
[build-image]: https://github.com/astrazeneca/chemicalx/workflows/ci/badge.svg
[build-url]: https://github.com/astrazeneca/chemicalx/actions?query=workflow%3aci
[docs-image]: https://readthedocs.org/projects/chemicalx/badge/?version=latest
[docs-url]: https://chemicalx.readthedocs.io/en/latest/?badge=latest
[coverage-image]: https://codecov.io/gh/astrazeneca/chemicalx/branch/main/graph/badge.svg
[coverage-url]: https://codecov.io/github/astrazeneca/chemicalx?branch=main

<p align=""center"">
  <img width=""90%"" src=""https://github.com/astrazeneca/chemicalx/blob/main/images/chemicalx_logo.jpg?sanitize=true"" />
</p>

--------------------------------------------------------------------------------

[![pypi version][pypi-image]][pypi-url]
[![docs status][docs-image]][docs-url]
[![code coverage][coverage-image]][coverage-url]
[![build status][build-image]][build-url]
[![doi](https://img.shields.io/badge/doi-10.1145/3534678.3539023-blue.svg)](https://bioregistry.io/doi:10.1145/3534678.3539023)

**[documentation](https://chemicalx.readthedocs.io)** | **[external resources](https://chemicalx.readthedocs.io/en/latest/notes/resources.html)** | **[datasets](https://chemicalx.readthedocs.io/en/latest/notes/introduction.html#datasets)** | **[examples](https://github.com/astrazeneca/chemicalx/tree/main/examples)** 

*chemicalx* is a deep learning library for drug-drug interaction, polypharmacy side effect, and synergy prediction. the library consists of data loaders and integrated benchmark datasets. it also includes state-of-the-art deep neural network architectures that solve the [drug pair scoring task](https://arxiv.org/pdf/2111.02916v4.pdf). implemented methods cover traditional smiles string based techniques and neural message passing based models.

--------------------------------------------------------------------------------

**citing**


if you find *chemicalx* and the new datasets useful in your research, please consider adding the following citation:

```bibtex
@inproceedings{10.1145/3534678.3539023,
  author = {rozemberczki, benedek and hoyt, charles tapley and gogleva, anna and grabowski, piotr and karis, klas and lamov, andrej and nikolov, andriy and nilsson, sebastian and ughetto, michael and wang, yu and derr, tyler and gyori, benjamin m.},
  title = {chemicalx: a deep learning library for drug pair scoring},
  year = {2022},
  isbn = {9781450393850},
  publisher = {association for computing machinery},
  address = {new york, ny, usa},
  url = {https://doi.org/10.1145/3534678.3539023},
  doi = {10.1145/3534678.3539023},
  booktitle = {proceedings of the 28th acm sigkdd conference on knowledge discovery and data mining},
  pages = {3819‚Äì3828},
  numpages = {10},
  keywords = {chemistry, neural networks, deep learning},
  location = {washington dc, usa},
  series = {kdd '22}
}
```
--------------------------------------------------------------------------------

**drug pair scoring explained**

our framework solves the [drug pair scoring task](https://arxiv.org/abs/2111.02916) of computational chemistry. in this task a machine learning model has to predict the outcome of administering two drugs together in a biological or chemical context. deep learning models which solve this task have an architecture with two distinctive parts:

1. a drug encoder layer which takes a pair of drugs as an input (blue and red drugs below).
2. a head layer which outputs scores in the administration context - polypharmacy in our explanatory figure.

<p align=""center"">
  <img width=""90%"" src=""https://github.com/astrazeneca/chemicalx/blob/main/images/pair_scoring.jpg?sanitize=true"" />
</p>


**getting started**

the api of `chemicalx` provides a high-level function for training and evaluating models
that's heavily influenced by the [pykeen](https://github.com/pykeen/pykeen/)
training and evaluation pipeline:

```python
from chemicalx import pipeline
from chemicalx.models import deepsynergy
from chemicalx.data import drugcombdb

model = deepsynergy(context_channels=112, drug_channels=256)
dataset = drugcombdb()

results = pipeline(
    dataset=dataset,
    model=model,
    # data arguments
    batch_size=5120,
    context_features=true,
    drug_features=true,
    drug_molecules=false,
    # training arguments
    epochs=100,
)

# outputs information about the auc-roc, etc. to the console.
results.summarize()

# save the model, losses, evaluation, and other metadata.
results.save(""~/test_results/"")
```

--------------------------------------------------------------------------------

**case study tutorials**

we provide in-depth case study like tutorials in the¬†[documentation](https://chemicalx.readthedocs.io/en/latest/), each covers an aspect of chemicalx‚Äôs functionality.

--------------------------------------------------------------------------------

**methods included**

in detail, the following drug pair scoring models were implemented.

**2018**

* **[deepddi](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.deepddi.deepddi)** from [deep learning improves prediction of drug‚Äìdrug and drug‚Äìfood interactions](https://www.pnas.org/content/115/18/e4304) (pnas)

* **[deepsynergy](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.deepsynergy.deepsynergy)** from [deepsynergy: predicting anti-cancer drug synergy with deep learning](https://academic.oup.com/bioinformatics/article/34/9/1538/4747884) (bioinformatics)

**2019**

* **[mr-gnn](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.mrgnn.mrgnn)** from [mr-gnn: multi-resolution and dual graph neural network for predicting structured entity interactions](https://arxiv.org/abs/1905.09558) (ijcai)

* **[mhcaddi](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.mhcaddi.mhcaddi)** from [drug-drug adverse effect prediction with graph co-attention](https://arxiv.org/pdf/1905.00534v1.pdf) (icml)

**2020**

* **[caster](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.caster.caster)** from [caster: predicting drug interactions with chemical substructure representation](https://arxiv.org/abs/1911.06446) (aaai)

* **[ssi-ddi](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.ssiddi.ssiddi)** from [ssi‚Äìddi: substructure‚Äìsubstructure interactions for drug‚Äìdrug interaction prediction](https://academic.oup.com/bib/article-abstract/22/6/bbab133/6265181) (briefings in bioinformatics)

* **[epgcn-ds](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.epgcnds.epgcnds)** from [structure-based drug-drug interaction detection via expressive graph convolutional networks and deep sets](https://ojs.aaai.org/index.php/aaai/article/view/7236) (aaai)

* **[deepdrug](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.deepdrug.deepdrug)** from [deepdrug: a general graph-based deep learning framework for drug relation prediction](https://europepmc.org/article/ppr/ppr236757) (pmc)

* **[gcn-bmp](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.gcnbmp.gcnbmp)** from [gcn-bmp: investigating graph representation learning for ddi prediction task](https://www.sciencedirect.com/science/article/pii/s1046202320300608) (methods)

**2021**

* **[deepdds](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.deepdds.deepdds)** from [deepdds: deep graph neural network with attention mechanism to predict synergistic drug combinations](https://arxiv.org/abs/2107.02467) (briefings in bioinformatics)

* **[matchmaker](https://chemicalx.readthedocs.io/en/latest/modules/root.html#chemicalx.models.matchmaker.matchmaker)** from [matchmaker: a deep learning framework for drug synergy prediction](https://pubmed.ncbi.nlm.nih.gov/34086576/) (acm tcbb)

--------------------------------------------------------------------------------

head over to our [documentation](https://chemicalx.readthedocs.io) to find out more about installation, creation of datasets and a full list of implemented methods and available datasets.
for a quick start, check out the [examples](https://chemicalx.readthedocs.io) in the `examples/` directory.

if you notice anything unexpected, please open an [issue](github.com/astrazeneca/chemicalx/issues). if you are missing a specific method, feel free to open a [feature request](https://github.com/astrazeneca/chemicalx/issues).


--------------------------------------------------------------------------------

**installation**

**pytorch 1.10.0**

to install for pytorch 1.10.0, simply run

```sh
pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+${cuda}.html
pip install torchdrug
pip install chemicalx
```

where `${cuda}` should be replaced by either `cpu`, `cu102`, or `cu111` depending on your pytorch installation.

|             | `cpu` | `cu102` | `cu111` |
|-------------|-------|---------|---------|
| **linux**   | ‚úÖ    | ‚úÖ      | ‚úÖ      |
| **windows** | ‚úÖ    | ‚úÖ      | ‚úÖ      |
| **macos**   | ‚úÖ    |         |         |


--------------------------------------------------------------------------------

**running tests**

```
$ tox -e py
```
--------------------------------------------------------------------------------

**license**

- [apache 2.0 license](https://github.com/astrazeneca/chemicalx/blob/main/license)
"
"Shapley","[pypi-image]: https://badge.fury.io/py/shapley.svg
[pypi-url]: https://pypi.python.org/pypi/shapley
[size-image]: https://img.shields.io/github/repo-size/benedekrozemberczki/shapley.svg
[size-url]: https://github.com/benedekrozemberczki/shapley/archive/master.zip
[build-image]: https://github.com/benedekrozemberczki/shapley/workflows/ci/badge.svg
[build-url]: https://github.com/benedekrozemberczki/shapley/actions?query=workflow%3aci
[docs-image]: https://readthedocs.org/projects/shapley/badge/?version=latest
[docs-url]: https://shapley.readthedocs.io/en/latest/?badge=latest
[coverage-image]: https://codecov.io/gh/benedekrozemberczki/shapley/branch/master/graph/badge.svg
[coverage-url]: https://codecov.io/github/benedekrozemberczki/shapley?branch=master
[arxiv-image]: https://img.shields.io/badge/arxiv-2101.02153-orange.svg
[arxiv-url]: https://arxiv.org/abs/2101.02153

<p align=""center"">
  <img width=""90%"" src=""https://github.com/benedekrozemberczki/shapley/raw/master/shapley.jpg?sanitize=true"" />
</p>

[![pypi version][pypi-image]][pypi-url]
[![docs status][docs-image]][docs-url]
[![repo size][size-image]][size-url]
[![code coverage][coverage-image]][coverage-url]
[![build status][build-image]][build-url]
[![arxiv][arxiv-image]][arxiv-url]

**[documentation](https://shapley.readthedocs.io)** | **[external resources](https://shapley.readthedocs.io/en/latest/notes/resources.html)**
| **[research paper](https://arxiv.org/abs/2101.02153)**

*shapley* is a python library for evaluating binary classifiers in a machine learning ensemble.

the library consists of various methods to compute (approximate) the shapley value of players (models) in weighted voting games (ensemble games) - a class of transferable utility cooperative games. we covered the exact enumeration based computation and various widely know approximation methods from economics and computer science research papers. there are also functionalities to identify the heterogeneity of the player pool based on the [shapley entropy](https://arxiv.org/abs/2101.02153). in addition, the framework comes with a [detailed documentation](https://shapley.readthedocs.io/en/latest/), an intuitive [tutorial](https://shapley.readthedocs.io/en/latest/notes/introduction.html), 100% test coverage, and illustrative toy [examples](https://github.com/benedekrozemberczki/shapley/tree/master/examples).

-------------------------------------------------------

**citing**


if you find *shapley* useful in your research please consider adding the following citation:

```bibtex
@inproceedings{rozemberczki2021shapley,
      title = {{the shapley value of classifiers in ensemble games}}, 
      author = {benedek rozemberczki and rik sarkar},
      year = {2021},
      booktitle={proceedings of the 30th acm international conference on information and knowledge management},
      pages = {1558‚Äì1567},
}
```

--------------------------------------------------------------

**a simple example**

shapley makes solving voting games quite easy - see the accompanying [tutorial](https://shapley.readthedocs.io/en/latest/notes/introduction.html#applications). for example, this is all it takes to solve a weighted voting game with defined on the fly with permutation sampling:

```python
import numpy as np
from shapley import permutationsampler

w = np.random.uniform(0, 1, (1, 7))
w = w/w.sum()
q = 0.5

solver = permutationsampler()
solver.solve_game(w, q)
shapley_values = solver.get_solution()
```
----------------------------------------------------------------------------------

**methods included**

in detail, the following methods can be used.


* **[expected marginal contribution approximation](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.expected_marginal_contributions.expectedmarginalcontributions)** from fatima *et al.*: [a linear approximation method for the shapley value](https://www.sciencedirect.com/science/article/pii/s0004370208000696)

* **[multilinear extension](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.multilinear_extension.multilinearextension)** from owen: [multilinear extensions of games](https://www.jstor.org/stable/2661445?seq=1#metadata_info_tab_contents)

* **[monte carlo permutation sampling](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.permutation_sampler.permutationsampler)** from maleki *et al.*: [bounding the estimation error of sampling-based shapley value approximation](https://arxiv.org/abs/1306.4265)

* **[exact enumeration](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.exact_enumeration.exactenumeration)** from shapley: [a value for n-person games](https://www.rand.org/pubs/papers/p0295.html)

--------------------------------------------------------------------------------


head over to our [documentation](https://shapley.readthedocs.io) to find out more about installation, creation of datasets and a full list of implemented methods and available datasets.
for a quick start, check out the [examples](https://github.com/benedekrozemberczki/shapley/tree/master/examples) in the `examples/` directory.

if you notice anything unexpected, please open an [issue](https://benedekrozemberczki/shapley/issues). if you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/shapley/issues).

--------------------------------------------------------------------------------

**installation**

```
$ pip install shapley
```

**running tests**

```
$ python setup.py test
```
----------------------------------------------------------------------------------

**running examples**

```
$ cd examples
$ python permutation_sampler_example.py
```

----------------------------------------------------------------------------------

**license**

- [mit license](https://github.com/benedekrozemberczki/shapley/blob/master/license)
"
"PyTorch Geometric Temporal","[pypi-image]: https://badge.fury.io/py/torch-geometric-temporal.svg
[pypi-url]: https://pypi.python.org/pypi/torch-geometric-temporal
[size-image]: https://img.shields.io/github/repo-size/benedekrozemberczki/pytorch_geometric_temporal.svg
[size-url]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/archive/master.zip
[build-image]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/workflows/ci/badge.svg
[build-url]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/actions?query=workflow%3aci
[docs-image]: https://readthedocs.org/projects/pytorch-geometric-temporal/badge/?version=latest
[docs-url]: https://pytorch-geometric-temporal.readthedocs.io/en/latest/?badge=latest
[coverage-image]: https://codecov.io/gh/benedekrozemberczki/pytorch_geometric_temporal/branch/master/graph/badge.svg
[coverage-url]: https://codecov.io/github/benedekrozemberczki/pytorch_geometric_temporal?branch=master



<p align=""center"">
  <img width=""90%"" src=""https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/docs/source/_static/img/text_logo.jpg?sanitize=true"" />
</p>

-----------------------------------------------------

[![pypi version][pypi-image]][pypi-url]
[![docs status][docs-image]][docs-url]
[![code coverage][coverage-image]][coverage-url]
[![build status][build-image]][build-url]
[![arxiv](https://img.shields.io/badge/arxiv-2104.07788-orange.svg)](https://arxiv.org/abs/2104.07788)
[![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)

**[documentation](https://pytorch-geometric-temporal.readthedocs.io)** | **[external resources](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/resources.html)** | **[datasets](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#discrete-time-datasets)**

*pytorch geometric temporal* is a temporal (dynamic) extension library for [pytorch geometric](https://github.com/rusty1s/pytorch_geometric).

<p align=""justify"">the library consists of various dynamic and temporal geometric deep learning, embedding, and spatio-temporal regression methods from a variety of published research papers. moreover, it comes with an easy-to-use dataset loader, train-test splitter and temporal snaphot iterator for dynamic and temporal graphs. the framework naturally provides gpu support. it also comes with a number of benchmark datasets from the epidemological forecasting, sharing economy, energy production and web traffic management domains. finally, you can also create your own datasets.</p>

the package interfaces well with [pytorch lightning](https://pytorch-lightning.readthedocs.io) which allows training on cpus, single and multiple gpus out-of-the-box. take a look at this [introductory example](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/examples/recurrent/lightning_example.py) of using pytorch geometric temporal with pytorch lighning.

we also provide detailed examples for each of the [recurrent](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/examples/recurrent) models and [notebooks](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/notebooks) for the attention based ones.


--------------------------------------------------------------------------------

**case study tutorials**

we provide in-depth case study tutorials in the¬†[documentation](https://pytorch-geometric-temporal.readthedocs.io/en/latest/), each covers an aspect of pytorch geometric temporal‚Äôs functionality.

**incremental training**:¬†[epidemiological forecasting case study](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#epidemiological-forecasting)

**cumulative training**:¬†[web traffic management case study](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#web-traffic-prediction)

--------------------------------------------------------------------------------

**citing**


if you find *pytorch geometric temporal* and the new datasets useful in your research, please consider adding the following citation:

```bibtex
@inproceedings{rozemberczki2021pytorch,
               author = {benedek rozemberczki and paul scherer and yixuan he and george panagopoulos and alexander riedel and maria astefanoaei and oliver kiss and ferenc beres and guzman lopez and nicolas collignon and rik sarkar},
               title = {{pytorch geometric temporal: spatiotemporal signal processing with neural machine learning models}},
               year = {2021},
               booktitle={proceedings of the 30th acm international conference on information and knowledge management},
               pages = {4564‚Äì4573},
}
```

--------------------------------------------------------------------------------

**a simple example**

pytorch geometric temporal makes implementing dynamic and temporal graph neural networks quite easy - see the accompanying [tutorial](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#applications). for example, this is all it takes to implement a recurrent graph convolutional network with two consecutive [graph convolutional gru](https://arxiv.org/abs/1612.07659) cells and a linear layer:

```python
import torch
import torch.nn.functional as f
from torch_geometric_temporal.nn.recurrent import gconvgru

class recurrentgcn(torch.nn.module):

    def __init__(self, node_features, num_classes):
        super(recurrentgcn, self).__init__()
        self.recurrent_1 = gconvgru(node_features, 32, 5)
        self.recurrent_2 = gconvgru(32, 16, 5)
        self.linear = torch.nn.linear(16, num_classes)

    def forward(self, x, edge_index, edge_weight):
        x = self.recurrent_1(x, edge_index, edge_weight)
        x = f.relu(x)
        x = f.dropout(x, training=self.training)
        x = self.recurrent_2(x, edge_index, edge_weight)
        x = f.relu(x)
        x = f.dropout(x, training=self.training)
        x = self.linear(x)
        return f.log_softmax(x, dim=1)
```
--------------------------------------------------------------------------------

**methods included**

in detail, the following temporal graph neural networks were implemented.


**recurrent graph convolutions**

* **[dcrnn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dcrnn.dcrnn)** from li *et al.*: [diffusion convolutional recurrent neural network: data-driven traffic forecasting](https://arxiv.org/abs/1707.01926) (iclr 2018)

* **[gconvgru](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gconv_gru.gconvgru)** from seo *et al.*: [structured sequence modeling with graph  convolutional recurrent networks](https://arxiv.org/abs/1612.07659) (iconip 2018)

* **[gconvlstm](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gconv_lstm.gconvlstm)** from seo *et al.*: [structured sequence modeling with graph  convolutional recurrent networks](https://arxiv.org/abs/1612.07659) (iconip 2018)

* **[gc-lstm](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gc_lstm.gclstm)** from chen *et al.*: [gc-lstm: graph convolution embedded lstm for dynamic link prediction](https://arxiv.org/abs/1812.04206) (corr 2018)

* **[lrgcn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.lrgcn.lrgcn)** from li *et al.*: [predicting path failure in time-evolving graphs](https://arxiv.org/abs/1905.03994) (kdd 2019)

* **[dygrencoder](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dygrae.dygrencoder)** from taheri *et al.*: [learning to represent the evolution of dynamic graphs with recurrent models](https://dl.acm.org/doi/10.1145/3308560.3316581)

* **[evolvegcnh](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.evolvegcnh.evolvegcnh)** from pareja *et al.*: [evolvegcn: evolving graph convolutional networks for dynamic graphs](https://arxiv.org/abs/1902.10191)

* **[evolvegcno](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.evolvegcno.evolvegcno)** from pareja *et al.*: [evolvegcn: evolving graph convolutional networks for dynamic graphs](https://arxiv.org/abs/1902.10191)

* **[t-gcn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.temporalgcn.tgcn)** from zhao *et al.*: [t-gcn: a temporal graph convolutional network for traffic prediction](https://arxiv.org/abs/1811.05320)

* **[a3t-gcn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.attentiontemporalgcn.a3tgcn)** from zhu *et al.*: [a3t-gcn: attention temporal graph convolutional network for traffic forecasting](https://arxiv.org/abs/2006.11583) 

* **[agcrn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.agcrn.agcrn)** from bai *et al.*: [adaptive graph convolutional recurrent network for traffic forecasting](https://arxiv.org/abs/2007.02842) (neurips 2020)

* **[mpnn lstm](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.mpnn_lstm.mpnnlstm)** from panagopoulos *et al.*: [transfer graph neural networks for pandemic forecasting](https://arxiv.org/abs/2009.08388) (aaai 2021)
  
**attention aggregated temporal graph convolutions**

* **[stgcn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.stgcn.stconv)** from yu *et al.*: [spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting](https://arxiv.org/abs/1709.04875) (ijcai 2018)

* **[astgcn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.astgcn.astgcn)** from guo *et al.*: [attention based spatial-temporal graph convolutional networks for traffic flow forecasting](https://ojs.aaai.org/index.php/aaai/article/view/3881) (aaai 2019)

* **[mstgcn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.mstgcn.mstgcn)** from guo *et al.*: [attention based spatial-temporal graph convolutional networks for traffic flow forecasting](https://ojs.aaai.org/index.php/aaai/article/view/3881) (aaai 2019)

* **[gman](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.gman.gman)** from zheng *et al.*: [gman: a graph multi-attention network for traffic prediction](https://arxiv.org/pdf/1911.08415.pdf) (aaai 2020)

* **[mtgnn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.mtgnn.mtgnn)** from wu *et al.*: [connecting the dots: multivariate time series forecasting with graph neural networks](https://arxiv.org/abs/2005.11650) (kdd 2020)

* **[2s-agcn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.tsagcn.aagcn)** from shi *et al.*: [two-stream adaptive graph convolutional networks for skeleton-based action recognition](https://arxiv.org/abs/1805.07694) (cvpr 2019)

* **[dnntsp](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.dnntsp.dnntsp)** from yu *et al.*: [predicting temporal sets with deep neural networks](https://dl.acm.org/doi/abs/10.1145/3394486.3403152) (kdd 2020)

**auxiliary graph convolutions**

* **[temporalconv](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.stgcn.temporalconv)** from yu *et al.*: [spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting](https://arxiv.org/abs/1709.04875) (ijcai 2018)

* **[dconv](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dcrnn.dconv)** from li *et al.*: [diffusion convolutional recurrent neural network: data-driven traffic forecasting](https://arxiv.org/abs/1707.01926) (iclr 2018)

* **[chebconvattention](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.astgcn.chebconvattention)** from guo *et al.*: [attention based spatial-temporal graph convolutional networks for traffic flow forecasting](https://ojs.aaai.org/index.php/aaai/article/view/3881) (aaai 2019)

* **[avwgcn](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.agcrn.avwgcn)** from bai *et al.*: [adaptive graph convolutional recurrent network for traffic forecasting](https://arxiv.org/abs/2007.02842) (neurips 2020)
  
--------------------------------------------------------------------------------


head over to our [documentation](https://pytorch-geometric-temporal.readthedocs.io) to find out more about installation, creation of datasets and a full list of implemented methods and available datasets.
for a quick start, check out the [examples](https://pytorch-geometric-temporal.readthedocs.io) in the `examples/` directory.

if you notice anything unexpected, please open an [issue](https://benedekrozemberczki/pytorch_geometric_temporal/issues). if you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/issues).


--------------------------------------------------------------------------------

**installation**

first install [pytorch][pytorch-install] and [pytorch-geometric][pyg-install]
and then run

```sh
pip install torch-geometric-temporal
```

[pytorch-install]: https://pytorch.org/get-started/locally/
[pyg-install]: https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html

--------------------------------------------------------------------------------

**running tests**

```
$ python -m pytest test
```
--------------------------------------------------------------------------------

**license**

- [mit license](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/license)
"
"Little Ball of Fur","![version](https://badge.fury.io/py/littleballoffur.svg?style=plastic) [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/littleballoffur.svg)](https://github.com/benedekrozemberczki/littleballoffur/archive/master.zip) [![arxiv](https://img.shields.io/badge/arxiv-2006.04311-orange.svg)](https://arxiv.org/abs/2006.04311) [![build badge](https://github.com/benedekrozemberczki/littleballoffur/workflows/ci/badge.svg)](https://github.com/benedekrozemberczki/littleballoffur/actions?query=workflow%3aci) [![coverage badge](https://codecov.io/gh/benedekrozemberczki/littleballoffur/branch/master/graph/badge.svg)](https://codecov.io/github/benedekrozemberczki/littleballoffur?branch=master) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)

<p align=""center"">
  <img width=""90%"" src=""https://github.com/benedekrozemberczki/littleballoffur/blob/master/littleballoffurlogo.jpg?sanitize=true"" />
</p>

----------------------------------------------------------------------------

**little ball of fur** is a graph sampling extension library for python.

please look at the **[documentation](https://little-ball-of-fur.readthedocs.io/)**, relevant **[paper](https://arxiv.org/abs/2006.04311)**, **[promo video](https://youtu.be/5opjbqlpwme)** and **[external resources](https://little-ball-of-fur.readthedocs.io/en/latest/notes/resources.html)**.

----------------------------------------------------------------------------

**little ball of fur** consists of methods that can sample from graph structured data. to put it simply it is a swiss army knife for graph sampling tasks. first, it includes a large variety of vertex, edge, and exploration sampling techniques. second, it provides a unified application public interface which makes the application of sampling algorithms trivial for end-users. implemented methods cover a wide range of networking ([networking](https://link.springer.com/conference/networking), [infocom](https://infocom2020.ieee-infocom.org/), [sigcomm](http://www.sigcomm.org/)) and data mining ([kdd](https://www.kdd.org/kdd2020/), [tkdd](https://dl.acm.org/journal/tkdd), [icde](http://www.wikicfp.com/cfp/program?id=1331&s=icde&f=international%20conference%20on%20data%20engineering)) conferences, workshops, and pieces from prominent journals. 

------------------------------------------------------------------------------

**citing**

if you find **little ball of fur** useful in your research, please consider citing the following paper:

```bibtex
@inproceedings{littleballoffur,
               title={{little ball of fur: a python library for graph sampling}},
               author={benedek rozemberczki and oliver kiss and rik sarkar},
               year={2020},
               pages = {3133‚Äì3140},
               booktitle={proceedings of the 29th acm international conference on information and knowledge management (cikm '20)},
               organization={acm},
}
```
------------------------------------------------------------------------------

**a simple example**

**little ball of fur** makes using modern graph subsampling techniques quite easy (see [here](https://little-ball-of-fur.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial).
for example, this is all it takes to use [diffusion sampling](https://arxiv.org/abs/2001.07463) on a watts-strogatz graph:

```python
import networkx as nx
from littleballoffur import diffusionsampler

graph = nx.newman_watts_strogatz_graph(1000, 20, 0.05)

sampler = diffusionsampler()

new_graph = sampler.sample(graph)
```

--------------------------------------------------------------------------------

**methods included**

in detail, the following sampling methods were implemented.

**node sampling**


* **[degree based node sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/node_sampling.html#littleballoffur.node_sampling.degreebasedsampler.degreebasedsampler)** from adamic *et al.*: [search in power-law networks](https://arxiv.org/abs/cs/0103016) (physical review e 2001)

* **[random node sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/node_sampling.html#littleballoffur.node_sampling.randomnodesampler.randomnodesampler)** from stumpf *et al.*: [subnets of scale-free networks are not scale-free: sampling properties of networks](https://www.pnas.org/content/102/12/4221) (pnas 2005)

* **[pagerank based node sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/node_sampling.html#littleballoffur.node_sampling.pagerankbasedsampler.pagerankbasedsampler)** from leskovec *et al.*: [sampling from large graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (kdd 2006)

**edge sampling**

* **[random edge sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomedgesampler.randomedgesampler)** from krishnamurthy *et al.*: [reducing large internet topologies for faster simulations](http://www.cs.ucr.edu/~michalis/papers/sampling-networking-05.pdf) (networking 2005)

* **[random node-edge sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomnodeedgesampler.randomnodeedgesampler)** from krishnamurthy *et al.*: [reducing large internet topologies for faster simulations](http://www.cs.ucr.edu/~michalis/papers/sampling-networking-05.pdf) (networking 2005)

* **[hybrid node-edge sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.hybridnodeedgesampler.hybridnodeedgesampler)** from krishnamurthy *et al.*: [reducing large internet topologies for faster simulations](http://www.cs.ucr.edu/~michalis/papers/sampling-networking-05.pdf) (networking 2005)

* **[random edge sampler with induction](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomedgesamplerwithinduction.randomedgesamplerwithinduction)** from ahmed *et al.*: [network sampling: from static to streaming graphs](https://dl.acm.org/doi/10.1145/2601438) (tkdd 2013)

* **[random edge sampler with partial induction](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomedgesamplerwithpartialinduction.randomedgesamplerwithpartialinduction)** from ahmed *et al.*: [network sampling: from static to streaming graphs](https://dl.acm.org/doi/10.1145/2601438) (tkdd 2013)

**exploration based sampling**

* **[snowball sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.snowballsampler.snowballsampler)** from goodman: [snowball sampling](https://projecteuclid.org/euclid.aoms/1177705148) (the annals of mathematical statistics 1961)

* **[loop-erased random walk sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.looperasedrandomwalksampler.looperasedrandomwalksampler)** from wilson: [generating random spanning trees more quickly than the cover time](https://link.springer.com/chapter/10.1007/978-1-4612-2168-5_12) (stoc 1996)

* **[forest fire sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.forestfiresampler.forestfiresampler)** from leskovec *et al.*: [graphs over time: densification laws, shrinking diameters and possible explanations](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (kdd 2005)

<details>
<summary><b>expand to see all exploration samplers...</b></summary>


* **[random node-neighbor sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomnodeneighborsampler.randomnodeneighborsampler)** from leskovec *et al.*: [sampling from large graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (kdd 2006)

* **[random walk with restart sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomwalkwithrestartsampler.randomwalkwithrestartsampler)** from leskovec *et al.*: [sampling from large graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (kdd 2006)

* **[metropolis hastings random walk sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.metropolishastingsrandomwalksampler.metropolishastingsrandomwalksampler)** from hubler *et al.*: [metropolis algorithms for representative subgraph sampling](http://mlcb.is.tuebingen.mpg.de/veroeffentlichungen/papers/hueborkrigha08.pdf) (icdm 2008)

* **[random walk sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomwalksampler.randomwalksampler)** from gjoka *et al.*: [walking in facebook: a case study of unbiased sampling of osns](https://ieeexplore.ieee.org/document/5462078) (infocom 2010)

* **[random walk with jump sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomwalkwithjumpsampler.randomwalkwithjumpsampler)** from ribeiro *et al.*: [estimating and sampling graphs with multidimensional random walks](https://arxiv.org/abs/1002.1751) (sigcomm 2010)

* **[frontier sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.frontiersampler.frontiersampler)** from ribeiro *et al.*: [estimating and sampling graphs with multidimensional random walks](https://arxiv.org/abs/1002.1751) (sigcomm 2010)

* **[community structure expansion sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.communitystructureexpansionsampler.communitystructureexpansionsampler)** from maiya *et al.*: [sampling community structure](http://arun.maiya.net/papers/maiya_etal-sampcomm.pdf) (www 2010)

* **[non-backtracking random walk sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.nonbacktrackingrandomwalksampler.nonbacktrackingrandomwalksampler)** from lee *et al.*: [beyond random walk and metropolis-hastings samplers: why you should not backtrack for unbiased graph sampling](https://dl.acm.org/doi/10.1145/2318857.2254795) (sigmetrics 2012)

* **[randomized depth first search sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.depthfirstsearchsampler.depthfirstsearchsampler)** from doerr *et al.*: [metric convergence in social network sampling](https://dl.acm.org/doi/10.1145/2491159.2491168) (hotplanet 2013)

* **[randomized breadth first search sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.breadthfirstsearchsampler.breadthfirstsearchsampler)** from doerr *et al.*: [metric convergence in social network sampling](https://dl.acm.org/doi/10.1145/2491159.2491168) (hotplanet 2013)

* **[rejection constrained metropolis hastings random walk sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.metropolishastingsrandomwalksampler.metropolishastingsrandomwalksampler)** from li *et al.*: [on random walk based graph sampling](https://ieeexplore.ieee.org/document/7113345) (icde 2015)

* **[circulated neighbors random walk sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.circulatedneighborsrandomwalksampler.circulatedneighborsrandomwalksampler)** from zhou *et al.*: [leveraging history for faster sampling of online social networks](https://dl.acm.org/doi/10.5555/2794367.2794373) (vldb 2015)

* **[shortest path sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.shortestpathsampler.shortestpathsampler)** from rezvanian *et al.*: [sampling social networks using shortest paths](https://www.sciencedirect.com/science/article/pii/s0378437115000321) (physica a 2015)

* **[diffusion sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.diffusionsampler.diffusionsampler)** from rozemberczki *et al.*: [fast sequence-based embedding with diffusion graphs](https://arxiv.org/abs/2001.07463) (complex networks 2018)

* **[diffusion tree sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.diffusiontreesampler.diffusiontreesampler)** from rozemberczki *et al.*: [fast sequence-based embedding with diffusion graphs](https://arxiv.org/abs/2001.07463) (complex networks 2018)

* **[common neighbor aware random walk sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.commonneighborawarerandomwalksampler.commonneighborawarerandomwalksampler)** from li *et al.*: [walking with perception: efficient random walk sampling via common neighbor awareness](https://ieeexplore.ieee.org/document/8731555) (icde 2019)

* **[spiky ball sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.spikyballsampler.spikyballsampler)** from ricaud *et al.*: [spikyball sampling: exploring large networks via an inhomogeneous filtered diffusion](https://www.mdpi.com/1999-4893/13/11/275) (algorithms 2020)

  </details>

head over to our [documentation](https://little-ball-of-fur.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.
for a quick start, check out our [examples](https://github.com/benedekrozemberczki/littleballoffur/tree/master/examples.py).

if you notice anything unexpected, please open an [issue](https://github.com/benedekrozemberczki/littleballoffur/issues) and let us know.
if you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/littleballoffur/issues).
we are motivated to constantly make **little ball of fur** even better.


--------------------------------------------------------------------------------

**installation**

**little ball of fur** can be installed with the following pip command.

```sh
$ pip install littleballoffur
```

as we create new releases frequently, upgrading the package casually might be beneficial.

```sh
$ pip install littleballoffur --upgrade
```

--------------------------------------------------------------------------

**running examples**

as part of the documentation we provide a number of use cases to show how to use various sampling techniques. these can accessed [here](https://little-ball-of-fur.readthedocs.io/en/latest/notes/introduction.html) with detailed explanations.


besides the case studies we provide synthetic examples for each model. these can be tried out by running the scripts in the examples folder. you can try out the random walk sampling example by running:

```sh
$ cd examples
$ python ./exploration_sampling/randomwalk_sampler.py
```

---------------------------------------------------------------------


**running tests**

```sh
$ python setup.py test
```

---------------------------------------------------------------------


**license**

- [gnu general public license v3.0](https://github.com/benedekrozemberczki/littleballoffur/blob/master/license)
"
"Karate Club","
 ![version](https://badge.fury.io/py/karateclub.svg?style=plastic)
 ![license](https://img.shields.io/github/license/benedekrozemberczki/karateclub.svg)
[![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/karateclub.svg)](https://github.com/benedekrozemberczki/karateclub/archive/master.zip)
 [![arxiv](https://img.shields.io/badge/arxiv-2003.04819-orange.svg)](https://arxiv.org/abs/2003.04819)
[![build badge](https://github.com/benedekrozemberczki/karateclub/workflows/ci/badge.svg)](https://github.com/benedekrozemberczki/karateclub/actions?query=workflow%3aci)
 [![coverage badge](https://codecov.io/gh/benedekrozemberczki/karateclub/branch/master/graph/badge.svg)](https://codecov.io/github/benedekrozemberczki/karateclub?branch=master)
<p align=""center"">
  <img width=""90%"" src=""https://github.com/benedekrozemberczki/karateclub/blob/master/karatelogo.jpg?sanitize=true"" />
</p>

------------------------------------------------------


**karate club** is an unsupervised machine learning extension library for [networkx](https://networkx.github.io/).


please look at the **[documentation](https://karateclub.readthedocs.io/)**, relevant **[paper](https://arxiv.org/abs/2003.04819)**, **[promo video](https://www.youtube.com/watch?v=t212-ntxu2u)**, and **[external resources](https://karateclub.readthedocs.io/en/latest/notes/resources.html)**.

*karate club* consists of state-of-the-art methods to do unsupervised learning on graph structured data. to put it simply it is a swiss army knife for small-scale graph mining research. first, it provides network embedding techniques at the node and graph level. second, it includes a variety of overlapping and non-overlapping community detection methods. implemented methods cover a wide range of network science ([netsci](https://netscisociety.net/home), [complenet](https://complenet.weebly.com/)), data mining ([icdm](http://icdm2019.bigke.org/), [cikm](http://www.cikm2019.net/), [kdd](https://www.kdd.org/kdd2020/)), artificial intelligence ([aaai](http://www.aaai.org/conferences/conferences.php), [ijcai](https://www.ijcai.org/)) and machine learning ([neurips](https://nips.cc/), [icml](https://icml.cc/), [iclr](https://iclr.cc/)) conferences, workshops, and pieces from prominent journals.

the newly introduced graph classification datasets are available at [snap](https://snap.stanford.edu/data/#disjointgraphs), [tud graph kernel datasets](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets), and [graphlearning.io](https://chrsmrrs.github.io/datasets/).

--------------------------------------------------------------

**citing**

if you find *karate club* and the new datasets useful in your research, please consider citing the following paper:

```bibtex
@inproceedings{karateclub,
       title = {{karate club: an api oriented open-source python framework for unsupervised learning on graphs}},
       author = {benedek rozemberczki and oliver kiss and rik sarkar},
       year = {2020},
       pages = {3125‚Äì3132},
       booktitle = {proceedings of the 29th acm international conference on information and knowledge management (cikm '20)},
       organization = {acm},
}
```
----------------------------------------------------------------

**a simple example**

*karate club* makes the use of modern community detection techniques quite easy (see [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial). for example, this is all it takes to use on a watts-strogatz graph [ego-splitting](https://www.eecs.yorku.ca/course_archive/2017-18/f/6412/reading/kdd17p145.pdf):

```python
import networkx as nx
from karateclub import egonetsplitter

g = nx.newman_watts_strogatz_graph(1000, 20, 0.05)

splitter = egonetsplitter(1.0)

splitter.fit(g)

print(splitter.get_memberships())
```

----------------------------------------------------------------

**models included**

in detail, the following community detection and embedding methods were implemented.

**overlapping community detection**

* **[danmf](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.danmf)** from ye *et al.*: [deep autoencoder-like nonnegative matrix factorization for community detection](https://github.com/benedekrozemberczki/danmf/blob/master/18danmf.pdf) (cikm 2018)

* **[m-nmf](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.m_nmf)** from wang *et al.*: [community preserving network embedding](https://aaai.org/ocs/index.php/aaai/aaai17/paper/view/14589) (aaai 2017)

* **[ego-splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.egonetsplitter)** from epasto *et al.*: [ego-splitting framework: from non-overlapping to overlapping clusters](https://www.eecs.yorku.ca/course_archive/2017-18/f/6412/reading/kdd17p145.pdf) (kdd 2017)

* **[nnsed](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.nnsed)** from sun *et al.*: [a non-negative symmetric encoder-decoder approach for community detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (cikm 2017)

* **[bigclam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.bigclam)** from yang and leskovec: [overlapping community detection at scale: a nonnegative matrix factorization approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (wsdm 2013)

* **[symmnmf](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.symmnmf)** from kuang *et al.*: [symmetric nonnegative matrix factorization for graph clustering](https://www.cc.gatech.edu/~hpark/papers/dadingparksdm12.pdf) (sdm 2012)

**non-overlapping community detection**

* **[gemsec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.gemsec)** from rozemberczki *et al.*: [gemsec: graph embedding with self clustering](https://arxiv.org/abs/1802.03997) (asonam 2019)

* **[edmot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.edmot)** from li *et al.*: [edmot: an edge enhancement approach for motif-aware community detection](https://arxiv.org/abs/1906.04560) (kdd 2019)

* **[scd](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.scd)** from prat-perez *et al.*: [high quality, scalable and parallel community detectionfor large real graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (www 2014)

* **[label propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.labelpropagation)** from raghavan *et al.*: [near linear time algorithm to detect community structures in large-scale networks](https://arxiv.org/abs/0709.2938) (physics review e 2007)


**proximity preserving node embedding**

* **[grarep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.grarep)** from cao *et al.*: [grarep: learning graph representations with global structural information](https://dl.acm.org/citation.cfm?id=2806512) (cikm 2015)

* **[deepwalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.deepwalk)** from perozzi *et al.*: [deepwalk: online learning of social representations](https://arxiv.org/abs/1403.6652) (kdd 2014)

* **[node2vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.node2vec)** from grover *et al.*: [node2vec: scalable feature learning for networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (kdd 2016)

* **[sociodim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.sociodim)** from tang *et al.*: [relational learning via latent social dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (kdd 2009)

* **[glee](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.glee)** from torres *et al.*: [glee: geometric laplacian eigenmap embedding](https://arxiv.org/abs/1905.09763) (journal of complex networks 2020)

* **[boostne](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.boostne)** from li *et al.*: [multi-level network embedding with boosted low-rank matrix approximation](https://arxiv.org/abs/1808.08627) (asonam 2019)

* **[nodesketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.nodesketch)**  from yang *et al.*: [nodesketch: highly-efficient graph embeddings via recursive sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (kdd 2019)

* **[diff2vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.diff2vec)** from rozemberczki and sarkar: [fast sequence based embedding with diffusion graphs](https://arxiv.org/abs/2001.07463) (complenet 2018)

* **[netmf](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.netmf)** from qiu *et al.*: [network embedding as matrix factorization: unifying deepwalk, line, pte, and node2vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/wsdm18-qiu-et-al-netmf-network-embedding.pdf) (wsdm 2018)

* **[randne](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.randne)** from zhang *et al.*: [billion-scale network embedding with iterative random projection](https://arxiv.org/abs/1805.02396) (icdm 2018)

* **[walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.walklets)** from perozzi *et al.*: [don't walk, skip! online learning of multi-scale network embeddings](https://arxiv.org/abs/1605.02115) (asonam 2017)

* **[hope](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.hope)** from ou *et al.*: [asymmetric transitivity preserving graph embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (kdd 2016)

* **[nmf-admm](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.nmfadmm)** from sun and f√©votte: [alternating direction method of multipliers for non-negative matrix factorization with the beta-divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (icassp 2014)

* **[laplacian eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.laplacianeigenmaps)** from belkin and niyogi: [laplacian eigenmaps and spectral techniques for embedding and clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (nips 2001)

**structural node level embedding**

* **[graphwave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.graphwave)** from donnat *et al.*: [learning structural node embeddings via diffusion wavelets](https://arxiv.org/abs/1710.10321) (kdd 2018)

* **[role2vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.role2vec)** from ahmed *et al.*: [learning role-based graph embeddings](https://arxiv.org/abs/1802.02896) (ijcai starai 2018)

**attributed node level embedding**

* **[feather-n](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.feathernode)** from rozemberczki *et al.*: [characteristic functions on graphs: birds of a feather, from statistical descriptors to parametric models](https://arxiv.org/abs/2005.07959) (cikm 2020)

* **[tadw](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.tadw)** from yang *et al.*: [network representation learning with rich text information](https://www.ijcai.org/proceedings/15/papers/299.pdf) (ijcai 2015)

* **[musae](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.musae)** from rozemberczki *et al.*: [multi-scale attributed node embedding](https://arxiv.org/abs/1909.13021) (arxiv 2019)

* **[ae](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.ae)** from rozemberczki *et al.*: [multi-scale attributed node embedding](https://arxiv.org/abs/1909.13021) (arxiv 2019)

* **[fscnmf](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.fscnmf)** from bandyopadhyay *et al.*: [fusing structure and content via non-negative matrix factorization for embedding information networks](https://arxiv.org/pdf/1804.05313.pdf) (arxiv 2018)

* **[sine](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.sine)** from zhang *et al.*: [sine: scalable incomplete network embedding](https://arxiv.org/pdf/1810.06768.pdf) (icdm 2018)

* **[bane](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.bane)** from yang *et al.*: [binarized attributed network embedding](https://ieeexplore.ieee.org/document/8626170) (icdm 2018)

* **[tene](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.tene)** from yang *et al.*: [enhanced network embedding with text information](https://ieeexplore.ieee.org/document/8545577) (icpr 2018)

* **[asne](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.asne)** from liao *et al.*: [attributed social network embedding](https://arxiv.org/abs/1705.04969) (tkde 2018)

**meta node embedding**

* **[neu](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.neu)** from yang *et al.*: [fast network embedding enhancement via high order proximity approximation](https://www.ijcai.org/proceedings/2017/0544.pdf) (ijcai 2017)

**graph level embedding**

* **[feather-g](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.feathergraph)** from rozemberczki *et al.*: [characteristic functions on graphs: birds of a feather, from statistical descriptors to parametric models](https://arxiv.org/abs/2005.07959) (cikm 2020)

* **[graph2vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.graph2vec)** from narayanan *et al.*: [graph2vec: learning distributed representations of graphs](https://arxiv.org/abs/1707.05005) (mlgworkshop 2017)

* **[netlsd](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.netlsd)** from tsitsulin *et al.*: [netlsd: hearing the shape of a graph](https://arxiv.org/abs/1805.10712) (kdd 2018)

* **[waveletcharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.waveletcharacteristic)** from wang *et al.*: [graph embedding via diffusion-wavelets-based node feature distribution characterization](https://arxiv.org/abs/2109.07016) (cikm 2021)

* **[ige](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.ige)** from galland *et al.*: [invariant embedding for graph classification](https://graphreason.github.io/papers/16.pdf) (icml 2019 lrgsd workshop)

* **[ldp](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.ldp)** from cai *et al.*: [a simple yet effective baseline for non-attributed graph classification](https://arxiv.org/abs/1811.03508) (iclr 2019)

* **[geoscattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.geoscattering)** from gao *et al.*: [geometric scattering for graph data analysis](http://proceedings.mlr.press/v97/gao19e.html) (icml 2019)

* **[gl2vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.gl2vec)** from chen and koga: [gl2vec: graph embedding enriched by line graphs with edge features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (iconip 2019)

* **[sf](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.sf)** from de lara and pineau: [a simple baseline algorithm for graph classification](https://arxiv.org/abs/1810.09155) (neurips rrl workshop 2018)

* **[fgsd](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.fgsd)** from verma and zhang: [hunt for the unique, stable, sparse and fast feature learning on graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (neurips 2017)

head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets. for a quick start, check out our [examples](https://github.com/benedekrozemberczki/karateclub/tree/master/examples.py).

if you notice anything unexpected, please open an [issue](https://github.com/benedekrozemberczki/karateclub/issues) and let us know. if you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/karateclub/issues).
we are motivated to constantly make karate club even better.


--------------------------------------------------------------------------------

**installation**

karate club can be installed with the following pip command.

```sh
$ pip install karateclub
```

as we create new releases frequently, upgrading the package casually might be beneficial.

```sh
$ pip install karateclub --upgrade
```

--------------------------------------------------------------------------------

**running examples**

as part of the documentation we provide a number of use cases to show how the clusterings and embeddings can be utilized for downstream learning. these can accessed [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) with detailed line-by-line explanations.


besides the case studies we provide synthetic examples for each model. these can be tried out by running the example scripts. in order to run one of the examples, the graph2vec snippet:

```sh
$ cd examples/whole_graph_embedding/
$ python graph2vec_example.py
```

--------------------------------------------------------------------------------

**running tests**

from the project's root-level directory:

```sh
$ pytest
```

--------------------------------------------------------------------------------

**license**

- [gnu general public license v3.0](https://github.com/benedekrozemberczki/karateclub/blob/master/license)
"
"PyOD","python outlier detection (pyod)
===============================

**deployment & documentation & stats & license**

.. image:: https://img.shields.io/pypi/v/pyod.svg?color=brightgreen
   :target: https://pypi.org/project/pyod/
   :alt: pypi version


.. image:: https://anaconda.org/conda-forge/pyod/badges/version.svg
   :target: https://anaconda.org/conda-forge/pyod
   :alt: anaconda version


.. image:: https://readthedocs.org/projects/pyod/badge/?version=latest
   :target: https://pyod.readthedocs.io/en/latest/?badge=latest
   :alt: documentation status


.. image:: https://img.shields.io/github/stars/yzhao062/pyod.svg
   :target: https://github.com/yzhao062/pyod/stargazers
   :alt: github stars


.. image:: https://img.shields.io/github/forks/yzhao062/pyod.svg?color=blue
   :target: https://github.com/yzhao062/pyod/network
   :alt: github forks


.. image:: https://pepy.tech/badge/pyod
   :target: https://pepy.tech/project/pyod
   :alt: downloads

.. image:: https://github.com/yzhao062/pyod/actions/workflows/testing.yml/badge.svg
   :target: https://github.com/yzhao062/pyod/actions/workflows/testing.yml
   :alt: testing


.. image:: https://coveralls.io/repos/github/yzhao062/pyod/badge.svg
   :target: https://coveralls.io/github/yzhao062/pyod
   :alt: coverage status


.. image:: https://api.codeclimate.com/v1/badges/bdc3d8d0454274c753c4/maintainability
   :target: https://codeclimate.com/github/yzhao062/pyod/maintainability
   :alt: maintainability


.. image:: https://img.shields.io/github/license/yzhao062/pyod.svg
   :target: https://github.com/yzhao062/pyod/blob/master/license
   :alt: license

.. image:: https://img.shields.io/badge/adbench-benchmark_results-pink
   :target: https://github.com/minqi824/adbench
   :alt: benchmark


-----

**news**: we just released a 45-page, the most comprehensive `anomaly detection benchmark paper <https://www.andrew.cmu.edu/user/yuezhao2/papers/22-neurips-adbench.pdf>`_.
the fully `open-sourced adbench <https://github.com/minqi824/adbench>`_ compares 30 anomaly detection algorithms on 57 benchmark datasets.

**for time-series outlier detection**, please use `tods <https://github.com/datamllab/tods>`_.
**for graph outlier detection**, please use `pygod <https://pygod.org/>`_.

pyod is the most comprehensive and scalable **python library** for **detecting outlying objects** in
multivariate data. this exciting yet challenging field is commonly referred as 
`outlier detection <https://en.wikipedia.org/wiki/anomaly_detection>`_
or `anomaly detection <https://en.wikipedia.org/wiki/anomaly_detection>`_.

pyod includes more than 40 detection algorithms, from classical lof (sigmod 2000) to
the latest ecod (tkde 2022). since 2017, pyod has been successfully used in numerous academic researches and
commercial products with more than `10 million downloads <https://pepy.tech/project/pyod>`_.
it is also well acknowledged by the machine learning community with various dedicated posts/tutorials, including
`analytics vidhya <https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/>`_,
`kdnuggets <https://www.kdnuggets.com/2019/02/outlier-detection-methods-cheat-sheet.html>`_, and
`towards data science <https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1>`_.


**pyod is featured for**:

* **unified apis, detailed documentation, and interactive examples** across various algorithms.
* **advanced models**\, including **classical distance and density estimation**, **latest deep learning methods**, and **emerging algorithms like ecod**.
* **optimized performance with jit and parallelization** using `numba <https://github.com/numba/numba>`_ and `joblib <https://github.com/joblib/joblib>`_.
* **fast training & prediction with suod** [#zhao2021suod]_.


**outlier detection with 5 lines of code**\ :


.. code-block:: python


    # train an ecod detector
    from pyod.models.ecod import ecod
    clf = ecod()
    clf.fit(x_train)

    # get outlier scores
    y_train_scores = clf.decision_scores_  # raw outlier scores on the train data
    y_test_scores = clf.decision_function(x_test)  # predict raw outlier scores on test


**personal suggestion on selecting an od algorithm**. if you do not know which algorithm to try, go with:

- `ecod <https://github.com/yzhao062/pyod/blob/master/examples/ecod_example.py>`_: example of using ecod for outlier detection
- `isolation forest <https://github.com/yzhao062/pyod/blob/master/examples/iforest_example.py>`_: example of using isolation forest for outlier detection

they are both fast and interpretable. or, you could try more data-driven approach `metaod <https://github.com/yzhao062/metaod>`_.

**citing pyod**\ :

`pyod paper <http://www.jmlr.org/papers/volume20/19-011/19-011.pdf>`_ is published in
`journal of machine learning research (jmlr) <http://www.jmlr.org/>`_ (mloss track).
if you use pyod in a scientific publication, we would appreciate
citations to the following paper::

    @article{zhao2019pyod,
        author  = {zhao, yue and nasrullah, zain and li, zheng},
        title   = {pyod: a python toolbox for scalable outlier detection},
        journal = {journal of machine learning research},
        year    = {2019},
        volume  = {20},
        number  = {96},
        pages   = {1-7},
        url     = {http://jmlr.org/papers/v20/19-011.html}
    }

or::

    zhao, y., nasrullah, z. and li, z., 2019. pyod: a python toolbox for scalable outlier detection. journal of machine learning research (jmlr), 20(96), pp.1-7.

if you want more general insights of anomaly detection and/or algorithm performance comparison, please see our
neurips 2022 paper `adbench: anomaly detection benchmark paper <https://www.andrew.cmu.edu/user/yuezhao2/papers/22-neurips-adbench.pdf>`_::

    @inproceedings{han2022adbench,
        title={adbench: anomaly detection benchmark},
        author={songqiao han and xiyang hu and hailiang huang and mingqi jiang and yue zhao},
        booktitle={neural information processing systems (neurips)}
        year={2022},
    }

**key links and resources**\ :


* `view the latest codes on github <https://github.com/yzhao062/pyod>`_
* `anomaly detection resources <https://github.com/yzhao062/anomaly-detection-resources>`_


**table of contents**\ :


* `installation <#installation>`_
* `api cheatsheet & reference <#api-cheatsheet--reference>`_
* `adbench benchmark <#adbench-benchmark>`_
* `model save & load <#model-save--load>`_
* `fast train with suod <#fast-train-with-suod>`_
* `implemented algorithms <#implemented-algorithms>`_
* `quick start for outlier detection <#quick-start-for-outlier-detection>`_
* `how to contribute <#how-to-contribute>`_
* `inclusion criteria <#inclusion-criteria>`_


----


installation
^^^^^^^^^^^^

it is recommended to use **pip** or **conda** for installation. please make sure
**the latest version** is installed, as pyod is updated frequently:

.. code-block:: bash

   pip install pyod            # normal install
   pip install --upgrade pyod  # or update if needed

.. code-block:: bash

   conda install -c conda-forge pyod

alternatively, you could clone and run setup.py file:

.. code-block:: bash

   git clone https://github.com/yzhao062/pyod.git
   cd pyod
   pip install .


**required dependencies**\ :


* python 3.6+
* joblib
* matplotlib
* numpy>=1.19
* numba>=0.51
* scipy>=1.5.1
* scikit_learn>=0.20.0
* six
* statsmodels

**optional dependencies (see details below)**\ :

* combo (optional, required for models/combination.py and featurebagging)
* keras/tensorflow (optional, required for autoencoder, and other deep learning models)
* pandas (optional, required for running benchmark)
* suod (optional, required for running suod model)
* xgboost (optional, required for xgbod)
* pythresh to use thresholding

**warning**\ :
pyod has multiple neural network based models, e.g., autoencoders, which are
implemented in both tensorflow and pytorch. however, pyod does **not** install these deep learning libraries for you.
this reduces the risk of interfering with your local copies.
if you want to use neural-net based models, please make sure these deep learning libraries are installed.
instructions are provided: `neural-net faq <https://github.com/yzhao062/pyod/wiki/setting-up-keras-and-tensorflow-for-neural-net-based-models>`_.
similarly, models depending on **xgboost**, e.g., xgbod, would **not** enforce xgboost installation by default.



----


api cheatsheet & reference
^^^^^^^^^^^^^^^^^^^^^^^^^^

full api reference: (https://pyod.readthedocs.io/en/latest/pyod.html). api cheatsheet for all detectors:


* **fit(x)**\ : fit detector. y is ignored in unsupervised methods.
* **decision_function(x)**\ : predict raw anomaly score of x using the fitted detector.
* **predict(x)**\ : predict if a particular sample is an outlier or not using the fitted detector.
* **predict_proba(x)**\ : predict the probability of a sample being outlier using the fitted detector.
* **predict_confidence(x)**\ : predict the model's sample-wise confidence (available in predict and predict_proba) [#perini2020quantifying]_.


key attributes of a fitted model:


* **decision_scores_**\ : the outlier scores of the training data. the higher, the more abnormal.
  outliers tend to have higher scores.
* **labels_**\ : the binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies.


----


adbench benchmark
^^^^^^^^^^^^^^^^^

we just released a 45-page, the most comprehensive `adbench: anomaly detection benchmark <https://arxiv.org/abs/2206.09426>`_ [#han2022adbench]_.
the fully `open-sourced adbench <https://github.com/minqi824/adbench>`_ compares 30 anomaly detection algorithms on 57 benchmark datasets.

the organization of **adbench** is provided below:

.. image:: https://github.com/minqi824/adbench/blob/main/figs/adbench.png?raw=true
   :target: https://github.com/minqi824/adbench/blob/main/figs/adbench.png?raw=true
   :alt: benchmark-fig


**the comparison of selected models** is made available below
(\ `figure <https://raw.githubusercontent.com/yzhao062/pyod/master/examples/all.png>`_\ ,
`compare_all_models.py <https://github.com/yzhao062/pyod/blob/master/examples/compare_all_models.py>`_\ ,
`interactive jupyter notebooks <https://mybinder.org/v2/gh/yzhao062/pyod/master>`_\ ).
for jupyter notebooks, please navigate to **""/notebooks/compare all models.ipynb""**.


.. image:: https://raw.githubusercontent.com/yzhao062/pyod/master/examples/all.png
   :target: https://raw.githubusercontent.com/yzhao062/pyod/master/examples/all.png
   :alt: comparision_of_all



----

model save & load
^^^^^^^^^^^^^^^^^

pyod takes a similar approach of sklearn regarding model persistence.
see `model persistence <https://scikit-learn.org/stable/modules/model_persistence.html>`_ for clarification.

in short, we recommend to use joblib or pickle for saving and loading pyod models.
see `""examples/save_load_model_example.py"" <https://github.com/yzhao062/pyod/blob/master/examples/save_load_model_example.py>`_ for an example.
in short, it is simple as below:

.. code-block:: python

    from joblib import dump, load

    # save the model
    dump(clf, 'clf.joblib')
    # load the model
    clf = load('clf.joblib')

it is known that there are challenges in saving neural network models.
check `#328 <https://github.com/yzhao062/pyod/issues/328#issuecomment-917192704>`_
and `#88 <https://github.com/yzhao062/pyod/issues/88#issuecomment-615343139>`_
for temporary workaround.


----


fast train with suod
^^^^^^^^^^^^^^^^^^^^

**fast training and prediction**: it is possible to train and predict with
a large number of detection models in pyod by leveraging suod framework [#zhao2021suod]_.
see  `suod paper <https://www.andrew.cmu.edu/user/yuezhao2/papers/21-mlsys-suod.pdf>`_
and  `suod example <https://github.com/yzhao062/pyod/blob/master/examples/suod_example.py>`_.


.. code-block:: python

    from pyod.models.suod import suod

    # initialized a group of outlier detectors for acceleration
    detector_list = [lof(n_neighbors=15), lof(n_neighbors=20),
                     lof(n_neighbors=25), lof(n_neighbors=35),
                     copod(), iforest(n_estimators=100),
                     iforest(n_estimators=200)]

    # decide the number of parallel process, and the combination method
    # then clf can be used as any outlier detection model
    clf = suod(base_estimators=detector_list, n_jobs=2, combination='average',
               verbose=false)




----



implemented algorithms
^^^^^^^^^^^^^^^^^^^^^^

pyod toolkit consists of three major functional groups:

**(i) individual detection algorithms** :

===================  ==================  ======================================================================================================  =====  ========================================
type                 abbr                algorithm                                                                                               year   ref
===================  ==================  ======================================================================================================  =====  ========================================
probabilistic        ecod                unsupervised outlier detection using empirical cumulative distribution functions                        2022   [#li2021ecod]_
probabilistic        abod                angle-based outlier detection                                                                           2008   [#kriegel2008angle]_
probabilistic        fastabod            fast angle-based outlier detection using approximation                                                  2008   [#kriegel2008angle]_
probabilistic        copod               copod: copula-based outlier detection                                                                   2020   [#li2020copod]_
probabilistic        mad                 median absolute deviation (mad)                                                                         1993   [#iglewicz1993how]_
probabilistic        sos                 stochastic outlier selection                                                                            2012   [#janssens2012stochastic]_
probabilistic        kde                 outlier detection with kernel density functions                                                         2007   [#latecki2007outlier]_
probabilistic        sampling            rapid distance-based outlier detection via sampling                                                     2013   [#sugiyama2013rapid]_
probabilistic        gmm                 probabilistic mixture modeling for outlier analysis                                                            [#aggarwal2015outlier]_ [ch.2]
linear model         pca                 principal component analysis (the sum of weighted projected distances to the eigenvector hyperplanes)   2003   [#shyu2003a]_
linear model         kpca                kernel principal component analysis                                                                     2007   [#hoffmann2007kernel]_
linear model         mcd                 minimum covariance determinant (use the mahalanobis distances as the outlier scores)                    1999   [#hardin2004outlier]_ [#rousseeuw1999a]_
linear model         cd                  use cook's distance for outlier detection                                                               1977   [#cook1977detection]_
linear model         ocsvm               one-class support vector machines                                                                       2001   [#scholkopf2001estimating]_
linear model         lmdd                deviation-based outlier detection (lmdd)                                                                1996   [#arning1996a]_
proximity-based      lof                 local outlier factor                                                                                    2000   [#breunig2000lof]_
proximity-based      cof                 connectivity-based outlier factor                                                                       2002   [#tang2002enhancing]_
proximity-based      (incremental) cof   memory efficient connectivity-based outlier factor (slower but reduce storage complexity)               2002   [#tang2002enhancing]_
proximity-based      cblof               clustering-based local outlier factor                                                                   2003   [#he2003discovering]_
proximity-based      loci                loci: fast outlier detection using the local correlation integral                                       2003   [#papadimitriou2003loci]_
proximity-based      hbos                histogram-based outlier score                                                                           2012   [#goldstein2012histogram]_
proximity-based      knn                 k nearest neighbors (use the distance to the kth nearest neighbor as the outlier score)                 2000   [#ramaswamy2000efficient]_
proximity-based      avgknn              average knn (use the average distance to k nearest neighbors as the outlier score)                      2002   [#angiulli2002fast]_
proximity-based      medknn              median knn (use the median distance to k nearest neighbors as the outlier score)                        2002   [#angiulli2002fast]_
proximity-based      sod                 subspace outlier detection                                                                              2009   [#kriegel2009outlier]_
proximity-based      rod                 rotation-based outlier detection                                                                        2020   [#almardeny2020a]_
outlier ensembles    iforest             isolation forest                                                                                        2008   [#liu2008isolation]_
outlier ensembles    inne                isolation-based anomaly detection using nearest-neighbor ensembles                                      2018   [#bandaragoda2018isolation]_
outlier ensembles    fb                  feature bagging                                                                                         2005   [#lazarevic2005feature]_
outlier ensembles    lscp                lscp: locally selective combination of parallel outlier ensembles                                       2019   [#zhao2019lscp]_
outlier ensembles    xgbod               extreme boosting based outlier detection **(supervised)**                                               2018   [#zhao2018xgbod]_
outlier ensembles    loda                lightweight on-line detector of anomalies                                                               2016   [#pevny2016loda]_
outlier ensembles    suod                suod: accelerating large-scale unsupervised heterogeneous outlier detection **(acceleration)**          2021   [#zhao2021suod]_
neural networks      autoencoder         fully connected autoencoder (use reconstruction error as the outlier score)                                    [#aggarwal2015outlier]_ [ch.3]
neural networks      vae                 variational autoencoder (use reconstruction error as the outlier score)                                 2013   [#kingma2013auto]_
neural networks      beta-vae            variational autoencoder (all customized loss term by varying gamma and capacity)                        2018   [#burgess2018understanding]_
neural networks      so_gaal             single-objective generative adversarial active learning                                                 2019   [#liu2019generative]_
neural networks      mo_gaal             multiple-objective generative adversarial active learning                                               2019   [#liu2019generative]_
neural networks      deepsvdd            deep one-class classification                                                                           2018   [#ruff2018deep]_
neural networks      anogan              anomaly detection with generative adversarial networks                                                  2017   [#schlegl2017unsupervised]_
neural networks      alad                adversarially learned anomaly detection                                                                 2018   [#zenati2018adversarially]_
graph-based          r-graph             outlier detection by r-graph                                                                            2017   [#you2017provable]_
graph-based          lunar               lunar: unifying local outlier detection methods via graph neural networks                               2022   [#goodge2022lunar]_
===================  ==================  ======================================================================================================  =====  ========================================


**(ii) outlier ensembles & outlier detector combination frameworks**:

===================  ================  =====================================================================================================  =====  ========================================
type                 abbr              algorithm                                                                                              year   ref
===================  ================  =====================================================================================================  =====  ========================================
outlier ensembles    fb                feature bagging                                                                                        2005   [#lazarevic2005feature]_
outlier ensembles    lscp              lscp: locally selective combination of parallel outlier ensembles                                      2019   [#zhao2019lscp]_
outlier ensembles    xgbod             extreme boosting based outlier detection **(supervised)**                                              2018   [#zhao2018xgbod]_
outlier ensembles    loda              lightweight on-line detector of anomalies                                                              2016   [#pevny2016loda]_
outlier ensembles    suod              suod: accelerating large-scale unsupervised heterogeneous outlier detection **(acceleration)**         2021   [#zhao2021suod]_
outlier ensembles    inne              isolation-based anomaly detection using nearest-neighbor ensembles                                     2018   [#bandaragoda2018isolation]_
combination          average           simple combination by averaging the scores                                                             2015   [#aggarwal2015theoretical]_
combination          weighted average  simple combination by averaging the scores with detector weights                                       2015   [#aggarwal2015theoretical]_
combination          maximization      simple combination by taking the maximum scores                                                        2015   [#aggarwal2015theoretical]_
combination          aom               average of maximum                                                                                     2015   [#aggarwal2015theoretical]_
combination          moa               maximization of average                                                                                2015   [#aggarwal2015theoretical]_
combination          median            simple combination by taking the median of the scores                                                  2015   [#aggarwal2015theoretical]_
combination          majority vote     simple combination by taking the majority vote of the labels (weights can be used)                     2015   [#aggarwal2015theoretical]_
===================  ================  =====================================================================================================  =====  ========================================


**(iii) utility functions**:

===================  ======================  =====================================================================================================================================================  ======================================================================================================================================
type                 name                    function                                                                                                                                               documentation
===================  ======================  =====================================================================================================================================================  ======================================================================================================================================
data                 generate_data           synthesized data generation; normal data is generated by a multivariate gaussian and outliers are generated by a uniform distribution                  `generate_data <https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.data.generate_data>`_
data                 generate_data_clusters  synthesized data generation in clusters; more complex data patterns can be created with multiple clusters                                              `generate_data_clusters <https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_clusters>`_
stat                 wpearsonr               calculate the weighted pearson correlation of two samples                                                                                              `wpearsonr <https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.stat_models.wpearsonr>`_
utility              get_label_n             turn raw outlier scores into binary labels by assign 1 to top n outlier scores                                                                         `get_label_n <https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.utility.get_label_n>`_
utility              precision_n_scores      calculate precision @ rank n                                                                                                                           `precision_n_scores <https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.utility.precision_n_scores>`_
===================  ======================  =====================================================================================================================================================  ======================================================================================================================================

----

quick start for outlier detection
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pyod has been well acknowledged by the machine learning community with a few featured posts and tutorials.

**analytics vidhya**: `an awesome tutorial to learn outlier detection in python using pyod library <https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/>`_

**kdnuggets**: `intuitive visualization of outlier detection methods <https://www.kdnuggets.com/2019/02/outlier-detection-methods-cheat-sheet.html>`_, `an overview of outlier detection methods from pyod <https://www.kdnuggets.com/2019/06/overview-outlier-detection-methods-pyod.html>`_

**towards data science**: `anomaly detection for dummies <https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1>`_

**computer vision news (march 2019)**: `python open source toolbox for outlier detection <https://rsipvision.com/computervisionnews-2019march/18/>`_

`""examples/knn_example.py"" <https://github.com/yzhao062/pyod/blob/master/examples/knn_example.py>`_
demonstrates the basic api of using knn detector. **it is noted that the api across all other algorithms are consistent/similar**.

more detailed instructions for running examples can be found in `examples directory <https://github.com/yzhao062/pyod/blob/master/examples>`_.


#. initialize a knn detector, fit the model, and make the prediction.

   .. code-block:: python


       from pyod.models.knn import knn   # knn detector

       # train knn detector
       clf_name = 'knn'
       clf = knn()
       clf.fit(x_train)

       # get the prediction label and outlier scores of the training data
       y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)
       y_train_scores = clf.decision_scores_  # raw outlier scores

       # get the prediction on the test data
       y_test_pred = clf.predict(x_test)  # outlier labels (0 or 1)
       y_test_scores = clf.decision_function(x_test)  # outlier scores

       # it is possible to get the prediction confidence as well
       y_test_pred, y_test_pred_confidence = clf.predict(x_test, return_confidence=true)  # outlier labels (0 or 1) and confidence in the range of [0,1]

#. evaluate the prediction by roc and precision @ rank n (p@n).

   .. code-block:: python

       from pyod.utils.data import evaluate_print
       
       # evaluate and print the results
       print(""\non training data:"")
       evaluate_print(clf_name, y_train, y_train_scores)
       print(""\non test data:"")
       evaluate_print(clf_name, y_test, y_test_scores)


#. see a sample output & visualization.


   .. code-block:: python


       on training data:
       knn roc:1.0, precision @ rank n:1.0

       on test data:
       knn roc:0.9989, precision @ rank n:0.9

   .. code-block:: python


       visualize(clf_name, x_train, y_train, x_test, y_test, y_train_pred,
           y_test_pred, show_figure=true, save_figure=false)

visualization (\ `knn_figure <https://raw.githubusercontent.com/yzhao062/pyod/master/examples/knn.png>`_\ ):

.. image:: https://raw.githubusercontent.com/yzhao062/pyod/master/examples/knn.png
   :target: https://raw.githubusercontent.com/yzhao062/pyod/master/examples/knn.png
   :alt: knn example figure

----

how to contribute
^^^^^^^^^^^^^^^^^

you are welcome to contribute to this exciting project:


* please first check issue lists for ""help wanted"" tag and comment the one
  you are interested. we will assign the issue to you.

* fork the master branch and add your improvement/modification/fix.

* create a pull request to **development branch** and follow the pull request template `pr template <https://github.com/yzhao062/pyod/blob/master/pull_request_template.md>`_

* automatic tests will be triggered. make sure all tests are passed. please make sure all added modules are accompanied with proper test functions.


to make sure the code has the same style and standard, please refer to abod.py, hbos.py, or feature_bagging.py for example.

you are also welcome to share your ideas by opening an issue or dropping me an email at zhaoy@cmu.edu :)


inclusion criteria
^^^^^^^^^^^^^^^^^^

similarly to `scikit-learn <https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms>`_,
we mainly consider well-established algorithms for inclusion.
a rule of thumb is at least two years since publication, 50+ citations, and usefulness.

however, we encourage the author(s) of newly proposed models to share and add your implementation into pyod
for boosting ml accessibility and reproducibility.
this exception only applies if you could commit to the maintenance of your model for at least two year period.


----

reference
^^^^^^^^^


.. [#aggarwal2015outlier] aggarwal, c.c., 2015. outlier analysis. in data mining (pp. 237-263). springer, cham.

.. [#aggarwal2015theoretical] aggarwal, c.c. and sathe, s., 2015. theoretical foundations and algorithms for outlier ensembles.\ *acm sigkdd explorations newsletter*\ , 17(1), pp.24-47.

.. [#aggarwal2017outlier] aggarwal, c.c. and sathe, s., 2017. outlier ensembles: an introduction. springer.

.. [#almardeny2020a] almardeny, y., boujnah, n. and cleary, f., 2020. a novel outlier detection method for multivariate data. *ieee transactions on knowledge and data engineering*.

.. [#angiulli2002fast] angiulli, f. and pizzuti, c., 2002, august. fast outlier detection in high dimensional spaces. in *european conference on principles of data mining and knowledge discovery* pp. 15-27.

.. [#arning1996a] arning, a., agrawal, r. and raghavan, p., 1996, august. a linear method for deviation detection in large databases. in *kdd* (vol. 1141, no. 50, pp. 972-981).

.. [#bandaragoda2018isolation] bandaragoda, t. r., ting, k. m., albrecht, d., liu, f. t., zhu, y., and wells, j. r., 2018, isolation-based anomaly detection using nearest-neighbor ensembles. *computational intelligence*\ , 34(4), pp. 968-998.

.. [#breunig2000lof] breunig, m.m., kriegel, h.p., ng, r.t. and sander, j., 2000, may. lof: identifying density-based local outliers. *acm sigmod record*\ , 29(2), pp. 93-104.

.. [#burgess2018understanding] burgess, christopher p., et al. ""understanding disentangling in beta-vae."" arxiv preprint arxiv:1804.03599 (2018).

.. [#cook1977detection] cook, r.d., 1977. detection of influential observation in linear regression. technometrics, 19(1), pp.15-18.

.. [#goldstein2012histogram] goldstein, m. and dengel, a., 2012. histogram-based outlier score (hbos): a fast unsupervised anomaly detection algorithm. in *ki-2012: poster and demo track*\ , pp.59-63.

.. [#goodge2022lunar] goodge, a., hooi, b., ng, s.k. and ng, w.s., 2022, june. lunar: unifying local outlier detection methods via graph neural networks. in proceedings of the aaai conference on artificial intelligence.

.. [#gopalan2019pidforest] gopalan, p., sharan, v. and wieder, u., 2019. pidforest: anomaly detection via partial identification. in advances in neural information processing systems, pp. 15783-15793.

.. [#han2022adbench] han, s., hu, x., huang, h., jiang, m. and zhao, y., 2022. adbench: anomaly detection benchmark. arxiv preprint arxiv:2206.09426.

.. [#hardin2004outlier] hardin, j. and rocke, d.m., 2004. outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. *computational statistics & data analysis*\ , 44(4), pp.625-638.

.. [#he2003discovering] he, z., xu, x. and deng, s., 2003. discovering cluster-based local outliers. *pattern recognition letters*\ , 24(9-10), pp.1641-1650.

.. [#hoffmann2007kernel] hoffmann, h., 2007. kernel pca for novelty detection. pattern recognition, 40(3), pp.863-874.

.. [#iglewicz1993how] iglewicz, b. and hoaglin, d.c., 1993. how to detect and handle outliers (vol. 16). asq press.

.. [#janssens2012stochastic] janssens, j.h.m., husz√°r, f., postma, e.o. and van den herik, h.j., 2012. stochastic outlier selection. technical report ticc tr 2012-001, tilburg university, tilburg center for cognition and communication, tilburg, the netherlands.

.. [#kingma2013auto] kingma, d.p. and welling, m., 2013. auto-encoding variational bayes. arxiv preprint arxiv:1312.6114.

.. [#kriegel2008angle] kriegel, h.p. and zimek, a., 2008, august. angle-based outlier detection in high-dimensional data. in *kdd '08*\ , pp. 444-452. acm.

.. [#kriegel2009outlier] kriegel, h.p., kr√∂ger, p., schubert, e. and zimek, a., 2009, april. outlier detection in axis-parallel subspaces of high dimensional data. in *pacific-asia conference on knowledge discovery and data mining*\ , pp. 831-838. springer, berlin, heidelberg.

.. [#latecki2007outlier] latecki, l.j., lazarevic, a. and pokrajac, d., 2007, july. outlier detection with kernel density functions. in international workshop on machine learning and data mining in pattern recognition (pp. 61-75). springer, berlin, heidelberg.

.. [#lazarevic2005feature] lazarevic, a. and kumar, v., 2005, august. feature bagging for outlier detection. in *kdd '05*. 2005.

.. [#li2019madgan] li, d., chen, d., jin, b., shi, l., goh, j. and ng, s.k., 2019, september. mad-gan: multivariate anomaly detection for time series data with generative adversarial networks. in *international conference on artificial neural networks* (pp. 703-716). springer, cham.

.. [#li2020copod] li, z., zhao, y., botta, n., ionescu, c. and hu, x. copod: copula-based outlier detection. *ieee international conference on data mining (icdm)*, 2020.

.. [#li2021ecod] li, z., zhao, y., hu, x., botta, n., ionescu, c. and chen, h. g. ecod: unsupervised outlier detection using empirical cumulative distribution functions. *ieee transactions on knowledge and data engineering (tkde)*, 2022.

.. [#liu2008isolation] liu, f.t., ting, k.m. and zhou, z.h., 2008, december. isolation forest. in *international conference on data mining*\ , pp. 413-422. ieee.

.. [#liu2019generative] liu, y., li, z., zhou, c., jiang, y., sun, j., wang, m. and he, x., 2019. generative adversarial active learning for unsupervised outlier detection. *ieee transactions on knowledge and data engineering*.

.. [#papadimitriou2003loci] papadimitriou, s., kitagawa, h., gibbons, p.b. and faloutsos, c., 2003, march. loci: fast outlier detection using the local correlation integral. in *icde '03*, pp. 315-326. ieee.

.. [#pevny2016loda] pevn√Ω, t., 2016. loda: lightweight on-line detector of anomalies. *machine learning*, 102(2), pp.275-304.

.. [#perini2020quantifying] perini, l., vercruyssen, v., davis, j. quantifying the confidence of anomaly detectors in their example-wise predictions. in *joint european conference on machine learning and knowledge discovery in databases (ecml-pkdd)*, 2020.

.. [#ramaswamy2000efficient] ramaswamy, s., rastogi, r. and shim, k., 2000, may. efficient algorithms for mining outliers from large data sets. *acm sigmod record*\ , 29(2), pp. 427-438.

.. [#rousseeuw1999a] rousseeuw, p.j. and driessen, k.v., 1999. a fast algorithm for the minimum covariance determinant estimator. *technometrics*\ , 41(3), pp.212-223.

.. [#ruff2018deep] ruff, l., vandermeulen, r., goernitz, n., deecke, l., siddiqui, s.a., binder, a., m√ºller, e. and kloft, m., 2018, july. deep one-class classification. in *international conference on machine learning* (pp. 4393-4402). pmlr.

.. [#schlegl2017unsupervised] schlegl, t., seeb√∂ck, p., waldstein, s.m., schmidt-erfurth, u. and langs, g., 2017, june. unsupervised anomaly detection with generative adversarial networks to guide marker discovery. in international conference on information processing in medical imaging (pp. 146-157). springer, cham.

.. [#scholkopf2001estimating] scholkopf, b., platt, j.c., shawe-taylor, j., smola, a.j. and williamson, r.c., 2001. estimating the support of a high-dimensional distribution. *neural computation*, 13(7), pp.1443-1471.

.. [#shyu2003a] shyu, m.l., chen, s.c., sarinnapakorn, k. and chang, l., 2003. a novel anomaly detection scheme based on principal component classifier. *miami univ coral gables fl dept of electrical and computer engineering*.

.. [#sugiyama2013rapid] sugiyama, m. and borgwardt, k., 2013. rapid distance-based outlier detection via sampling. advances in neural information processing systems, 26.

.. [#tang2002enhancing] tang, j., chen, z., fu, a.w.c. and cheung, d.w., 2002, may. enhancing effectiveness of outlier detections for low density patterns. in *pacific-asia conference on knowledge discovery and data mining*, pp. 535-548. springer, berlin, heidelberg.

.. [#wang2020advae] wang, x., du, y., lin, s., cui, p., shen, y. and yang, y., 2019. advae: a self-adversarial variational autoencoder with gaussian anomaly prior knowledge for anomaly detection. *knowledge-based systems*.

.. [#you2017provable] you, c., robinson, d.p. and vidal, r., 2017. provable self-representation based outlier detection in a union of subspaces. in proceedings of the ieee conference on computer vision and pattern recognition.

.. [#zenati2018adversarially] zenati, h., romain, m., foo, c.s., lecouat, b. and chandrasekhar, v., 2018, november. adversarially learned anomaly detection. in 2018 ieee international conference on data mining (icdm) (pp. 727-736). ieee.

.. [#zhao2018xgbod] zhao, y. and hryniewicki, m.k. xgbod: improving supervised outlier detection with unsupervised representation learning. *ieee international joint conference on neural networks*\ , 2018.

.. [#zhao2019lscp] zhao, y., nasrullah, z., hryniewicki, m.k. and li, z., 2019, may. lscp: locally selective combination in parallel outlier ensembles. in *proceedings of the 2019 siam international conference on data mining (sdm)*, pp. 585-593. society for industrial and applied mathematics.

.. [#zhao2021suod] zhao, y., hu, x., cheng, c., wang, c., wan, c., wang, w., yang, j., bai, h., li, z., xiao, c., wang, y., qiao, z., sun, j. and akoglu, l. (2021). suod: accelerating large-scale unsupervised heterogeneous outlier detection. *conference on machine learning and systems (mlsys)*.
"
"auto_ml","# auto_ml
> automated machine learning for production and analytics

[![build status](https://travis-ci.org/climbsrocks/auto_ml.svg?branch=master)](https://travis-ci.org/climbsrocks/auto_ml)
[![documentation status](http://readthedocs.org/projects/auto-ml/badge/?version=latest)](http://auto-ml.readthedocs.io/en/latest/?badge=latest)
[![pypi version](https://badge.fury.io/py/auto_ml.svg)](https://badge.fury.io/py/auto_ml)
[![coverage status](https://coveralls.io/repos/github/climbsrocks/auto_ml/badge.svg?branch=master&cachebuster=1)](https://coveralls.io/github/climbsrocks/auto_ml?branch=master&cachebuster=1)
[![license](https://img.shields.io/github/license/mashape/apistatus.svg)]((https://img.shields.io/github/license/mashape/apistatus.svg))
<!-- stars badge?! -->

## installation

- `pip install auto_ml`

## getting started

```python
from auto_ml import predictor
from auto_ml.utils import get_boston_dataset

df_train, df_test = get_boston_dataset()

column_descriptions = {
    'medv': 'output',
    'chas': 'categorical'
}

ml_predictor = predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)

ml_predictor.train(df_train)

ml_predictor.score(df_test, df_test.medv)
```

## show off some more features!

auto_ml is designed for production. here's an example that includes serializing and loading the trained model, then getting predictions on single dictionaries, roughly the process you'd likely follow to deploy the trained model.

```python
from auto_ml import predictor
from auto_ml.utils import get_boston_dataset
from auto_ml.utils_models import load_ml_model

# load data
df_train, df_test = get_boston_dataset()

# tell auto_ml which column is 'output'
# also note columns that aren't purely numerical
# examples include ['nlp', 'date', 'categorical', 'ignore']
column_descriptions = {
  'medv': 'output'
  , 'chas': 'categorical'
}

ml_predictor = predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)

ml_predictor.train(df_train)

# score the model on test data
test_score = ml_predictor.score(df_test, df_test.medv)

# auto_ml is specifically tuned for running in production
# it can get predictions on an individual row (passed in as a dictionary)
# a single prediction like this takes ~1 millisecond
# here we will demonstrate saving the trained model, and loading it again
file_name = ml_predictor.save()

trained_model = load_ml_model(file_name)

# .predict and .predict_proba take in either:
# a pandas dataframe
# a list of dictionaries
# a single dictionary (optimized for speed in production evironments)
predictions = trained_model.predict(df_test)
print(predictions)
```

## 3rd party packages- deep learning with tensorflow & keras, xgboost, lightgbm, catboost

auto_ml has all of these awesome libraries integrated!
generally, just pass one of them in for model_names.
`ml_predictor.train(data, model_names=['deeplearningclassifier'])`

available options are
- `deeplearningclassifier` and `deeplearningregressor`
- `xgbclassifier` and `xgbregressor`
- `lgbmclassifier` and `lgbmregressor`
- `catboostclassifier` and `catboostregressor`

all of these projects are ready for production. these projects all have prediction time in the 1 millisecond range for a single prediction, and are able to be serialized to disk and loaded into a new environment after training.

depending on your machine, they can occasionally be difficult to install, so they are not included in auto_ml's default installation. you are responsible for installing them yourself. auto_ml will run fine without them installed (we check what's installed before choosing which algorithm to use).


## feature responses
get linear-model-esque interpretations from non-linear models. see the [docs](http://auto-ml.readthedocs.io/en/latest/feature_responses.html) for more information and caveats.


## classification

binary and multiclass classification are both supported. note that for now, labels must be integers (0 and 1 for binary classification). auto_ml will automatically detect if it is a binary or multiclass classification problem - you just have to pass in `ml_predictor = predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)`


## feature learning

also known as ""finally found a way to make this deep learning stuff useful for my business"". deep learning is great at learning important features from your data. but the way it turns these learned features into a final prediction is relatively basic. gradient boosting is great at turning features into accurate predictions, but it doesn't do any feature learning.

in auto_ml, you can now automatically use both types of models for what they're great at. if you pass `feature_learning=true, fl_data=some_dataframe` to `.train()`, we will do exactly that: train a deep learning model on your `fl_data`. we won't ask it for predictions (standard stacking approach), instead, we'll use it's penultimate layer to get it's 10 most useful features. then we'll train a gradient boosted model (or any other model of your choice) on those features plus all the original features.

across some problems, we've witnessed this lead to a 5% gain in accuracy, while still making predictions in 1-4 milliseconds, depending on model complexity.

`ml_predictor.train(df_train, feature_learning=true, fl_data=df_fl_data)`

this feature only supports regression and binary classification currently. the rest of auto_ml supports multiclass classification.

## categorical ensembling

ever wanted to train one market for every store/customer, but didn't want to maintain hundreds of thousands of independent models? with `ml_predictor.train_categorical_ensemble()`, we will handle that for you. you'll still have just one consistent api, `ml_predictor.predict(data)`, but behind this single api will be one model for each category you included in your training data.

just tell us which column holds the category you want to split on, and we'll handle the rest. as always, saving the model, loading it in a different environment, and getting speedy predictions live in production is baked right in.

`ml_predictor.train_categorical_ensemble(df_train, categorical_column='store_name')`


### more details available in the docs

http://auto-ml.readthedocs.io/en/latest/


### advice

before you go any further, try running the code. load up some data (either a dataframe, or a list of dictionaries, where each dictionary is a row of data). make a `column_descriptions` dictionary that tells us which attribute name in each row represents the value we're trying to predict. pass all that into `auto_ml`, and see what happens!

everything else in these docs assumes you have done at least the above. start there and everything else will build on top. but this part gets you the output you're probably interested in, without unnecessary complexity.


## docs

the full docs are available at https://auto_ml.readthedocs.io
again though, i'd strongly recommend running this on an actual dataset before referencing the docs any futher.


## what this project does

automates the whole machine learning process, making it super easy to use for both analytics, and getting real-time predictions in production.

a quick overview of buzzwords, this project automates:

- analytics (pass in data, and auto_ml will tell you the relationship of each variable to what it is you're trying to predict).
- feature engineering (particularly around dates, and nlp).
- robust scaling (turning all values into their scaled versions between the range of 0 and 1, in a way that is robust to outliers, and works with sparse data).
- feature selection (picking only the features that actually prove useful).
- data formatting (turning a dataframe or a list of dictionaries into a sparse matrix, one-hot encoding categorical variables, taking the natural log of y for regression problems, etc).
- model selection (which model works best for your problem- we try roughly a dozen apiece for classification and regression problems, including favorites like xgboost if it's installed on your machine).
- hyperparameter optimization (what hyperparameters work best for that model).
- big data (feed it lots of data- it's fairly efficient with resources).
- unicorns (you could conceivably train it to predict what is a unicorn and what is not).
- ice cream (mmm, tasty...).
- hugs (this makes it much easier to do your job, hopefully leaving you more time to hug those those you care about).


### running the tests

if you've cloned the source code and are making any changes (highly encouraged!), or just want to make sure everything works in your environment, run
`nosetests -v tests`.

ci is also set up, so if you're developing on this, you can just open a pr, and the tests will run automatically on travis-ci.

the tests are relatively comprehensive, though as with everything with auto_ml, i happily welcome your contributions here!

[![analytics](https://ga-beacon.appspot.com/ua-58170643-5/auto_ml/readme)](https://github.com/igrigorik/ga-beacon)
"
"machine learning","# machine learning [![build status](https://travis-ci.org/jeff1evesque/machine-learning.svg?branch=master)](https://travis-ci.org/jeff1evesque/machine-learning) [![coverage status](https://coveralls.io/repos/github/jeff1evesque/machine-learning/badge.svg?branch=master)](https://coveralls.io/github/jeff1evesque/machine-learning?branch=master)

this project provides a [web-interface](https://github.com/jeff1evesque/machine-learning/blob/master/readme.md#web-interface),
 as well as a [programmatic-api](https://github.com/jeff1evesque/machine-learning/blob/master/readme.md#programmatic-interface)
 for various machine learning algorithms.

**supported algorithms**:

- [support vector machine](https://en.wikipedia.org/wiki/support_vector_machine) (svm)
- [support vector regression](https://en.wikipedia.org/wiki/support_vector_machine#regression) (svr)

## contributing

please adhere to [`contributing.md`](https://github.com/jeff1evesque/machine-learning/blob/master/contributing.md),
 when contributing code. pull requests that deviate from the
 [`contributing.md`](https://github.com/jeff1evesque/machine-learning/blob/master/contributing.md),
 could be [labelled](https://github.com/jeff1evesque/machine-learning/labels)
 as `invalid`, and closed (without merging to master). these best practices
 will ensure integrity, when revisions of code, or issues need to be reviewed.

**note:** support, and philantropy can be [inquired](https://jeff1evesque.github.io/machine-learning.docs/latest/html/contribution/support),
 to further assist with development.

## configuration

fork this project, using of the following methods:

- [simple clone](https://jeff1evesque.github.io/machine-learning.docs/latest/html/configuration/setup-clone#simple-clone):
 clone the remote master branch.
- [commit hash](https://jeff1evesque.github.io/machine-learning.docs/latest/html/configuration/setup-clone#commit-hash):
 clone the remote master branch, then checkout a specific commit hash.
- [release tag](https://jeff1evesque.github.io/machine-learning.docs/latest/html/configuration/setup-clone#release-tag):
 clone the remote branch, associated with the desired release tag.

## installation

to proceed with the installation for this project, users will need to decide
whether to use the rancher ecosystem, or use `docker-compose`. the former will
likely be less reliable, since the corresponding install script, may not work
nicely across different operating systems. additionally, this project will
assume rancher as the primary method to deploy, and run the application. so,
when using the `docker-compose` alternate, keep track what the corresponding
[endpoints](https://github.com/jeff1evesque/machine-learning/blob/master/readme.md#execution)
should be.

if users choose rancher, both docker and rancher must be installed.
installing docker must be done manually, to fulfill a set of [dependencies](https://jeff1evesque.github.io/machine-learning.docs/latest/html/installation/dependencies).
once completed, rancher can be installed, and automatically configured, by simply
executing a provided bash script, from the docker quickstart terminal:

```bash
cd /path/to/machine-learning
./install-rancher
```

**note:** the installation, and the configuration of rancher, has been [outlined](https://jeff1evesque.github.io/machine-learning.docs/latest/html/installation/rancher)
if more explicit instructions are needed.

if users choose to forgo rancher, and use the `docker-compose`, then simply
install `docker`, as well as `docker-compose`. this will allow the application
to be deployed from any terminal console:

```bash
cd /path/to/machine-learning
docker-compose up
```

**note:** the installation, and the configuration of `docker-compose`, has been [outlined](https://jeff1evesque.github.io/machine-learning.docs/latest/html/installation/docker-compose)
if more explicit instructions are needed.

## execution

both the web-interface, and the programmatic-api, have corresponding
 [unit tests](https://github.com/jeff1evesque/machine-learning/blob/master/doc/test/pytest.rst)
 which can be reviewed, and implemented. it is important to remember,
 the installation of this application will dictate the endpoint. more
 specifically, if the application was installed via rancher, then the
 endpoint will take the form of `https://192.168.99.101:xxxx`. however,
 if the `docker-compose up` alternate was used, then the endpoint will
 likely change to `https://localhost:xxxx`, or `https://127.0.0.1:xxxx`.

### web interface

the [web-interface](https://github.com/jeff1evesque/machine-learning/blob/master/interface/templates/index.html),
 can be accessed within the browser on `https://192.168.99.101:8080`:

![web-interface](https://user-images.githubusercontent.com/2907085/39499223-97b96fce-4d7a-11e8-96e2-c4e31f6b8e09.jpg 'web-interface')

the following sessions are available:

- `data_new`: store the provided dataset(s), within the implemented sql
 database.
- `data_append`: append additional dataset(s), to an existing representation
 (from an earlier `data_new` session), within the implemented sql database.
- `model_generate`: using previous stored dataset(s) (from an earlier
- `data_new`, or `data_append` session), generate a corresponding model into
- `model_predict`: using a previous stored model (from an earlier
 `model_predict` session), from the implemented nosql datastore, along with
 user supplied values, generate a corresponding prediction.

when using the web-interface, it is important to ensure the csv, xml, or json
 file(s), representing the corresponding dataset(s), are properly formatted.
 dataset(s) poorly formatted will fail to create respective json dataset
 representation(s). subsequently, the dataset(s) will not succeed being stored
 into corresponding database tables. this will prevent any models, and subsequent
 predictions from being made.

the following dataset(s), show acceptable syntax:

- [csv sample datasets](https://github.com/jeff1evesque/machine-learning/tree/master/interface/static/data/csv/)
- [xml sample datasets](https://github.com/jeff1evesque/machine-learning/tree/master/interface/static/data/xml/)
- [json sample datasets](https://github.com/jeff1evesque/machine-learning/tree/master/interface/static/data/json/web_interface)

**note:** each dependent variable value (for json datasets), is an array
 (square brackets), since each dependent variable may have multiple
 observations.

### programmatic interface

the programmatic-interface, or set of api, allow users to implement the
 following sessions:

- `data_new`: store the provided dataset(s), within the implemented sql
 database.
- `data_append`: append additional dataset(s), to an existing representation
 (from an earlier `data_new` session), within the implemented sql database.
- `model_generate`: using previous stored dataset(s) (from an earlier
- `data_new`, or `data_append` session), generate a corresponding model into
- `model_predict`: using a previous stored model (from an earlier
 `model_predict` session), from the implemented nosql datastore, along with
 user supplied values, generate a corresponding prediction.

a post request, can be implemented in python, as follows:

```python
import requests

endpoint = 'https://192.168.99.101:9090/load-data'
headers = {
    'authorization': 'bearer ' + token,
    'content-type': 'application/json'
}

requests.post(endpoint, headers=headers, data=json_string_here)
```

**note:** more information, regarding how to obtain a valid `token`, can be further
 reviewed, in the `/login` [documentation](https://jeff1evesque.github.io/machine-learning.docs/latest/html/programmatic-interface/authentication/login).

**note:** various `data` [attributes](https://jeff1evesque.github.io/machine-learning.docs/latest/html/programmatic-interface/data-attributes) can be nested in above `post` request.

it is important to remember that the [`docker-compose.development.yml`](https://github.com/jeff1evesque/machine-learning/blob/3889788a8343a4b7cef2cf84166f9bd35d83021c/docker-compose.development.yml#l33-l43),
 has defined two port forwards, each assigned to its corresponding reverse
 proxy. this allows port `8080` on the host, to map into the `webserver-web`
 container. a similar case for the programmatic-api, uses port `9090` on the
 host.
"
"XGBoost","<img src=https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/logo-m/xgboost.png width=135/>  extreme gradient boosting
===========
[![build status](https://xgboost-ci.net/job/xgboost/job/master/badge/icon)](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/activity)
[![xgboost-ci](https://github.com/dmlc/xgboost/workflows/xgboost-ci/badge.svg?branch=master)](https://github.com/dmlc/xgboost/actions)
[![documentation status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org)
[![github license](http://dmlc.github.io/img/apache2.svg)](./license)
[![cran status badge](http://www.r-pkg.org/badges/version/xgboost)](http://cran.r-project.org/web/packages/xgboost)
[![pypi version](https://badge.fury.io/py/xgboost.svg)](https://pypi.python.org/pypi/xgboost/)
[![conda version](https://img.shields.io/conda/vn/conda-forge/py-xgboost.svg)](https://anaconda.org/conda-forge/py-xgboost)
[![optuna](https://img.shields.io/badge/optuna-integrated-blue)](https://optuna.org)
[![twitter](https://img.shields.io/badge/@xgboostproject--_.svg?style=social&logo=twitter)](https://twitter.com/xgboostproject)
[![openssf scorecard](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost/badge)](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost)

[community](https://xgboost.ai/community) |
[documentation](https://xgboost.readthedocs.org) |
[resources](demo/readme.md) |
[contributors](contributors.md) |
[release notes](news.md)

xgboost is an optimized distributed gradient boosting library designed to be highly ***efficient***, ***flexible*** and ***portable***.
it implements machine learning algorithms under the [gradient boosting](https://en.wikipedia.org/wiki/gradient_boosting) framework.
xgboost provides a parallel tree boosting (also known as gbdt, gbm) that solve many data science problems in a fast and accurate way.
the same code runs on major distributed environment (kubernetes, hadoop, sge, mpi, dask) and can solve problems beyond billions of examples.

license
-------
¬© contributors, 2021. licensed under an [apache-2](https://github.com/dmlc/xgboost/blob/master/license) license.

contribute to xgboost
---------------------
xgboost has been developed and used by a group of active community members. your help is very valuable to make the package better for everyone.
checkout the [community page](https://xgboost.ai/community).

reference
---------
- tianqi chen and carlos guestrin. [xgboost: a scalable tree boosting system](http://arxiv.org/abs/1603.02754). in 22nd sigkdd conference on knowledge discovery and data mining, 2016
- xgboost originates from research project at university of washington.

sponsors
--------
become a sponsor and get a logo here. see details at [sponsoring the xgboost project](https://xgboost.ai/sponsors). the funds are used to defray the cost of continuous integration and testing infrastructure (https://xgboost-ci.net).

## open source collective sponsors
[![backers on open collective](https://opencollective.com/xgboost/backers/badge.svg)](#backers) [![sponsors on open collective](https://opencollective.com/xgboost/sponsors/badge.svg)](#sponsors)

### sponsors
[[become a sponsor](https://opencollective.com/xgboost#sponsor)]

<a href=""https://www.nvidia.com/en-us/"" target=""_blank""><img src=""https://raw.githubusercontent.com/xgboost-ai/xgboost-ai.github.io/master/images/sponsors/nvidia.jpg"" alt=""nvidia"" width=""72"" height=""72""></a>
<a href=""https://www.intel.com/"" target=""_blank""><img src=""https://images.opencollective.com/intel-corporation/2fa85c1/logo/256.png"" width=""72"" height=""72""></a>
<a href=""https://getkoffie.com/?utm_source=opencollective&utm_medium=github&utm_campaign=xgboost"" target=""_blank""><img src=""https://images.opencollective.com/koffielabs/f391ab8/logo/256.png"" width=""72"" height=""72""></a>

### backers
[[become a backer](https://opencollective.com/xgboost#backer)]

<a href=""https://opencollective.com/xgboost#backers"" target=""_blank""><img src=""https://opencollective.com/xgboost/backers.svg?width=890""></a>
"
"ChefBoost","# chefboost

[![downloads](https://pepy.tech/badge/chefboost)](https://pepy.tech/project/chefboost)
[![stars](https://img.shields.io/github/stars/serengil/chefboost?color=yellow)](https://github.com/serengil/chefboost)
[![license](http://img.shields.io/:license-mit-green.svg?style=flat)](https://github.com/serengil/chefboost/blob/master/license)
[![support me on patreon](https://img.shields.io/endpoint.svg?url=https%3a%2f%2fshieldsio-patreon.vercel.app%2fapi%3fusername%3dserengil%26type%3dpatrons&style=flat)](https://www.patreon.com/serengil?repo=chefboost)
[![github sponsors](https://img.shields.io/github/sponsors/serengil?logo=github&color=lightgray)](https://github.com/sponsors/serengil)
[![doi](http://img.shields.io/:doi-10.5281/zenodo.5576203-blue.svg?style=flat)](https://doi.org/10.5281/zenodo.5576203)

**chefboost** is a lightweight decision tree framework for python **with categorical feature support**. it covers regular decision tree algorithms: [id3](https://sefiks.com/2017/11/20/a-step-by-step-id3-decision-tree-example/), [c4.5](https://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/), [cart](https://sefiks.com/2018/08/27/a-step-by-step-cart-decision-tree-example/), [chaid](https://sefiks.com/2020/03/18/a-step-by-step-chaid-decision-tree-example/) and [regression tree](https://sefiks.com/2018/08/28/a-step-by-step-regression-decision-tree-example/); also some advanved techniques: [gradient boosting](https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/), [random forest](https://sefiks.com/2017/11/19/how-random-forests-can-keep-you-from-decision-tree/) and [adaboost](https://sefiks.com/2018/11/02/a-step-by-step-adaboost-example/). you just need to write **a few lines of code** to build decision trees with chefboost.

**installation** - [`demo`](https://youtu.be/yyf993hthf8)

the easiest way to install chefboost framework is to download it from [from pypi](https://pypi.org/project/chefboost). it's going to install the library itself and its prerequisites as well.

```
pip install chefboost
```

then, you will be able to import the library and use its functionalities

```python
from chefboost import chefboost as chef
```

**usage** - [`demo`](https://youtu.be/z93qe5eb6eg)

basically, you just need to pass the dataset as pandas data frame and the optional tree configurations as illustrated below.

```python
import pandas as pd

df = pd.read_csv(""dataset/golf.txt"")
config = {'algorithm': 'c4.5'}
model = chef.fit(df, config = config, target_label = 'decision')
```

**pre-processing**

chefboost handles the both numeric and nominal features and target values in contrast to its alternatives. so, you don't have to apply any pre-processing to build trees.

**outcomes**

built decision trees are stored as python if statements in the `tests/outputs/rules` directory. a sample of decision rules is demonstrated below.

```python
def finddecision(outlook, temperature, humidity, wind):
   if outlook == 'rain':
      if wind == 'weak':
         return 'yes'
      elif wind == 'strong':
         return 'no'
      else:
         return 'no'
   elif outlook == 'sunny':
      if humidity == 'high':
         return 'no'
      elif humidity == 'normal':
         return 'yes'
      else:
         return 'yes'
   elif outlook == 'overcast':
      return 'yes'
   else:
      return 'yes'
 ```

**testing for custom instances**

decision rules will be stored in `outputs/rules/` folder when you build decision trees. you can run the built decision tree for new instances as illustrated below.

```python
prediction = chef.predict(model, param = ['sunny', 'hot', 'high', 'weak'])
```

you can consume built decision trees directly as well. in this way, you can restore already built decision trees and skip learning steps, or apply [transfer learning](https://youtu.be/9hx8ir7_zta). loaded trees offer you finddecision method to test for new instances.

```python
modulename = ""outputs/rules/rules"" #this will load outputs/rules/rules.py
tree = chef.restoretree(modulename)
prediction = tree.finddecision(['sunny', 'hot', 'high', 'weak'])
```

tests/global-unit-test.py will guide you how to build a different decision trees and make predictions.

**model save and restoration**

you can save your trained models. this makes your model ready for transfer learning.

```python
chef.save_model(model, ""model.pkl"")
```

in this way, you can use the same model later to just make predictions. this skips the training steps. restoration requires to store .py and .pkl files under `outputs/rules`.

```python
model = chef.load_model(""model.pkl"")
prediction = chef.predict(model, ['sunny',85,85,'weak'])
```

### sample configurations

chefboost supports several decision tree, bagging and boosting algorithms. you just need to pass the configuration to use different algorithms.

**regular decision trees**

regular decision tree algorithms find the best feature and the best split point maximizing the information gain. it builds decision trees recursively in child nodes.

```python
config = {'algorithm': 'c4.5'} #set algorithm to id3, c4.5, cart, chaid or regression
model = chef.fit(df, config)
```

the following regular decision tree algorithms are wrapped in the library.

| algorithm  | metric | tutorial | demo |
| ---        | --- | ---      | ---  |
| id3        | entropy, information gain |[`tutorial`](https://sefiks.com/2017/11/20/a-step-by-step-id3-decision-tree-example/) | [`demo`](https://youtu.be/z93qe5eb6eg) |
| c4.5       | entropy, gain ratio | [`tutorial`](https://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/) | [`demo`](https://youtu.be/kjhqhmtdaaa) |
| cart       | gini | [`tutorial`](https://sefiks.com/2018/08/27/a-step-by-step-cart-decision-tree-example/) | [`demo`](https://youtu.be/csapbetgukm) |
| chaid      | chi square | [`tutorial`](https://sefiks.com/2020/03/18/a-step-by-step-chaid-decision-tree-example/) | [`demo`](https://youtu.be/dcnfus4qilg) |
| regression | standard deviation | [`tutorial`](https://sefiks.com/2018/08/28/a-step-by-step-regression-decision-tree-example/) | [`demo`](https://youtu.be/pcq2rca20bg) |

**gradient boosting** [`tutorial`](https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/), [`demo`](https://youtu.be/kfsnzkmknae)

gradient boosting is basically based on building a tree, and then building another based on the previous one's error. in this way, it boosts results. predictions will be the sum of each tree'e prediction result.

```python
config = {'enablegbm': true, 'epochs': 7, 'learning_rate': 1, 'max_depth': 5}
```

**random forest** [`tutorial`](https://sefiks.com/2017/11/19/how-random-forests-can-keep-you-from-decision-tree/), [`demo`](https://youtu.be/j7hdtv261pq)

random forest basically splits the data set into several sub data sets and builds different data set for those sub data sets. predictions will be the average of each tree's prediction result.

```python
config = {'enablerandomforest': true, 'num_of_trees': 5}
```

**adaboost** [`tutorial`](https://sefiks.com/2018/11/02/a-step-by-step-adaboost-example/), [`demo`](https://youtu.be/obj208f6e7k)

adaboost applies a decision stump instead of a decision tree. this is a weak classifier and aims to get min 50% score. it then increases the unclassified ones and decreases the classified ones. in this way, it aims to have a high score with weak classifiers.

```python
config = {'enableadaboost': true, 'num_of_weak_classifier': 4}
```

**feature importance** - [`demo`](https://youtu.be/nflqt6ta4-k)

decision trees are naturally interpretable and explainable algorithms. a decision is clear made by a single tree. still we need some extra layers to understand the built models. besides, random forest and gbm are hard to explain. herein, [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) is one of the most common way to see the big picture and understand built models.

```python
df = chef.feature_importance(""outputs/rules/rules.py"")
```

| feature     | final_importance |
| ---         | ---              |
| humidity    | 0.3688           |
| wind        | 0.3688           |
| outlook     | 0.2624           |
| temperature | 0.0000           |

### paralellism

chefboost offers parallelism to speed model building up. branches of a decision tree will be created in parallel in this way. you should set enableparallelism argument to false in the configuration if you don't want to use parallelism. its default value is true. it allocates half of the total number of cores in your environment if parallelism is enabled.

```python
if __name__ == '__main__':
   config = {'algorithm': 'c4.5', 'enableparallelism': true, 'num_cores': 2}
   model = chef.fit(df, config)
```

notice that you have to locate training step in an if block and it should check you are in main.

to not use parallelism set the parameter to false.

```python
config = {'algorithm': 'c4.5', 'enableparallelism': false}
model = chef.fit(df, config)
```

### contributing

pull requests are welcome. you should run the unit tests locally by running [`test/global-unit-test.py`](https://github.com/serengil/chefboost/blob/master/tests/global-unit-test.py). please share the unit test result logs in the pr.

### support

there are many ways to support a project - starring‚≠êÔ∏è the github repos is just one üôè

you can also support this work on [patreon](https://www.patreon.com/serengil?repo=chefboost)

<a href=""https://www.patreon.com/serengil?repo=chefboost"">
<img src=""https://raw.githubusercontent.com/serengil/chefboost/master/icon/patreon.png"" width=""30%"" height=""30%"">
</a>

### citation

please cite [chefboost](https://doi.org/10.5281/zenodo.5576203) in your publications if it helps your research. here is an example bibtex entry:

```bibtex
@misc{serengil2021chefboost,
  author       = {serengil, sefik ilkin},
  title        = {chefboost: a lightweight boosted decision tree framework},
  month        = oct,
  year         = 2021,
  publisher    = {zenodo},
  doi          = {10.5281/zenodo.5576203},
  howpublished = {https://doi.org/10.5281/zenodo.5576203}
}
```

also, if you use chefboost in your github projects, please add chefboost in the requirements.txt.

### licence

chefboost is licensed under the mit license - see [`license`](https://github.com/serengil/chefboost/blob/master/license) for more details.
"
"Apache SINGA","<!--
    licensed to the apache software foundation (asf) under one
    or more contributor license agreements.  see the notice file
    distributed with < this work for additional information
    regarding copyright ownership.  the asf licenses this file
    to you under the apache license, version 2.0 (the
    ""license""); you may not use this file except in compliance
    with the license.  you may obtain a copy of the license at

      http://www.apache.org/licenses/license-2.0

    unless required by applicable law or agreed to in writing,
    software distributed under the license is distributed on an
    ""as is"" basis, without warranties or conditions of any
    kind, either express or implied.  see the license for the
    specific language governing permissions and limitations
    under the license.
-->

![logo](doc/_static/singa.png)

# apache singa

![native ubuntu build status](https://github.com/apache/singa/workflows/native-ubuntu/badge.svg)
![native mac build status](https://github.com/apache/singa/workflows/native-macos/badge.svg)
![conda build status](https://github.com/apache/singa/workflows/conda/badge.svg)
[![documentation status](https://readthedocs.org/projects/apache-singa/badge/?version=latest)](https://apache-singa.readthedocs.io/en/latest/?badge=latest)
![license](http://img.shields.io/:license-apache%202.0-blue.svg)
[![follow apache singa on twitter](https://img.shields.io/twitter/follow/apachesinga.svg?style=social&label=follow)](https://twitter.com/apachesinga)
[![docker pulls](https://img.shields.io/docker/pulls/apache/singa.svg)](https://hub.docker.com/r/apache/singa/)

distributed deep learning system

[http://singa.apache.org](http://singa.apache.org)

## quick start

* [installation](http://singa.apache.org/docs/installation/)
* [examples](examples)

## issues

* [jira tickets](https://issues.apache.org/jira/browse/singa)

## code analysis:

![lgtm c++ grade](https://img.shields.io/lgtm/grade/cpp/github/apache/singa)
![lgtm python grade](https://img.shields.io/lgtm/grade/python/github/apache/singa)
[![codecov](https://codecov.io/gh/apache/singa/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/singa)

[![stargazers over time](https://starchart.cc/apache/singa.svg)](https://starchart.cc/apache/singa)

## mailing lists

* [development mailing list](mailto:dev-subscribe@singa.apache.org) ([archive](http://mail-archives.apache.org/mod_mbox/singa-dev/))
* [commits mailing list](mailto:commits-subscribe@singa.apache.org) ([archive](http://mail-archives.apache.org/mod_mbox/singa-commits/))
"
"Towhee","&nbsp;

<p align=""center"">
    <img src=""towhee_logo.png#gh-light-mode-only"" width=""60%""/>
    <img src=""assets/towhee_logo_dark.png#gh-dark-mode-only"" width=""60%""/>
</p>


<h3 align=""center"">
  <p style=""text-align: center;""> <span style=""font-weight: bold; font: arial, sans-serif;"">x</span>2vec, towhee is all you need! </p>
</h3>

<h3 align=""center"">
  <p style=""text-align: center;"">
  <a href=""readme.md"" target=""_blank"">english</a> | <a href=""readme_cn.md"">‰∏≠ÊñáÊñáÊ°£</a>
  </p>
</h3>

<div class=""column"" align=""middle"">
  <a href=""https://slack.towhee.io"">
    <img src=""https://img.shields.io/badge/join-slack-orange?style=flat"" alt=""join-slack""/>
  </a>
  <a href=""https://twitter.com/towheeio"">
    <img src=""https://img.shields.io/badge/follow-twitter-blue?style=flat"" alt=""twitter""/>
  </a>
  <a href=""https://www.apache.org/licenses/license-2.0"">
    <img src=""https://img.shields.io/badge/license-apache2.0-green?style=flat"" alt=""license""/>
  </a>
  <a href=""https://github.com/towhee-io/towhee/actions/workflows/pylint.yml"">
    <img src=""https://img.shields.io/github/workflow/status/towhee-io/towhee/workflow%20for%20pylint/main?label=pylint&style=flat"" alt=""github actions""/>
  </a>
  <a href=""https://app.codecov.io/gh/towhee-io/towhee"">
    <img src=""https://img.shields.io/codecov/c/github/towhee-io/towhee?style=flat"" alt=""coverage""/>
  </a>
</div>

&nbsp;

[towhee](https://towhee.io) makes it easy to build neural data processing pipelines for ai applications.
we provide hundreds of models, algorithms, and transformations that can be used as standard pipeline building blocks.
you can use towhee's pythonic api to build a prototype of your pipeline and
automatically optimize it for production-ready environments.

:art:&emsp;**various modalities:** towhee supports data processing on a variety of modalities, including images, videos, text, audio, molecular structures, etc.

:mortar_board:&emsp;**sota models:** towhee provides sota models across 5 fields (cv, nlp, multimodal, audio, medical), 15 tasks, and 140+ model architectures. these include bert, clip, vit, swintransformer, mae, and data2vec, all pretrained and ready to use.

:package:&emsp;**data processing:** towhee also provides traditional methods alongside neural network models to help you build practical data processing pipelines. we have a rich pool of operators available, such as video decoding, audio slicing, frame sampling, feature vector dimension reduction, ensembling, and database operations.

:snake:&emsp;**pythonic api:** towhee includes a pythonic method-chaining api for describing custom data processing pipelines. we also support schemas, which makes processing unstructured data as easy as handling tabular data.

## what's new

**v0.9.0 dec. 2, 2022**
* added one video classification model:
[*vis4mer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/vis4mer)
* added three visual backbones:
[*mcprop*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/mcprop), 
[*replknet*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/replknet), 
[*shunted transformer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/shunted_transformer)
* add two code search operators:
[*code_search.codebert*](https://towhee.io/code-search/codebert), 
[*code_search.unixcoder*](https://towhee.io/code-search/unixcoder)
* add five image captioning operators: 
[*image_captioning.expansionnet-v2*](https://towhee.io/image-captioning/expansionnet-v2), 
[*image_captioning.magic*](https://towhee.io/image-captioning/magic),
[*image_captioning.clip_caption_reward*](https://towhee.io/image-captioning/clip-caption-reward), 
[*image_captioning.blip*](https://towhee.io/image-captioning/blip), 
[*image_captioning.clipcap*](https://towhee.io/image-captioning/clipcap)
* add five image-text embedding operators: 
[*image_text_embedding.albef*](https://towhee.io/image-text-embedding/albef), 
[*image_text_embedding.ru_clip*](https://towhee.io/image-text-embedding/ru-clip), 
[*image_text_embedding.japanese_clip*](https://towhee.io/image-text-embedding/japanese-clip),
[*image_text_embedding.taiyi*](https://towhee.io/image-text-embedding/taiyi),
[*image_text_embedding.slip*](https://towhee.io/image-text-embedding/slip)
* add one machine-translation operator: 
[*machine_translation.opus_mt*](https://towhee.io/machine-translation/opus-mt)
* add one filter-tiny-segments operator:
[*video-copy-detection.filter-tiny-segments*](https://towhee.io/video-copy-detection/filter-tiny-segments)
* add an advanced tutorial for audio fingerprinting: 
[*audio fingerprint ii: music detection with temporal localization*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_advanced.ipynb) (increased accuracy from 84% to 90%)

**v0.8.1 sep. 30, 2022**

* added four visual backbones:
[*isc*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/isc),
[*metaformer*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/metaformer),
[*convnext*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/convnext),
[*hornet*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/hornet)
* add two video de-copy operators:
[*select-video*](https://towhee.io/video-copy-detection/select-video), 
[*temporal-network*](https://towhee.io/video-copy-detection/temporal-network)
* add one image embedding operator specifically designed for image retrieval and video de-copy with sota performance on vcsl dataset:
[*isc*](https://towhee.io/image-embedding/isc)
* add one audio embedding operator specified for audio fingerprint:
[*audio_embedding.nnfp*](https://towhee.io/audio-embedding/nnfp) (with pretrained weights)
* add one tutorial for video de-copy: 
[*how to build a video segment copy detection system*](https://github.com/towhee-io/examples/blob/main/video/video_deduplication/segment_level/video_deduplication_at_segment_level.ipynb)
* add one beginner tutorial for audio fingerprint:
[*audio fingerprint i: build a demo with towhee & milvus*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_beginner.ipynb)


**v0.8.0 aug. 16, 2022**

* towhee now supports generating an nvidia triton server from a towhee pipeline, with aditional support for gpu image decoding.
* added one audio fingerprinting model: 
[*nnfp*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/nnfp)
* added two image embedding models: 
[*repmlp*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp), [**wavevit**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/wave_vit)

**v0.7.3 jul. 27, 2022**
* added one multimodal (text/image) model:
[*coca*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coca).
* added two video models for grounded situation recognition & repetitive action counting:
[*coformer*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coformer),
[*transrac*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac).
* added two sota models for image tasks (image retrieval, image classification, etc.):
[*cvnet*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/cvnet),
[*maxvit*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/max_vit)

**v0.7.1 jul. 1, 2022**
* added one image embedding model:
[*mpvit*](https://towhee.io/image-embedding/mpvit).
* added two video retrieval models:
[*bridgeformer*](https://towhee.io/video-text-embedding/bridge-former),
[*collaborative-experts*](https://towhee.io/video-text-embedding/collaborative-experts).
* added faiss-based annsearch operators: *to_faiss*, *faiss_search*.

**v0.7.0 jun. 24, 2022**

* added six video understanding/classification models:
[*video swin transformer*](https://towhee.io/action-classification/video-swin-transformer), 
[*tsm*](https://towhee.io/action-classification/tsm), 
[*uniformer*](https://towhee.io/action-classification/uniformer), 
[*omnivore*](https://towhee.io/action-classification/omnivore), 
[*timesformer*](https://towhee.io/action-classification/timesformer), 
[*movinets*](https://towhee.io/action-classification/movinet).
* added four video retrieval models:
[*clip4clip*](https://towhee.io/video-text-embedding/clip4clip), 
[*drl*](https://towhee.io/video-text-embedding/drl), 
[*frozen in time*](https://towhee.io/video-text-embedding/frozen-in-time), 
[*mdmmt*](https://towhee.io/video-text-embedding/mdmmt).

**v0.6.1  may. 13, 2022**

* added three text-image retrieval models:
[*clip*](https://towhee.io/image-text-embedding/clip),
[*blip*](https://towhee.io/image-text-embedding/blip),
[*lightningdot*](https://towhee.io/image-text-embedding/lightningdot).
* added six video understanding/classification models from pytorchvideo:
[*i3d*](https://towhee.io/action-classification/pytorchvideo),
[*c2d*](https://towhee.io/action-classification/pytorchvideo),
[*slow*](https://towhee.io/action-classification/pytorchvideo),
[*slowfast*](https://towhee.io/action-classification/pytorchvideo),
[*x3d*](https://towhee.io/action-classification/pytorchvideo),
[*mvit*](https://towhee.io/action-classification/pytorchvideo).

## getting started

towhee requires python 3.6+. you can install towhee via `pip`:

```bash
pip install towhee towhee.models
```

if you run into any pip-related install problems, please try to upgrade pip with `pip install -u pip`.

let's try your first towhee pipeline. below is an example for how to create a clip-based cross modal retrieval pipeline with only 15 lines of code.

```python
import towhee

# create image embeddings and build index
(
    towhee.glob['file_name']('./*.png')
          .image_decode['file_name', 'img']()
          .image_text_embedding.clip['img', 'vec'](model_name='clip_vit_base_patch32', modality='image')
          .tensor_normalize['vec','vec']()
          .to_faiss[('file_name', 'vec')](findex='./index.bin')
)

# search image by text
results = (
    towhee.dc['text'](['puppy corgi'])
          .image_text_embedding.clip['text', 'vec'](model_name='clip_vit_base_patch32', modality='text')
          .tensor_normalize['vec', 'vec']()
          .faiss_search['vec', 'results'](findex='./index.bin', k=3)
          .select['text', 'results']()
)
```
<img src=""assets/towhee_example.png"" style=""width: 60%; height: 60%"">

learn more examples from the [towhee bootcamp](https://codelabs.towhee.io/).

## core concepts

towhee is composed of four main building blocks - `operators`, `pipelines`, `datacollection api` and `engine`.

- __operators__: an operator is a single building block of a neural data processing pipeline. different implementations of operators are categorized by tasks, with each task having a standard interface. an operator can be a deep learning model, a data processing method, or a python function.

- __pipelines__: a pipeline is composed of several operators interconnected in the form of a dag (directed acyclic graph). this dag can direct complex functionalities, such as embedding feature extraction, data tagging, and cross modal data analysis.

- __datacollection api__: a pythonic and method-chaining style api for building custom pipelines. a pipeline defined by the datacolltion api can be run locally on a laptop for fast prototyping and then be converted to a docker image, with end-to-end optimizations, for production-ready environments.

- __engine__: the engine sits at towhee's core. given a pipeline, the engine will drive dataflow among individual operators, schedule tasks, and monitor compute resource usage (cpu/gpu/etc). we provide a basic engine within towhee to run pipelines on a single-instance machine and a triton-based engine for docker containers.

## contributing

writing code is not the only way to contribute! submitting issues, answering questions, and improving documentation are just some of the many ways you can help our growing community. check out our [contributing page](https://github.com/towhee-io/towhee/blob/main/contributing.md) for more information.

special thanks goes to these folks for contributing to towhee, either on github, our towhee hub, or elsewhere:
<br><!-- do not remove start of hero-bot --><br>
<img src=""https://img.shields.io/badge/all--contributors-33-orange""><br>
<a href=""https://github.com/anitho""><img src=""https://avatars.githubusercontent.com/u/34787227?v=4"" width=""30px"" /></a>
<a href=""https://github.com/chiiizzzy""><img src=""https://avatars.githubusercontent.com/u/72550076?v=4"" width=""30px"" /></a>
<a href=""https://github.com/guorentong""><img src=""https://avatars.githubusercontent.com/u/57477222?v=4"" width=""30px"" /></a>
<a href=""https://github.com/nicoyuan1986""><img src=""https://avatars.githubusercontent.com/u/109071306?v=4"" width=""30px"" /></a>
<a href=""https://github.com/tumao727""><img src=""https://avatars.githubusercontent.com/u/20420181?v=4"" width=""30px"" /></a>
<a href=""https://github.com/yudongpan""><img src=""https://avatars.githubusercontent.com/u/88148730?v=4"" width=""30px"" /></a>
<a href=""https://github.com/binbinlv""><img src=""https://avatars.githubusercontent.com/u/83755740?v=4"" width=""30px"" /></a>
<a href=""https://github.com/derekdqc""><img src=""https://avatars.githubusercontent.com/u/11754703?v=4"" width=""30px"" /></a>
<a href=""https://github.com/dreamfireyu""><img src=""https://avatars.githubusercontent.com/u/47691077?v=4"" width=""30px"" /></a>
<a href=""https://github.com/filip-halt""><img src=""https://avatars.githubusercontent.com/u/81822489?v=4"" width=""30px"" /></a>
<a href=""https://github.com/fzliu""><img src=""https://avatars.githubusercontent.com/u/6334158?v=4"" width=""30px"" /></a>
<a href=""https://github.com/gexy185""><img src=""https://avatars.githubusercontent.com/u/103474331?v=4"" width=""30px"" /></a>
<a href=""https://github.com/hyf3513onego""><img src=""https://avatars.githubusercontent.com/u/67197231?v=4"" width=""30px"" /></a>
<a href=""https://github.com/jaelgu""><img src=""https://avatars.githubusercontent.com/u/86251631?v=4"" width=""30px"" /></a>
<a href=""https://github.com/jeffoverflow""><img src=""https://avatars.githubusercontent.com/u/24581746?v=4"" width=""30px"" /></a>
<a href=""https://github.com/jingkl""><img src=""https://avatars.githubusercontent.com/u/34296482?v=4"" width=""30px"" /></a>
<a href=""https://github.com/jinlingxu06""><img src=""https://avatars.githubusercontent.com/u/106302799?v=4"" width=""30px"" /></a>
<a href=""https://github.com/junjiejiangjjj""><img src=""https://avatars.githubusercontent.com/u/14136703?v=4"" width=""30px"" /></a>
<a href=""https://github.com/krishnakatyal""><img src=""https://avatars.githubusercontent.com/u/37455387?v=4"" width=""30px"" /></a>
<a href=""https://github.com/omartarek206""><img src=""https://avatars.githubusercontent.com/u/40853054?v=4"" width=""30px"" /></a>
<a href=""https://github.com/oneseer""><img src=""https://avatars.githubusercontent.com/u/28955741?v=4"" width=""30px"" /></a>
<a href=""https://github.com/pravee42""><img src=""https://avatars.githubusercontent.com/u/65100038?v=4"" width=""30px"" /></a>
<a href=""https://github.com/reiase""><img src=""https://avatars.githubusercontent.com/u/5417329?v=4"" width=""30px"" /></a>
<a href=""https://github.com/shiyu22""><img src=""https://avatars.githubusercontent.com/u/53459423?v=4"" width=""30px"" /></a>
<a href=""https://github.com/songxianj""><img src=""https://avatars.githubusercontent.com/u/107831450?v=4"" width=""30px"" /></a>
<a href=""https://github.com/soulteary""><img src=""https://avatars.githubusercontent.com/u/1500781?v=4"" width=""30px"" /></a>
<a href=""https://github.com/sre-ci-robot""><img src=""https://avatars.githubusercontent.com/u/56469371?v=4"" width=""30px"" /></a>
<a href=""https://github.com/sutcalag""><img src=""https://avatars.githubusercontent.com/u/83750738?v=4"" width=""30px"" /></a>
<a href=""https://github.com/wxywb""><img src=""https://avatars.githubusercontent.com/u/5432721?v=4"" width=""30px"" /></a>
<a href=""https://github.com/zc277584121""><img src=""https://avatars.githubusercontent.com/u/17022025?v=4"" width=""30px"" /></a>
<a href=""https://github.com/zengxiang68""><img src=""https://avatars.githubusercontent.com/u/68835157?v=4"" width=""30px"" /></a>
<a href=""https://github.com/zhousicong""><img src=""https://avatars.githubusercontent.com/u/7541863?v=4"" width=""30px"" /></a>
<a href=""https://github.com/zhujiming""><img src=""https://avatars.githubusercontent.com/u/18031320?v=4"" width=""30px"" /></a>
<br><!-- do not remove end of hero-bot --><br>

looking for a database to store and index your embedding vectors? check out [milvus](https://github.com/milvus-io/milvus).
"
"scikit-learn",".. -*- mode: rst -*-

|azure|_ |travis|_ |cirrusci|_ |codecov|_ |circleci|_ |nightly wheels|_ |black|_ |pythonversion|_ |pypi|_ |doi|_ |benchmark|_

.. |azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchname=main
.. _azure: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionid=1&branchname=main

.. |circleci| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield&circle-token=:circle-token
.. _circleci: https://circleci.com/gh/scikit-learn/scikit-learn

.. |travis| image:: https://api.travis-ci.com/scikit-learn/scikit-learn.svg?branch=main
.. _travis: https://app.travis-ci.com/github/scikit-learn/scikit-learn

.. |cirrusci| image:: https://img.shields.io/cirrus/github/scikit-learn/scikit-learn/main?label=cirrus%20ci
.. _cirrusci: https://cirrus-ci.com/github/scikit-learn/scikit-learn/main

.. |codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=pk8g9gg3y9
.. _codecov: https://codecov.io/gh/scikit-learn/scikit-learn

.. |nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/workflows/wheel%20builder/badge.svg?event=schedule
.. _`nightly wheels`: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3a%22wheel+builder%22+event%3aschedule

.. |pythonversion| image:: https://img.shields.io/badge/python-3.8%20%7c%203.9%20%7c%203.10-blue
.. _pythonversion: https://pypi.org/project/scikit-learn/

.. |pypi| image:: https://img.shields.io/pypi/v/scikit-learn
.. _pypi: https://pypi.org/project/scikit-learn

.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg
.. _black: https://github.com/psf/black

.. |doi| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg
.. _doi: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn

.. |benchmark| image:: https://img.shields.io/badge/benchmarked%20by-asv-blue
.. _`benchmark`: https://scikit-learn.org/scikit-learn-benchmarks/

.. |pythonminversion| replace:: 3.8
.. |numpyminversion| replace:: 1.17.3
.. |scipyminversion| replace:: 1.3.2
.. |joblibminversion| replace:: 1.1.1
.. |threadpoolctlminversion| replace:: 2.0.0
.. |matplotlibminversion| replace:: 3.1.3
.. |scikit-imageminversion| replace:: 0.16.2
.. |pandasminversion| replace:: 1.0.5
.. |seabornminversion| replace:: 0.9.0
.. |pytestminversion| replace:: 5.3.1
.. |plotlyminversion| replace:: 5.10.0

.. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png
  :target: https://scikit-learn.org/

**scikit-learn** is a python module for machine learning built on top of
scipy and is distributed under the 3-clause bsd license.

the project was started in 2007 by david cournapeau as a google summer
of code project, and since then many volunteers have contributed. see
the `about us <https://scikit-learn.org/dev/about.html#authors>`__ page
for a list of core contributors.

it is currently maintained by a team of volunteers.

website: https://scikit-learn.org

installation
------------

dependencies
~~~~~~~~~~~~

scikit-learn requires:

- python (>= |pythonminversion|)
- numpy (>= |numpyminversion|)
- scipy (>= |scipyminversion|)
- joblib (>= |joblibminversion|)
- threadpoolctl (>= |threadpoolctlminversion|)

=======

**scikit-learn 0.20 was the last version to support python 2.7 and python 3.4.**
scikit-learn 1.0 and later require python 3.7 or newer.
scikit-learn 1.1 and later require python 3.8 or newer.

scikit-learn plotting capabilities (i.e., functions start with ``plot_`` and
classes end with ""display"") require matplotlib (>= |matplotlibminversion|).
for running the examples matplotlib >= |matplotlibminversion| is required.
a few examples require scikit-image >= |scikit-imageminversion|, a few examples
require pandas >= |pandasminversion|, some examples require seaborn >=
|seabornminversion| and plotly >= |plotlyminversion|.

user installation
~~~~~~~~~~~~~~~~~

if you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using ``pip``::

    pip install -u scikit-learn

or ``conda``::

    conda install -c conda-forge scikit-learn

the documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.


changelog
---------

see the `changelog <https://scikit-learn.org/dev/whats_new.html>`__
for a history of notable changes to scikit-learn.

development
-----------

we welcome new contributors of all experience levels. the scikit-learn
community goals are to be helpful, welcoming, and effective. the
`development guide <https://scikit-learn.org/stable/developers/index.html>`_
has detailed information about contributing code, documentation, tests, and
more. we've included some basic information in this readme.

important links
~~~~~~~~~~~~~~~

- official source code repo: https://github.com/scikit-learn/scikit-learn
- download releases: https://pypi.org/project/scikit-learn/
- issue tracker: https://github.com/scikit-learn/scikit-learn/issues

source code
~~~~~~~~~~~

you can check the latest sources with the command::

    git clone https://github.com/scikit-learn/scikit-learn.git

contributing
~~~~~~~~~~~~

to learn more about making a contribution to scikit-learn, please see our
`contributing guide
<https://scikit-learn.org/dev/developers/contributing.html>`_.

testing
~~~~~~~

after installation, you can launch the test suite from outside the source
directory (you will need to have ``pytest`` >= |pytestminversion| installed)::

    pytest sklearn

see the web page https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage
for more information.

    random number generation can be controlled during testing by setting
    the ``sklearn_seed`` environment variable.

submitting a pull request
~~~~~~~~~~~~~~~~~~~~~~~~~

before opening a pull request, have a look at the
full contributing page to make sure your code complies
with our guidelines: https://scikit-learn.org/stable/developers/index.html

project history
---------------

the project was started in 2007 by david cournapeau as a google summer
of code project, and since then many volunteers have contributed. see
the `about us <https://scikit-learn.org/dev/about.html#authors>`__ page
for a list of core contributors.

the project is currently maintained by a team of volunteers.

**note**: `scikit-learn` was previously referred to as `scikits.learn`.

help and support
----------------

documentation
~~~~~~~~~~~~~

- html documentation (stable release): https://scikit-learn.org
- html documentation (development version): https://scikit-learn.org/dev/
- faq: https://scikit-learn.org/stable/faq.html

communication
~~~~~~~~~~~~~

- mailing list: https://mail.python.org/mailman/listinfo/scikit-learn
- gitter: https://gitter.im/scikit-learn/scikit-learn
- logos & branding: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos
- blog: https://blog.scikit-learn.org
- calendar: https://blog.scikit-learn.org/calendar/
- twitter: https://twitter.com/scikit_learn
- twitter (commits): https://twitter.com/sklearn_commits
- stack overflow: https://stackoverflow.com/questions/tagged/scikit-learn
- github discussions: https://github.com/scikit-learn/scikit-learn/discussions
- website: https://scikit-learn.org
- linkedin: https://www.linkedin.com/company/scikit-learn
- youtube: https://www.youtube.com/channel/ucjosfjym0zyvuarxuozqnnw/playlists
- facebook: https://www.facebook.com/scikitlearnofficial/
- instagram: https://www.instagram.com/scikitlearnofficial/
- tiktok: https://www.tiktok.com/@scikit.learn

citation
~~~~~~~~

if you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn
"
"metric-learn","|github actions build status| |license| |pypi version| |code coverage|

metric-learn: metric learning in python
=======================================

metric-learn contains efficient python implementations of several popular supervised and weakly-supervised metric learning algorithms. as part of `scikit-learn-contrib <https://github.com/scikit-learn-contrib>`_, the api of metric-learn is compatible with `scikit-learn <http://scikit-learn.org/stable/>`_, the leading library for machine learning in python. this allows to use all the scikit-learn routines (for pipelining, model selection, etc) with metric learning algorithms through a unified interface.

**algorithms**

-  large margin nearest neighbor (lmnn)
-  information theoretic metric learning (itml)
-  sparse determinant metric learning (sdml)
-  least squares metric learning (lsml)
-  sparse compositional metric learning (scml)
-  neighborhood components analysis (nca)
-  local fisher discriminant analysis (lfda)
-  relative components analysis (rca)
-  metric learning for kernel regression (mlkr)
-  mahalanobis metric for clustering (mmc)

**dependencies**

-  python 3.6+ (the last version supporting python 2 and python 3.5 was
   `v0.5.0 <https://pypi.org/project/metric-learn/0.5.0/>`_)
-  numpy>= 1.11.0, scipy>= 0.17.0, scikit-learn>=0.21.3

**optional dependencies**

- for sdml, using skggm will allow the algorithm to solve problematic cases
  (install from commit `a0ed406 <https://github.com/skggm/skggm/commit/a0ed406586c4364ea3297a658f415e13b5cbdaf8>`_).
  ``pip install 'git+https://github.com/skggm/skggm.git@a0ed406586c4364ea3297a658f415e13b5cbdaf8'`` to install the required version of skggm from github.
-  for running the examples only: matplotlib

**installation/setup**

- if you use anaconda: ``conda install -c conda-forge metric-learn``. see more options `here <https://github.com/conda-forge/metric-learn-feedstock#installing-metric-learn>`_.

- to install from pypi: ``pip install metric-learn``.

- for a manual install of the latest code, download the source repository and run ``python setup.py install``. you may then run ``pytest test`` to run all tests (you will need to have the ``pytest`` package installed).

**usage**

see the `sphinx documentation`_ for full documentation about installation, api, usage, and examples.

**citation**

if you use metric-learn in a scientific publication, we would appreciate
citations to the following paper:

`metric-learn: metric learning algorithms in python
<http://www.jmlr.org/papers/volume21/19-678/19-678.pdf>`_, de vazelhes
*et al.*, journal of machine learning research, 21(138):1-6, 2020.

bibtex entry::

  @article{metric-learn,
    title = {metric-learn: {m}etric {l}earning {a}lgorithms in {p}ython},
    author = {{de vazelhes}, william and {carey}, cj and {tang}, yuan and
              {vauquier}, nathalie and {bellet}, aur{\'e}lien},
    journal = {journal of machine learning research},
    year = {2020},
    volume = {21},
    number = {138},
    pages = {1--6}
  }

.. _sphinx documentation: http://contrib.scikit-learn.org/metric-learn/

.. |github actions build status| image:: https://github.com/scikit-learn-contrib/metric-learn/workflows/ci/badge.svg
   :target: https://github.com/scikit-learn-contrib/metric-learn/actions?query=event%3apush+branch%3amaster
.. |license| image:: http://img.shields.io/:license-mit-blue.svg?style=flat
   :target: http://badges.mit-license.org
.. |pypi version| image:: https://badge.fury.io/py/metric-learn.svg
   :target: http://badge.fury.io/py/metric-learn
.. |code coverage| image:: https://codecov.io/gh/scikit-learn-contrib/metric-learn/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/scikit-learn-contrib/metric-learn
"
"Intel(R) Extension for Scikit-learn","# intel(r) extension for scikit-learn* <a href=""#oneapi""> <img align=""right"" width=""100"" height=""100"" src=""https://spec.oneapi.io/oneapi-logo-white-scaled.jpg""></a>

[installation](install.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[documentation](https://intel.github.io/scikit-learn-intelex/)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[examples](https://github.com/intel/scikit-learn-intelex/tree/master/examples/notebooks)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[support](#-support)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[faq](#-faq)&nbsp;&nbsp;&nbsp;

[![build status](https://dev.azure.com/daal/daal4py/_apis/build/status/ci?branchname=master)](https://dev.azure.com/daal/daal4py/_build/latest?definitionid=9&branchname=master)
[![coverity scan build status](https://scan.coverity.com/projects/21716/badge.svg)](https://scan.coverity.com/projects/daal4py)
[![join the community on github discussions](https://badgen.net/badge/join%20the%20discussion/on%20github/black?icon=github)](https://github.com/intel/scikit-learn-intelex/discussions)
[![pypi version](https://img.shields.io/pypi/v/scikit-learn-intelex)](https://pypi.org/project/scikit-learn-intelex/)
[![conda version](https://img.shields.io/conda/vn/conda-forge/scikit-learn-intelex)](https://anaconda.org/conda-forge/scikit-learn-intelex)
[![python version](https://img.shields.io/badge/python-3.8%20%7c%203.9%20%7c%203.10%20%7c%203.11-blue)](https://img.shields.io/badge/python-3.8%20%7c%203.9%20%7c%203.10%20%7c%203.11-blue)
[![scikit-learn supported versions](https://img.shields.io/badge/sklearn-0.24%20%7c%201.0%20%7c%201.1%20%7c%201.2-blue)](https://img.shields.io/badge/sklearn-0.24%20%7c%201.0%20%7c%201.1%20%7c%201.2-blue)

with intel(r) extension for scikit-learn you can accelerate your scikit-learn applications and still have full conformance with all scikit-learn apis and algorithms. this is a **free software ai accelerator** that brings over **10-100x** acceleration across a variety of applications. and you do not even need to change the existing code!

## how it works?

intel(r) extension for scikit-learn offers you a way to accelerate existing scikit-learn code.
the acceleration is achieved through **patching**: replacing the stock scikit-learn algorithms with their optimized versions provided by the extension.

one of the ways to patch scikit-learn is by modifying the code. first, you import an additional python package (`sklearnex`) and enable optimizations via `sklearnex.patch_sklearn()`. then import scikit-learn estimators:

- **enable intel cpu optimizations**

    ```py
    import numpy as np
    from sklearnex import patch_sklearn
    patch_sklearn()

    from sklearn.cluster import dbscan

    x = np.array([[1., 2.], [2., 2.], [2., 3.],
                [8., 7.], [8., 8.], [25., 80.]], dtype=np.float32)
    clustering = dbscan(eps=3, min_samples=2).fit(x)
    ```

- **enable intel gpu optimizations**

    ```py
    import numpy as np
    import dpctl
    from sklearnex import patch_sklearn, config_context
    patch_sklearn()

    from sklearn.cluster import dbscan

    x = np.array([[1., 2.], [2., 2.], [2., 3.],
                [8., 7.], [8., 8.], [25., 80.]], dtype=np.float32)
    with config_context(target_offload=""gpu:0""):
        clustering = dbscan(eps=3, min_samples=2).fit(x)
    ```

üëÄ read about [other ways to patch scikit-learn](https://intel.github.io/scikit-learn-intelex/index.html#usage) and [other methods for offloading to gpu devices](https://intel.github.io/scikit-learn-intelex/oneapi-gpu.html).
check out available [notebooks](https://github.com/intel/scikit-learn-intelex/tree/master/examples/notebooks) for more examples.

this software acceleration is achieved through the use of vector instructions, ia hardware-specific memory optimizations, threading, and optimizations for all upcoming intel platforms at launch time.

## supported algorithms

‚ùó the patching only affects [selected algorithms and their parameters](https://intel.github.io/scikit-learn-intelex/algorithms.html).

you may still use algorithms and parameters not supported by intel(r) extension for scikit-learn in your code. you will not get an error if you do this. when you use algorithms or parameters not supported by the extension, the package fallbacks into original stock version of scikit-learn.

## üöÄ acceleration

![](https://raw.githubusercontent.com/intel/scikit-learn-intelex/master/doc/sources/_static/scikit-learn-acceleration-2021.2.3.png)
configurations:
- hw: c5.24xlarge aws ec2 instance using an intel xeon platinum 8275cl with 2 sockets and 24 cores per socket
- sw: scikit-learn version 0.24.2, scikit-learn-intelex version 2021.2.3, python 3.8

[benchmarks code](https://github.com/intelpython/scikit-learn_bench)

## üõ† installation

[system requirements](https://intel.github.io/scikit-learn-intelex/system-requirements.html)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp; [install via pip or conda](https://github.com/intel/scikit-learn-intelex/blob/master/install.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[build from sources](install.md#build-from-sources)

intel(r) extension for scikit-learn is available at the [python package index](https://pypi.org/project/scikit-learn-intelex/),
on anaconda cloud in [conda-forge channel](https://anaconda.org/conda-forge/scikit-learn-intelex) and in [intel channel](https://anaconda.org/intel/scikit-learn-intelex). you can also build the extension from [sources](install.md#build-from-sources).

the extension is also available as a part of [intel¬Æ ai analytics toolkit](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html)‚ÄØ(ai kit). if you already have ai kit installed, you do not need to install the extension.

installation via `pip` package manager is recommended by default:

```bash
pip install scikit-learn-intelex
```

## üîó important links
- [notebook examples](https://github.com/intel/scikit-learn-intelex/tree/master/examples/notebooks)
- [documentation](https://intel.github.io/scikit-learn-intelex/)
- [supported algorithms and parameters](https://intel.github.io/scikit-learn-intelex/algorithms.html)
- [machine learning benchmarks](https://github.com/intelpython/scikit-learn_bench)

## üëÄ follow us on medium

we publish blogs on medium, so [follow us](https://medium.com/intel-analytics-software/tagged/machine-learning) to learn tips and tricks for more efficient data analysis with the help of intel(r) extension for scikit-learn. here are our latest blogs:

- [save time and money with intel extension for scikit-learn](https://medium.com/intel-analytics-software/save-time-and-money-with-intel-extension-for-scikit-learn-33627425ae4)
- [superior machine learning performance on the latest intel xeon scalable processors](https://medium.com/intel-analytics-software/superior-machine-learning-performance-on-the-latest-intel-xeon-scalable-processor-efdec279f5a3)
- [leverage intel optimizations in scikit-learn](https://medium.com/intel-analytics-software/leverage-intel-optimizations-in-scikit-learn-f562cb9d5544)
- [intel gives scikit-learn the performance boost data scientists need](https://medium.com/intel-analytics-software/intel-gives-scikit-learn-the-performance-boost-data-scientists-need-42eb47c80b18)
- [from hours to minutes: 600x faster svm](https://medium.com/intel-analytics-software/from-hours-to-minutes-600x-faster-svm-647f904c31ae)
- [improve the performance of xgboost and lightgbm inference](https://medium.com/intel-analytics-software/improving-the-performance-of-xgboost-and-lightgbm-inference-3b542c03447e)
- [accelerate kaggle challenges using intel ai analytics toolkit](https://medium.com/intel-analytics-software/accelerate-kaggle-challenges-using-intel-ai-analytics-toolkit-beb148f66d5a)
- [accelerate your scikit-learn applications](https://medium.com/intel-analytics-software/improving-the-performance-of-xgboost-and-lightgbm-inference-3b542c03447e)
- [accelerate linear models for machine learning](https://medium.com/intel-analytics-software/accelerating-linear-models-for-machine-learning-5a75ff50a0fe)
- [accelerate k-means clustering](https://medium.com/intel-analytics-software/accelerate-k-means-clustering-6385088788a1)

## ‚ùî faq

<details><summary>[see answers to frequently asked questions]</summary>

### ‚ùì are all algorithms affected by patching?

> no. the patching only affects [selected algorithms and their parameters](https://intel.github.io/scikit-learn-intelex/algorithms.html).

### ‚ùì what happens if i use parameters not supported by the extension?

> in cases when unsupported parameters are used, the package fallbacks into original stock version of scikit-learn. you will not get an error.

### ‚ùì what happens if i run algorithms not supported by the extension?

> if you use algorithms for which no optimizations are available, their original version from the stock scikit-learn is used.

### ‚ùì can i see which implementation of the algorithm is currently used?

> yes. to find out which implementation of the algorithm is currently used (intel(r) extension for scikit-learn or original scikit-learn), use the [verbose mode](https://intel.github.io/scikit-learn-intelex/verbose.html).

### ‚ùì how much faster scikit-learn is after the patching?

> we compare the performance of intel(r) extension for scikit-learn to other frameworks in [machine learning benchmarks](https://github.com/intelpython/scikit-learn_bench). read [our blogs on medium](#-follow-us-on-medium) if you are interested in the detailed comparison.

### ‚ùì what if the patching does not cover my scenario?

> if the patching does not cover your scenarios, [submit an issue on github](https://github.com/intel/scikit-learn-intelex/issues) with the description of what you would want to have.

</details>

## üí¨ support

report issues, ask questions, and provide suggestions using:

- [github issues](https://github.com/intel/scikit-learn-intelex/issues)
- [github discussions](https://github.com/intel/scikit-learn-intelex/discussions)
- [forum](https://community.intel.com/t5/intel-distribution-for-python/bd-p/distribution-python)

you may reach out to project maintainers privately at onedal.maintainers@intel.com

## oneapi

intel(r) extension for scikit-learn is part of [oneapi](https://oneapi.io) and [intel¬Æ ai analytics toolkit](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html)‚ÄØ(ai kit).

## daal4py and onedal

the acceleration is achieved through the use of the intel(r) oneapi data analytics library (onedal). learn more:
- [about intel(r) oneapi data analytics library](https://github.com/oneapi-src/onedal)
- [about daal4py](https://github.com/intel/scikit-learn-intelex/tree/master/daal4py)

---
‚ö†Ô∏èintel(r) extension for scikit-learn contains scikit-learn patching functionality that was originally available in [**daal4py**](https://github.com/intel/scikit-learn-intelex/tree/master/daal4py) package. all future updates for the patches will be available only in intel(r) extension for scikit-learn. we recommend you to use scikit-learn-intelex package instead of daal4py.
you can learn more about daal4py in [daal4py documentation](https://intelpython.github.io/daal4py).

---

"
"Lasagne",".. image:: https://readthedocs.org/projects/lasagne/badge/
    :target: http://lasagne.readthedocs.org/en/latest/

.. image:: https://travis-ci.org/lasagne/lasagne.svg
    :target: https://travis-ci.org/lasagne/lasagne

.. image:: https://img.shields.io/coveralls/lasagne/lasagne.svg
    :target: https://coveralls.io/r/lasagne/lasagne

.. image:: https://img.shields.io/badge/license-mit-blue.svg
    :target: https://github.com/lasagne/lasagne/blob/master/license

.. image:: https://zenodo.org/badge/16974/lasagne/lasagne.svg
   :target: https://zenodo.org/badge/latestdoi/16974/lasagne/lasagne

lasagne
=======

lasagne is a lightweight library to build and train neural networks in theano.
its main features are:

* supports feed-forward networks such as convolutional neural networks (cnns),
  recurrent networks including long short-term memory (lstm), and any
  combination thereof
* allows architectures of multiple inputs and multiple outputs, including
  auxiliary classifiers
* many optimization methods including nesterov momentum, rmsprop and adam
* freely definable cost function and no need to derive gradients due to
  theano's symbolic differentiation
* transparent support of cpus and gpus due to theano's expression compiler

its design is governed by `six principles
<http://lasagne.readthedocs.org/en/latest/user/development.html#philosophy>`_:

* simplicity: be easy to use, easy to understand and easy to extend, to
  facilitate use in research
* transparency: do not hide theano behind abstractions, directly process and
  return theano expressions or python / numpy data types
* modularity: allow all parts (layers, regularizers, optimizers, ...) to be
  used independently of lasagne
* pragmatism: make common use cases easy, do not overrate uncommon cases
* restraint: do not obstruct users with features they decide not to use
* focus: ""do one thing and do it well""


installation
------------

in short, you can install a known compatible version of theano and the latest
lasagne development version via:

.. code-block:: bash

  pip install -r https://raw.githubusercontent.com/lasagne/lasagne/master/requirements.txt
  pip install https://github.com/lasagne/lasagne/archive/master.zip

for more details and alternatives, please see the `installation instructions
<http://lasagne.readthedocs.org/en/latest/user/installation.html>`_.


documentation
-------------

documentation is available online: http://lasagne.readthedocs.org/

for support, please refer to the `lasagne-users mailing list
<https://groups.google.com/forum/#!forum/lasagne-users>`_.


example
-------

.. code-block:: python

  import lasagne
  import theano
  import theano.tensor as t

  # create theano variables for input and target minibatch
  input_var = t.tensor4('x')
  target_var = t.ivector('y')

  # create a small convolutional neural network
  from lasagne.nonlinearities import leaky_rectify, softmax
  network = lasagne.layers.inputlayer((none, 3, 32, 32), input_var)
  network = lasagne.layers.conv2dlayer(network, 64, (3, 3),
                                       nonlinearity=leaky_rectify)
  network = lasagne.layers.conv2dlayer(network, 32, (3, 3),
                                       nonlinearity=leaky_rectify)
  network = lasagne.layers.pool2dlayer(network, (3, 3), stride=2, mode='max')
  network = lasagne.layers.denselayer(lasagne.layers.dropout(network, 0.5),
                                      128, nonlinearity=leaky_rectify,
                                      w=lasagne.init.orthogonal())
  network = lasagne.layers.denselayer(lasagne.layers.dropout(network, 0.5),
                                      10, nonlinearity=softmax)

  # create loss function
  prediction = lasagne.layers.get_output(network)
  loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)
  loss = loss.mean() + 1e-4 * lasagne.regularization.regularize_network_params(
          network, lasagne.regularization.l2)

  # create parameter update expressions
  params = lasagne.layers.get_all_params(network, trainable=true)
  updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01,
                                              momentum=0.9)

  # compile training function that updates parameters and returns training loss
  train_fn = theano.function([input_var, target_var], loss, updates=updates)

  # train network (assuming you've got some training data in numpy arrays)
  for epoch in range(100):
      loss = 0
      for input_batch, target_batch in training_data:
          loss += train_fn(input_batch, target_batch)
      print(""epoch %d: loss %g"" % (epoch + 1, loss / len(training_data)))

  # use trained network for predictions
  test_prediction = lasagne.layers.get_output(network, deterministic=true)
  predict_fn = theano.function([input_var], t.argmax(test_prediction, axis=1))
  print(""predicted class for first test input: %r"" % predict_fn(test_data[0]))

for a fully-functional example, see `examples/mnist.py <examples/mnist.py>`_,
and check the `tutorial
<http://lasagne.readthedocs.org/en/latest/user/tutorial.html>`_ for in-depth
explanations of the same. more examples, code snippets and reproductions of
recent research papers are maintained in the separate `lasagne recipes
<https://github.com/lasagne/recipes>`_ repository.


citation
--------

if you find lasagne useful for your scientific work, please consider citing it
in resulting publications. we provide a ready-to-use `bibtex entry for citing
lasagne <https://github.com/lasagne/lasagne/wiki/lasagne-citation-(bibtex)>`_.


development
-----------

lasagne is a work in progress, input is welcome.

please see the `contribution instructions
<http://lasagne.readthedocs.org/en/latest/user/development.html>`_ for details
on how you can contribute!
"
"Chainer","***notice: as [announced](https://chainer.org/announcement/2019/12/05/released-v7.html), chainer is under the maintenance phase and further development will be limited to bug-fixes and maintenance only.***

----

<div align=""center""><img src=""https://raw.githubusercontent.com/chainer/chainer/master/docs/image/chainer_red_h.png"" width=""400""/></div>

# chainer: a deep learning framework

[![pypi](https://img.shields.io/pypi/v/chainer.svg)](https://pypi.python.org/pypi/chainer)
[![github license](https://img.shields.io/github/license/chainer/chainer.svg)](https://github.com/chainer/chainer)
[![travis](https://img.shields.io/travis/chainer/chainer/master.svg)](https://travis-ci.org/chainer/chainer)
[![coveralls](https://img.shields.io/coveralls/chainer/chainer.svg)](https://coveralls.io/github/chainer/chainer)
[![read the docs](https://readthedocs.org/projects/chainer/badge/?version=stable)](https://docs.chainer.org/en/stable/?badge=stable)
[![optuna](https://img.shields.io/badge/optuna-integrated-blue)](https://optuna.org)

[**website**](https://chainer.org/)
| [**docs**](https://docs.chainer.org/en/stable/)
| [**install guide**](https://docs.chainer.org/en/stable/install.html)
| **tutorials** ([ja](https://tutorials.chainer.org/ja/))
| **examples** ([official](examples), [external](https://github.com/chainer-community/awesome-chainer))
| [**concepts**](https://docs.chainer.org/en/stable/guides/)
| [**chainerx**](#chainerx)

**forum** ([en](https://groups.google.com/forum/#!forum/chainer), [ja](https://groups.google.com/forum/#!forum/chainer-jp))
| **slack invitation** ([en](https://bit.ly/go-chainer-slack), [ja](https://bit.ly/go-chainer-jp-slack))
| **twitter** ([en](https://twitter.com/cupy_team), [ja](https://twitter.com/chainerjp))

*chainer* is a python-based deep learning framework aiming at flexibility.
it provides automatic differentiation apis based on the **define-by-run** approach (a.k.a. dynamic computational graphs) as well as object-oriented high-level apis to build and train neural networks.
it also supports cuda/cudnn using [cupy](https://github.com/cupy/cupy) for high performance training and inference.
for more details about chainer, see the documents and resources listed above and join the community in forum, slack, and twitter.

## installation

*for more details, see the [installation guide](https://docs.chainer.org/en/stable/install.html).*

to install chainer, use `pip`.

```sh
$ pip install chainer
```

to enable cuda support, [cupy](https://github.com/cupy/cupy) is required.
refer to the [cupy installation guide](https://docs-cupy.chainer.org/en/stable/install.html).


## docker image

we are providing the official docker image.
this image supports [nvidia-docker](https://github.com/nvidia/nvidia-docker).
login to the environment with the following command, and run the python interpreter to use chainer with cuda and cudnn support.

```
$ nvidia-docker run -it chainer/chainer /bin/bash
```


## contribution

see the [contribution guide](https://docs.chainer.org/en/stable/contribution.html).


## chainerx

see the [chainerx documentation](https://docs.chainer.org/en/stable/chainerx/index.html).


## license

mit license (see `license` file).


## more information

- [release notes](https://github.com/chainer/chainer/releases)

## references

tokui, seiya, et al. ""chainer: a deep learning framework for accelerating the research cycle."" *proceedings of the 25th acm sigkdd international conference on knowledge discovery & data mining*. acm, 2019.
[url](https://dl.acm.org/citation.cfm?id=3330756) [bibtex](chainer2019_bibtex.txt)

tokui, s., oono, k., hido, s. and clayton, j.,
chainer: a next-generation open source framework for deep learning,
*proceedings of workshop on machine learning systems(learningsys) in
the twenty-ninth annual conference on neural information processing systems (nips)*, (2015)
[url](http://learningsys.org/papers/learningsys_2015_paper_33.pdf), [bibtex](chainer_bibtex.txt)

akiba, t., fukuda, k. and suzuki, s.,
chainermn: scalable distributed deep learning framework,
*proceedings of workshop on ml systems in
the thirty-first annual conference on neural information processing systems (nips)*, (2017)
[url](http://learningsys.org/nips17/assets/papers/paper_25.pdf), [bibtex](chainermn_bibtex.txt)
"
"topik","[![build status](https://travis-ci.org/continuumio/topik.svg?branch=master)](https://travis-ci.org/continuumio/topik)
[![coverage status](https://coveralls.io/repos/continuumio/topik/badge.svg?branch=master&service=github)](https://coveralls.io/github/continuumio/topik?branch=master)
[![scrutinizer code quality](https://scrutinizer-ci.com/g/continuumio/topik/badges/quality-score.png?b=master)](https://scrutinizer-ci.com/g/continuumio/topik/?branch=master)
[![documentation status](https://readthedocs.org/projects/topik/badge/?version=latest)](http://topik.readthedocs.org/en/latest/?badge=latest)

# topik

a topic modeling toolbox.


## introduction

the aim of `topik` is to provide a full suite and high-level interface for anyone interested in applying topic modeling.
for that purpose, `topik` includes many utilities beyond statistical modeling algorithms and wraps all of its
features into an easy callable function and a command line interface.

`topik` is built on top of existing natural language and topic modeling libraries and primarily provides a wrapper around them, for a quick and easy exploratory analysis of your text data sets.

please see our [complete documentation at readthedocs](http://topik.readthedocs.org/en/latest/).

## license

new bsd. see [license file](https://github.com/continuumio/topik/blob/master/license.txt).

"
"Restricted Boltzmann Machines","# how to use

first, initialize an rbm with the desired number of visible and hidden units.

    rbm = rbm(num_visible = 6, num_hidden = 2)
    
next, train the machine:

    training_data = np.array([[1,1,1,0,0,0],[1,0,1,0,0,0],[1,1,1,0,0,0],[0,0,1,1,1,0], [0,0,1,1,0,0],[0,0,1,1,1,0]]) # a 6x6 matrix where each row is a training example and each column is a visible unit.
    r.train(training_data, max_epochs = 5000) # don't run the training for more than 5000 epochs.
    
finally, run wild!

    # given a new set of visible units, we can see what hidden units are activated.
    visible_data = np.array([[0,0,0,1,1,0]]) # a matrix with a single row that contains the states of the visible units. (we can also include more rows.)
    r.run_visible(visible_data) # see what hidden units are activated.

    # given a set of hidden units, we can see what visible units are activated.
    hidden_data = np.array([[1,0]]) # a matrix with a single row that contains the states of the hidden units. (we can also include more rows.)
    r.run_hidden(hidden_data) # see what visible units are activated.
    
    # we can let the network run freely (aka, daydream).
    r.daydream(100) # daydream for 100 steps on a single initialization.

# introduction

suppose you ask a bunch of users to rate a set of movies on a 0-100 scale. in classical [factor analysis](http://en.wikipedia.org/wiki/factor_analysis), you could then try to explain each movie and user in terms of a set of latent *factors*. for example, movies like star wars and lord of the rings might have strong associations with a latent science fiction and fantasy factor, and users who like wall-e and toy story might have strong associations with a latent pixar factor.

restricted boltzmann machines essentially perform a *binary* version of factor analysis. (this is one way of thinking about rbms; there are, of course, others, and lots of different ways to use rbms, but i'll adopt this approach for this post.) instead of users rating a set of movies on a continuous scale, they simply tell you whether they like a movie or not, and the rbm will try to discover latent factors that can explain the activation of these movie choices.

more technically, a restricted boltzmann machine is a **stochastic neural network** (*neural network* meaning we have neuron-like units whose binary activations depend on the neighbors they're connected to; *stochastic* meaning these activations have a probabilistic element) consisting of:

* one layer of **visible units** (users' movie preferences whose states we know and set);
* one layer of **hidden units** (the latent factors we try to learn); and 
* a bias unit (whose state is always on, and is a way of adjusting for the different inherent popularities of each movie). 

furthermore, each visible unit is connected to all the hidden units (this connection is undirected, so each hidden unit is also connected to all the visible units), and the bias unit is connected to all the visible units and all the hidden units. to make learning easier, we restrict the network so that no visible unit is connected to any other visible unit and no hidden unit is connected to any other hidden unit.

for example, suppose we have a set of six movies (harry potter, avatar, lotr 3, gladiator, titanic, and glitter) and we ask users to tell us which ones they want to watch. if we want to learn two latent units underlying movie preferences -- for example, two natural groups in our set of six movies appear to be sf/fantasy (containing harry potter, avatar, and lotr 3) and oscar winners (containing lotr 3, gladiator, and titanic), so we might hope that our latent units will correspond to these categories -- then our rbm would look like the following:

[![rbm example](http://dl.dropbox.com/u/10506/blog/rbms/rbm-example.png)](http://dl.dropbox.com/u/10506/blog/rbms/rbm-example.png)

(note the resemblance to a factor analysis graphical model.)

# state activation

restricted boltzmann machines, and neural networks in general, work by updating the states of some neurons given the states of others, so let's talk about how the states of individual units change. assuming we know the connection weights in our rbm (we'll explain how to learn these below), to update the state of unit $i$:

* compute the **activation energy** $a_i = \sum_j w_{ij} x_j$ of unit $i$, where the sum runs over all units $j$ that unit $i$ is connected to, $w_{ij}$ is the weight of the connection between $i$ and $j$, and $x_j$ is the 0 or 1 state of unit $j$. in other words, all of unit $i$'s neighbors send it a message, and we compute the sum of all these messages.
* let $p_i = \sigma(a_i)$, where $\sigma(x) = 1/(1 + exp(-x))$ is the logistic function. note that $p_i$ is close to 1 for large positive activation energies, and $p_i$ is close to 0 for negative activation energies.
* we then turn unit $i$ on with probability $p_i$, and turn it off with probability $1 - p_i$.
* (in layman's terms, units that are positively connected to each other try to get each other to share the same state (i.e., be both on or off), while units that are negatively connected to each other are enemies that prefer to be in different states.)

for example, let's suppose our two hidden units really do correspond to sf/fantasy and oscar winners. 

* if alice has told us her six binary preferences on our set of movies, we could then ask our rbm which of the hidden units her preferences activate (i.e., ask the rbm to explain her preferences in terms of latent factors). so the six movies send messages to the hidden units, telling them to update themselves. (note that even if alice has declared she wants to watch harry potter, avatar, and lotr 3, this doesn't guarantee that the sf/fantasy hidden unit will turn on, but only that it will turn on with high *probability*. this makes a bit of sense: in the real world, alice wanting to watch all three of those movies makes us highly suspect she likes sf/fantasy in general, but there's a small chance she wants to watch them for other reasons. thus, the rbm allows us to *generate* models of people in the messy, real world.)
* conversely, if we know that one person likes sf/fantasy (so that the sf/fantasy unit is on), we can then ask the rbm which of the movie units that hidden unit turns on (i.e., ask the rbm to generate a set of movie recommendations). so the hidden units send messages to the movie units, telling them to update their states. (again, note that the sf/fantasy unit being on doesn't guarantee that we'll always recommend all three of harry potter, avatar, and lotr 3 because, hey, not everyone who likes science fiction liked avatar.)

# learning weights

so how do we learn the connection weights in our network? suppose we have a bunch of training examples, where each training example is a binary vector with six elements corresponding to a user's movie preferences. then for each epoch, do the following:

* take a training example (a set of six movie preferences). set the states of the visible units to these preferences.
* next, update the states of the hidden units using the logistic activation rule described above: for the $j$th hidden unit, compute its activation energy $a_j = \sum_i w_{ij} x_i$, and set $x_j$ to 1 with probability $\sigma(a_j)$ and to 0 with probability $1 - \sigma(a_j)$. then for each edge $e_{ij}$, compute $positive(e_{ij}) = x_i * x_j$ (i.e., for each pair of units, measure whether they're both on).
* now **reconstruct** the visible units in a similar manner: for each visible unit, compute its activation energy $a_i$, and update its state. (note that this *reconstruction* may not match the original preferences.) then update the hidden units again, and compute $negative(e_{ij}) = x_i * x_j$ for each edge.
* update the weight of each edge $e_{ij}$ by setting $w_{ij} = w_{ij} + l * (positive(e_{ij}) - negative(e_{ij}))$, where $l$ is a learning rate.
* repeat over all training examples.

continue until the network converges (i.e., the error between the training examples and their reconstructions falls below some threshold) or we reach some maximum number of epochs.

why does this update rule make sense? note that 

* in the first phase, $positive(e_{ij})$ measures the association between the $i$th and $j$th unit that we *want* the network to learn from our training examples;
* in the ""reconstruction"" phase, where the rbm generates the states of visible units based on its hypotheses about the hidden units alone, $negative(e_{ij})$ measures the association that the network *itself* generates (or ""daydreams"" about) when no units are fixed to training data. 

so by adding $positive(e_{ij}) - negative(e_{ij})$ to each edge weight, we're helping the network's daydreams better match the reality of our training examples.

(you may hear this update rule called **contrastive divergence**, which is basically a funky term for ""approximate gradient descent"".)

# examples

i wrote [a simple rbm implementation](https://github.com/echen/restricted-boltzmann-machines) in python (the code is heavily commented, so take a look if you're still a little fuzzy on how everything works), so let's use it to walk through some examples.

first, i trained the rbm using some fake data.

* alice: (harry potter = 1, avatar = 1, lotr 3 = 1, gladiator = 0, titanic = 0, glitter = 0). big sf/fantasy fan.
* bob: (harry potter = 1, avatar = 0, lotr 3 = 1, gladiator = 0, titanic = 0, glitter = 0). sf/fantasy fan, but doesn't like avatar.
* carol: (harry potter = 1, avatar = 1, lotr 3 = 1, gladiator = 0, titanic = 0, glitter = 0). big sf/fantasy fan.
* david: (harry potter = 0, avatar = 0, lotr 3 = 1, gladiator = 1, titanic = 1, glitter = 0). big oscar winners fan.
* eric:  (harry potter = 0, avatar = 0, lotr 3 = 1, gladiator = 1, titanic = 1, glitter = 0). oscar winners fan, except for titanic.
* fred: (harry potter = 0, avatar = 0, lotr 3 = 1, gladiator = 1, titanic = 1, glitter = 0). big oscar winners fan.

the network learned the following weights:

                     bias unit       hidden 1        hidden 2
    bias unit       -0.08257658     -0.19041546      1.57007782 
    harry potter    -0.82602559     -7.08986885      4.96606654 
    avatar          -1.84023877     -5.18354129      2.27197472 
    lotr 3           3.92321075      2.51720193      4.11061383 
    gladiator        0.10316995      6.74833901     -4.00505343 
    titanic         -0.97646029      3.25474524     -5.59606865 
    glitter         -4.44685751     -2.81563804     -2.91540988

note that the first hidden unit seems to correspond to the oscar winners, and the second hidden unit seems to correspond to the sf/fantasy movies, just as we were hoping.

what happens if we give the rbm a new user, george, who has (harry potter = 0, avatar = 0, lotr 3 = 0, gladiator = 1, titanic = 1, glitter = 0) as his preferences? it turns the oscar winners unit on (but not the sf/fantasy unit), correctly guessing that george probably likes movies that are oscar winners.

what happens if we activate only the sf/fantasy unit, and run the rbm a bunch of different times? in my trials, it turned on harry potter, avatar, and lotr 3 three times; it turned on avatar and lotr 3, but not harry potter, once; and it turned on harry potter and lotr 3, but not avatar, twice. note that, based on our training examples, these generated preferences do indeed match what we might expect real sf/fantasy fans want to watch.

# modifications

i tried to keep the connection-learning algorithm i described above pretty simple, so here are some modifications that often appear in practice:

* above, $negative(e_{ij})$ was determined by taking the product of the $i$th and $j$th units after reconstructing the visible units *once* and then updating the hidden units again. we could also take the product after some larger number of reconstructions (i.e., repeat updating the visible units, then the hidden units, then the visible units again, and so on); this is slower, but describes the network's daydreams more accurately.
* instead of using $positive(e_{ij})=x_i * x_j$, where $x_i$ and $x_j$ are binary 0 or 1 *states*, we could also let $x_i$ and/or $x_j$ be activation *probabilities*. similarly for $negative(e_{ij})$.
* we could penalize larger edge weights, in order to get a sparser or more regularized model.
* when updating edge weights, we could use a momentum factor: we would add to each edge a weighted sum of the current step as described above (i.e., $l * (positive(e_{ij}) - negative(e_{ij})$) and the step previously taken.
* instead of using only one training example in each epoch, we could use *batches* of examples in each epoch, and only update the network's weights after passing through all the examples in the batch. this can speed up the learning by taking advantage of fast matrix-multiplication algorithms.

# rbmcmd

there is command-line tool to train and run rbm.

here is the code that corresponds to the first example from ""how to use"" section


```
# first, initialize an rbm with the desired number of visible and hidden units.
./rbmcmd rbmstate.dat init  6  2  0.1

# next, train the machine:
./rbmcmd rbmstate.dat train 5000 << \eof
1 1 1 0 0 0
1 0 1 0 0 0
1 1 1 0 0 0
0 0 1 1 1 0
0 0 1 1 0 0
0 0 1 1 1 0
eof

# finally, run wild
# given a new set of visible units, we can see what hidden units are activated.
echo ""0 0 0 1 1 0"" | ./rbmcmd rbmstate.dat run_visible
# 1 0

# given a set of hidden units, we can see what visible units are activated.
echo ""1 0"" | ./rbmcmd rbmstate.dat run_hidden
# 0 1 1 1 0 0

# we can let the network run freely (aka, daydream).
# daydream for 3 steps on a single initialization.
./rbmcmd rbmstate.dat daydream_trace 3 
# 0.901633539115 0.718084610948 0.00650400574634 0.853636318291 0.938241835347 0.0747538486547 
# 1.0 1.0 1.0 0.0 0.0 0.0 
# 1.0 1.0 1.0 0.0 0.0 0.0 


# see 5 dreams, each of 2 step from random data
./rbmcmd rbmstate.dat daydream 3 5
# 1 1 1 0 0 0 
# 0 0 1 1 0 0 
# 1 0 1 0 0 0 
# 1 0 1 0 0 0 
# 1 1 1 0 0 0 

```


# further

if you're interested in learning more about restricted boltzmann machines, here are some good links.

* [a practical guide to training restricted boltzmann machines](http://www.cs.toronto.edu/~hinton/absps/guidetr.pdf), by geoffrey hinton.
* a talk by andrew ng on [unsupervised feature learning and deep learning](http://www.youtube.com/watch?v=zmnoatzigik).
* [restricted boltzmann machines for collaborative filtering](http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf). i found this paper hard to read, but it's an interesting application to the netflix prize.
* [geometry of the restricted boltzmann machine](http://arxiv.org/abs/0908.4425). a very readable introduction to rbms, ""starting with the observation that its zariski closure is a hadamard power of the first secant variety of the segre variety of projective lines"". (i kid, i kid.)
"
"nilearn",".. image:: https://img.shields.io/pypi/v/nilearn.svg
    :target: https://pypi.org/project/nilearn/
    :alt: pypi package

.. image:: https://img.shields.io/pypi/pyversions/nilearn.svg
    :target: https://pypi.org/project/nilearn/
    :alt: pypi - python version

.. image:: https://github.com/nilearn/nilearn/workflows/build/badge.svg?branch=main&event=push
   :target: https://github.com/nilearn/nilearn/actions
   :alt: github actions build status

.. image:: https://codecov.io/gh/nilearn/nilearn/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/nilearn/nilearn
   :alt: coverage status

.. image:: https://dev.azure.com/parietal/nilearn/_apis/build/status/nilearn.nilearn?branchname=main
   :target: https://dev.azure.com/parietal/nilearn/_apis/build/status/nilearn.nilearn?branchname=main
   :alt: azure build status

nilearn
=======

nilearn enables approachable and versatile analyses of brain volumes. it provides statistical and machine-learning tools, with instructive documentation & friendly community.

it supports general linear model (glm) based analysis and leverages the `scikit-learn <https://scikit-learn.org>`_ python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.

important links
===============

- official source code repo: https://github.com/nilearn/nilearn/
- html documentation (stable release): https://nilearn.github.io/

install
=======

latest release
--------------

**1. setup a virtual environment**

we recommend that you install ``nilearn`` in a virtual python environment,
either managed with the standard library ``venv`` or with ``conda``
(see `miniconda <https://docs.conda.io/en/latest/miniconda.html>`_ for instance).
either way, create and activate a new python environment.

with ``venv``:

.. code-block:: bash

    python3 -m venv /<path_to_new_env>
    source /<path_to_new_env>/bin/activate

windows users should change the last line to ``\<path_to_new_env>\scripts\activate.bat`` in order to activate their virtual environment.

with ``conda``:

.. code-block:: bash

    conda create -n nilearn python=3.9
    conda activate nilearn

**2. install nilearn with pip**

execute the following command in the command prompt / terminal
in the proper python environment:

.. code-block:: bash

    python -m pip install -u nilearn

development version
-------------------

please find all development setup instructions in the
`contribution guide <https://nilearn.github.io/stable/development.html#setting-up-your-environment>`_.

check installation
------------------

try importing nilearn in a python / ipython session:

.. code-block:: python

    import nilearn

if no error is raised, you have installed nilearn correctly.

drop-in hours
=============

the nilearn team organizes regular online drop-in hours to answer questions,
discuss feature requests, or have any nilearn-related discussions. nilearn
drop-in hours occur *every wednesday from 4pm to 5pm utc*, and we make sure that at
least one member of the core-developer team is available. these events are held
on our on `discord server <https://discord.gg/bmbhb7w>`_ and are fully open,
anyone is welcome to join!
for more information and ways to engage with the nilearn team see
`how to get help <https://nilearn.github.io/stable/development.html#how-to-get-help>`_.

dependencies
============

the required dependencies to use the software are listed in the file `nilearn/setup.cfg <https://github.com/nilearn/nilearn/blob/main/setup.cfg>`_.

if you are using nilearn plotting functionalities or running the examples, matplotlib >= 3.0 is required.

some plotting functions in nilearn support both matplotlib and plotly as plotting engines.
in order to use the plotly engine in these functions, you will need to install both plotly and kaleido, which can both be installed with pip and anaconda.

if you want to run the tests, you need pytest >= 3.9 and pytest-cov for coverage reporting.

development
===========

detailed instructions on how to contribute are available at
http://nilearn.github.io/stable/development.html"
"neuropredict","
.. image:: docs/logo_neuropredict.png
    :width: 150


.. image:: https://landscape.io/github/raamana/neuropredict/master/landscape.svg?style=flat
    :target: https://landscape.io/github/raamana/neuropredict/master
.. image:: https://api.codacy.com/project/badge/grade/501e560b8a424562a1b8f7cd2f3cadfe
        :target: https://www.codacy.com/app/raamana/neuropredict
.. image:: https://badge.fury.io/py/neuropredict.svg
    :target: https://badge.fury.io/py/neuropredict
.. image:: https://travis-ci.org/raamana/neuropredict.svg?branch=master
    :target: https://travis-ci.org/raamana/neuropredict
.. image:: https://img.shields.io/badge/python-3.6-blue.svg


.. image:: https://img.shields.io/badge/say-thanks-ff69b4.svg
    :target: https://saythanks.io/to/raamana


**documentation**: https://raamana.github.io/neuropredict/

news
----

-  as of ``v0.6``, **neuropredict now supports regression applications**
   i.e.¬†predicting continuous targets (in addition to categorical
   classes), as well as allow you to **regress out covariates /
   confounds** within the nested-cv (following all the best practices).
   utilizing this feature requires the input datasets be specified in
   the ``pyradigm`` data structures: code @ https://github.com/raamana/pyradigm,
   docs @ https://raamana.github.io/pyradigm/. check the changelog below for more details.

older news
----------

-  ``neuropredict`` can handle missing data now (that are encoded with
   ``numpy.nan``). this is done respecting the cross-validation splits
   without any data leakage.

overview
--------

on a high level,

.. image:: docs/high_level_flow.png
   :alt: roleofneuropredict


on a more detailed level,

.. image:: docs/role.png
   :alt: roleofneuropredict

-  docs: https://raamana.github.io/neuropredict/
-  contributors most welcome: `check ideas <contributing.md>`__ and the following
   `guidelines <http://contribution-guide-org.readthedocs.io>`__.
   thanks.

long term goals
---------------

neuropredict, the tool, is part of a broader initiative described below
to develop easy, comprehensive and standardized predictive analysis:

.. image:: docs/neuropredict_long_term_goals.jpg
   :alt: longtermgoals

citation
--------

if ``neuropredict`` helped you in your research in one way or another,
please consider citing one or more of the following, which were
essential building blocks of neuropredict: 

 - pradeep reddy raamana. (2017). neuropredict: easy machine learning and standardized predictive analysis of biomarkers (version 0.4.5). zenodo. http://doi.org/10.5281/zenodo.1058993 
 - raamana et al, (2017), python class defining a machine learning dataset ensuring key-based correspondence and maintaining integrity, journal of open source software, 2(17), 382, doi:10.21105/joss.00382

change log - version 0.6
--------------------------
- major feature: ability to predict continuous variables (regression)
- major feature: ability to handle confounds (regress them out, augmenting etc)
- redesigned the internal structure for easier extensibility
- new ``cvresults`` class for easier management of a wealth of outputs generated in the classification and regression workflows
- api access is refreshed and easier

change log - version 0.5.2
--------------------------

-  imputation of missing values
-  additional classifiers such as ``xgboost``, decision trees
-  better internal code structure
-  lot more tests
-  more precise tests, as we vary number of classes wildly in test
   suites
-  several bug fixes and enhancements
-  more cmd line options such as ``--print_options`` from a previous run

.. |logo| image:: docs/logo_neuropredict.png
.. |travis| image:: https://travis-ci.org/raamana/neuropredict.svg?branch=master
   :target: https://travis-ci.org/raamana/neuropredict.svg?branch=master
.. |code health| image:: https://landscape.io/github/raamana/neuropredict/master/landscape.svg?style=flat
   :target: https://landscape.io/github/raamana/neuropredict/master
.. |codacy badge| image:: https://api.codacy.com/project/badge/grade/501e560b8a424562a1b8f7cd2f3cadfe
   :target: https://www.codacy.com/app/raamana/neuropredict?utm_source=github.com&utm_medium=referral&utm_content=raamana/neuropredict&utm_campaign=badge_grade
.. |pypi version| image:: https://badge.fury.io/py/neuropredict.svg
   :target: https://badge.fury.io/py/neuropredict
.. |python versions| image:: https://img.shields.io/badge/python-3.5%2c%203.6-blue.svg
.. |saythanks| image:: https://img.shields.io/badge/say-thanks-ff69b4.svg
   :target: https://saythanks.io/to/raamana
"
"imbalanced-learn",".. -*- mode: rst -*-

.. _scikit-learn: http://scikit-learn.org/stable/

.. _scikit-learn-contrib: https://github.com/scikit-learn-contrib

|azure|_ |codecov|_ |circleci|_ |pythonversion|_ |pypi|_ |gitter|_ |black|_

.. |azure| image:: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_apis/build/status/scikit-learn-contrib.imbalanced-learn?branchname=master
.. _azure: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_build

.. |codecov| image:: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn/branch/master/graph/badge.svg
.. _codecov: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn

.. |circleci| image:: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn.svg?style=shield&circle-token=:circle-token
.. _circleci: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn/tree/master

.. |pythonversion| image:: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg
.. _pythonversion: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg

.. |pypi| image:: https://badge.fury.io/py/imbalanced-learn.svg
.. _pypi: https://badge.fury.io/py/imbalanced-learn

.. |gitter| image:: https://badges.gitter.im/scikit-learn-contrib/imbalanced-learn.svg
.. _gitter: https://gitter.im/scikit-learn-contrib/imbalanced-learn?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg
.. _black: :target: https://github.com/psf/black

.. |pythonminversion| replace:: 3.8
.. |numpyminversion| replace:: 1.17.3
.. |scipyminversion| replace:: 1.3.2
.. |scikitlearnminversion| replace:: 1.0.2
.. |matplotlibminversion| replace:: 3.1.2
.. |pandasminversion| replace:: 1.0.5
.. |tensorflowminversion| replace:: 2.4.3
.. |kerasminversion| replace:: 2.4.3
.. |seabornminversion| replace:: 0.9.0
.. |pytestminversion| replace:: 5.0.1

imbalanced-learn
================

imbalanced-learn is a python package offering a number of re-sampling techniques
commonly used in datasets showing strong between-class imbalance.
it is compatible with scikit-learn_ and is part of scikit-learn-contrib_
projects.

documentation
-------------

installation documentation, api documentation, and examples can be found on the
documentation_.

.. _documentation: https://imbalanced-learn.org/stable/

installation
------------

dependencies
~~~~~~~~~~~~

`imbalanced-learn` requires the following dependencies:

- python (>= |pythonminversion|)
- numpy (>= |numpyminversion|)
- scipy (>= |scipyminversion|)
- scikit-learn (>= |scikitlearnminversion|)

additionally, `imbalanced-learn` requires the following optional dependencies:

- pandas (>= |pandasminversion|) for dealing with dataframes
- tensorflow (>= |tensorflowminversion|) for dealing with tensorflow models
- keras (>= |kerasminversion|) for dealing with keras models

the examples will requires the following additional dependencies:

- matplotlib (>= |matplotlibminversion|)
- seaborn (>= |seabornminversion|)

installation
~~~~~~~~~~~~

from pypi or conda-forge repositories
.....................................

imbalanced-learn is currently available on the pypi's repositories and you can
install it via `pip`::

  pip install -u imbalanced-learn

the package is release also in anaconda cloud platform::

  conda install -c conda-forge imbalanced-learn

from source available on github
...............................

if you prefer, you can clone it and run the setup.py file. use the following
commands to get a copy from github and install all dependencies::

  git clone https://github.com/scikit-learn-contrib/imbalanced-learn.git
  cd imbalanced-learn
  pip install .

be aware that you can install in developer mode with::

  pip install --no-build-isolation --editable .

if you wish to make pull-requests on github, we advise you to install
pre-commit::

  pip install pre-commit
  pre-commit install

testing
~~~~~~~

after installation, you can use `pytest` to run the test suite::

  make coverage

development
-----------

the development of this scikit-learn-contrib is in line with the one
of the scikit-learn community. therefore, you can refer to their
`development guide
<http://scikit-learn.org/stable/developers>`_.

about
-----

if you use imbalanced-learn in a scientific publication, we would appreciate
citations to the following paper::

  @article{jmlr:v18:16-365,
  author  = {guillaume  lema{{\^i}}tre and fernando nogueira and christos k. aridas},
  title   = {imbalanced-learn: a python toolbox to tackle the curse of imbalanced datasets in machine learning},
  journal = {journal of machine learning research},
  year    = {2017},
  volume  = {18},
  number  = {17},
  pages   = {1-5},
  url     = {http://jmlr.org/papers/v18/16-365}
  }

most classification algorithms will only perform optimally when the number of
samples of each class is roughly the same. highly skewed datasets, where the
minority is heavily outnumbered by one or more classes, have proven to be a
challenge while at the same time becoming more and more common.

one way of addressing this issue is by re-sampling the dataset as to offset this
imbalance with the hope of arriving at a more robust and fair decision boundary
than you would otherwise.

you can refer to the `imbalanced-learn`_ documentation to find details about
the implemented algorithms.

.. _imbalanced-learn: https://imbalanced-learn.org/stable/user_guide.html
"
"imbalanced-ensemble","![](https://raw.githubusercontent.com/zhiningliu1998/figures/master/imbalanced-ensemble/imbens-logo.png)

<!-- ![](https://raw.githubusercontent.com/zhiningliu1998/figures/master/imbalanced-ensemble/imbalanced_ensemble_header.png) -->

<!-- ![](https://raw.githubusercontent.com/zhiningliu1998/figures/master/imbalanced-ensemble/example_gallery_snapshot_horizontal.png) -->

<h1 align=""center"">
  imbens: class-imbalanced ensemble learning in python
</h1>

<p align=""center"">
  <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble"">
    <img src=""https://img.shields.io/badge/imbalanced-ensemble-orange"">
  </a>
  <a href='https://imbalanced-ensemble.readthedocs.io/en/latest/?badge=latest'>
    <img src='https://readthedocs.org/projects/imbalanced-ensemble/badge/?version=latest' alt='documentation status' />
  </a>
  <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/stargazers"">
    <img src=""https://img.shields.io/github/stars/zhiningliu1998/imbalanced-ensemble"">
  </a>
  <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/network/members"">
    <img src=""https://img.shields.io/github/forks/zhiningliu1998/imbalanced-ensemble"">
  </a>
  <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/issues"">
    <img src=""https://img.shields.io/github/issues/zhiningliu1998/imbalanced-ensemble"">
  </a>
  <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/blob/master/license"">
    <img src=""https://img.shields.io/github/license/zhiningliu1998/imbalanced-ensemble"">
  </a>
  <a href=""https://pypi.org/project/imbalanced-ensemble/"">
    <img src=""https://badge.fury.io/py/imbalanced-ensemble.svg"">
  </a>
  <a href=""https://www.python.org/"">
    <img src=""https://img.shields.io/pypi/pyversions/imbalanced-ensemble.svg"">
  </a>
  <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/graphs/traffic"">
    <img src=""https://visitor-badge.glitch.me/badge?page_id=zhiningliu1998.imbalanced-ensemble"">
  </a>
  <!-- all-contributors-badge:start - do not remove or modify this section -->
<a href=""https://github.com/zhiningliu1998/imbalanced-ensemble#contributors-""><img src=""https://img.shields.io/badge/all_contributors-4-orange.svg""></a>
<!-- all-contributors-badge:end -->
  <a href=""https://pepy.tech/project/imbalanced-ensemble"">
    <img src=""https://pepy.tech/badge/imbalanced-ensemble"">
  </a>
  <a href=""https://pepy.tech/project/imbalanced-ensemble"">
    <img src=""https://pepy.tech/badge/imbalanced-ensemble/month"">
  </a>
</p>

<h3 align=""center"">
language: <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble"">english</a> | <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/blob/main/docs/readme_cn.md"">chinese/‰∏≠Êñá</a>
</h3>

**release: 
  <a href=""https://pypi.org/project/imbalanced-ensemble/"">pypi</a> |
  <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/tree/main/imbalanced_ensemble"">source</a> |
  <a href=""https://pypi.org/project/imbalanced-ensemble/#files"">download</a> |
  <a href=""https://imbalanced-ensemble.readthedocs.io/en/latest/release_history.html"">changelog</a>  
  links: 
  <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble#5-min-quick-start-with-imbens"">getting started</a> |
  <a href=""https://imbalanced-ensemble.readthedocs.io/"">api reference</a> |
  <a href=""https://imbalanced-ensemble.readthedocs.io/en/latest/auto_examples/index.html#"">examples</a> |
  <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble#related-projects"">related projects</a> |
  <a href=""https://zhuanlan.zhihu.com/p/376572330"">Áü•‰πé/zhihu</a>  
  paper:
  <a href=""https://arxiv.org/abs/2111.12776"">""imbens: ensemble class-imbalanced learning in python""</a>**

***imbens*** (imported as `imbalanced_ensemble`) is a python library for quick implementation, modification, evaluation, and visualization of **ensemble [learning from class-imbalanced data](https://github.com/zhiningliu1998/awesome-imbalanced-learning)**. 
currently, imbens includes more than **[15 ensemble imbalanced learning algorithms](#list-of-implemented-methods)**, from the classical *smoteboost* (2003), *rusboost* (2010) to recent [*self-paced ensemble*](https://github.com/zhiningliu1998/self-paced-ensemble) (2020), from *resampling* to *cost-sensitive learning*.

***imbens*** is developed on top of [imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn) (imblearn) and follows the api design of [scikit-learn](https://github.com/scikit-learn/scikit-learn). compared to imblearn, imbens provides more powerful ensemble learning algorithms with ***multi-class learning*** support and many other **advanced features**:

<!-- the problem of learning from imbalanced data is known as imbalanced learning or long-tail learning (under multi-class scenario). see related papers/libraries/resources [here](https://github.com/zhiningliu1998/awesome-imbalanced-learning). -->
<!-- more algorithms will be included in the future. we also provide detailed documentation and examples across various algorithms. see full list of implemented methods [here](#list-of-implemented-methods).** -->

<!-- - ‚≠ê **please leave a <font color='orange'>star</font> if you like this project!** ‚≠ê
- **if you find any bugs or have any suggestions, please consider opening an issue or a pr.** 
- **we would greatly appreciate your contribution, and you will appear in the [contributors‚ú®](#contributors-)!** -->

<!-- **imbens is featured for:** -->
- &#x1f34e; unified, easy-to-use apis, detailed [documentation](https://imbalanced-ensemble.readthedocs.io/) and [examples](https://imbalanced-ensemble.readthedocs.io/en/latest/auto_examples/index.html#).
- &#x1f34e; capable for out-of-the-box ***multi-class*** imbalanced (long-tailed) learning.
- &#x1f34e; optimized performance with parallelization when possible using [joblib](https://github.com/joblib/joblib).
- &#x1f34e; powerful, customizable, interactive training logging and visualizer.
- &#x1f34e; full compatibility with other popular packages like [scikit-learn](https://scikit-learn.org/stable/) and [imbalanced-learn](https://imbalanced-learn.org/stable/).

**ensemble imbalanced learning with 4 lines of code:**
```python
# train an spe classifier
from imbalanced_ensemble.ensemble import selfpacedensembleclassifier
clf = selfpacedensembleclassifier(random_state=42)
clf.fit(x_train, y_train)

# predict with an spe classifier
y_pred = clf.predict(x_test)
```
**citing imbens**

the [imbens paper](https://arxiv.org/pdf/2111.12776.pdf) is available on arxiv.
if you use imbens in a scientific publication, we would appreciate citations to the following paper:

```bib
@article{liu2021imbens,
  title={imbens: ensemble class-imbalanced learning in python},
  author={liu, zhining and wei, zhepei and yu, erxin and huang, qiang and guo, kai and yu, boyang and cai, zhaonian and ye, hangting and cao, wei and bian, jiang and wei, pengfei and jiang, jing and chang, yi},
  journal={arxiv preprint arxiv:2111.12776},
  year={2021}
}
```

## table of contents

- [table of contents](#table-of-contents)
- [installation](#installation)
- [highlights](#highlights)
- [list of implemented methods](#list-of-implemented-methods)
- [5-min quick start with imbens](#5-min-quick-start-with-imbens)
  - [a minimal working example](#a-minimal-working-example)
  - [visualize ensemble classifiers](#visualize-ensemble-classifiers)
  - [customizing training log](#customizing-training-log)
- [about imbalanced learning](#about-imbalanced-learning)
- [acknowledgements](#acknowledgements)
- [references](#references)
- [related projects](#related-projects)
- [contributors ‚ú®](#contributors-)


## installation

it is recommended to use **pip** for installation.  
please make sure the **latest version** is installed to avoid potential problems:
```shell
$ pip install imbalanced-ensemble            # normal install
$ pip install --upgrade imbalanced-ensemble  # update if needed
```

or you can install imbalanced-ensemble by clone this repository:
```shell
$ git clone https://github.com/zhiningliu1998/imbalanced-ensemble.git
$ cd imbalanced-ensemble
$ pip install .
```

imbalanced-ensemble requires following dependencies:

- [python](https://www.python.org/) (>=3.6)
- [numpy](https://numpy.org/) (>=1.16.0)
- [pandas](https://pandas.pydata.org/) (>=1.1.3)
- [scipy](https://www.scipy.org/) (>=0.19.1)
- [joblib](https://pypi.org/project/joblib/) (>=0.11)
- [scikit-learn](https://scikit-learn.org/stable/) (>=1.0.0)
- [matplotlib](https://matplotlib.org/) (>=3.3.2)
- [seaborn](https://seaborn.pydata.org/) (>=0.11.0)
- [tqdm](https://tqdm.github.io/) (>=4.50.2)


## highlights

- &#x1f34e; ***unified, easy-to-use api design.***  
all ensemble learning methods implemented in imbens share a unified api design. 
similar to sklearn, all methods have functions (e.g., `fit()`, `predict()`, `predict_proba()`) that allow users to deploy them with only a few lines of code.
- &#x1f34e; ***extended functionalities, wider application scenarios.***  
*all methods in imbens are ready for **multi-class imbalanced classification**.* we extend binary ensemble imbalanced learning methods to get them to work under the multi-class scenario. additionally, for supported methods, we provide more training options like class-wise resampling control, balancing scheduler during the ensemble training process, etc.
- &#x1f34e; ***detailed training log, quick intuitive visualization.***   
we provide additional parameters (e.g., `eval_datasets`, `eval_metrics`, `training_verbose`) in `fit()` for users to control the information they want to monitor during the ensemble training. we also implement an [`ensemblevisualizer`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/visualizer/_autosummary/imbalanced_ensemble.visualizer.imbalancedensemblevisualizer.html) to quickly visualize the ensemble estimator(s) for providing further information/conducting comparison. see an example [here](https://imbalanced-ensemble.readthedocs.io/en/latest/auto_examples/basic/plot_basic_example.html#sphx-glr-auto-examples-basic-plot-basic-example-py).
- &#x1f34e; ***wide compatiblilty.***   
imbens is designed to be compatible with [scikit-learn](https://scikit-learn.org/stable/) (sklearn) and also other compatible projects like [imbalanced-learn](https://imbalanced-learn.org/stable/). therefore, users can take advantage of various utilities from the sklearn community for data processing/cross-validation/hyper-parameter tuning, etc.

<!-- ## background

class-imbalance (also known as the long-tail problem in multi-class) is the fact that the classes are not represented equally in a classification problem, which is quite common in practice. for instance, fraud detection, prediction of rare adverse drug reactions and prediction gene families. failure to account for the class imbalance often causes inaccurate and decreased predictive performance of many classification algorithms.

imbalanced learning (il) aims to tackle the class imbalance problem to learn an unbiased model from imbalanced data. this is usually achieved by changing the training data distribution by resampling or reweighting. however, naive resampling or reweighting may introduce bias/variance to the training data, especially when the data has class-overlapping or contains noise.

ensemble imbalanced learning (eil) is known to effectively improve typical il solutions by combining the outputs of multiple classifiers, thereby reducing the variance introduce by resampling/reweighting. -->

## list of implemented methods

**currently (v0.1.3, 2021/06), *16* ensemble imbalanced learning methods were implemented:  
(click to jump to the document page)**

- **resampling-based**
  - *under-sampling + ensemble*
    1. **[`selfpacedensembleclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.under_sampling.selfpacedensembleclassifier.html) [1] ([in github](https://github.com/zhiningliu1998/self-paced-ensemble))**
    2. **[`balancecascadeclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.under_sampling.balancecascadeclassifier.html) [2]**
    3. **[`balancedrandomforestclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.under_sampling.balancedrandomforestclassifier.html) [3] ([imblearn version](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.balancedrandomforestclassifier.html))**
    4. **[`easyensembleclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.under_sampling.easyensembleclassifier.html) [2] ([imblearn version](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.easyensembleclassifier.html))**
    5. **[`rusboostclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.under_sampling.rusboostclassifier.html) [4] ([imblearn version](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.rusboostclassifier.html))**
    6. **[`underbaggingclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.under_sampling.underbaggingclassifier.html) [5] ([imblearn version](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.balancedbaggingclassifier.html))**
  - *over-sampling + ensemble*
    1. **[`overboostclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.over_sampling.overboostclassifier.html)**
    2. **[`smoteboostclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.over_sampling.smoteboostclassifier.html) [6]**
    3. **[`kmeanssmoteboostclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.over_sampling.kmeanssmoteboostclassifier.html)**
    4. **[`overbaggingclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.over_sampling.overbaggingclassifier.html) [5] ([imblearn version](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.balancedbaggingclassifier.html))**
    5. **[`smotebaggingclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.over_sampling.smotebaggingclassifier.html) [7] ([imblearn version](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.balancedbaggingclassifier.html))**
- **reweighting-based**
  - *cost-sensitive learning*
    1. **[`adacostclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.reweighting.adacostclassifier.html) [8]**
    2. **[`adauboostclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.reweighting.adauboostclassifier.html) [9]**
    3. **[`asymboostclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.reweighting.asymboostclassifier.html) [10]**
- **compatible**
  - **[`compatibleadaboostclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.compatible.compatibleadaboostclassifier.html) [11]**
  - **[`compatiblebaggingclassifier`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.compatible.compatiblebaggingclassifier.html) [12]**

> **note: `imbalanced-ensemble` is still under development, please see [api reference](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/api.html) for the latest list.**

## 5-min quick start with imbens

**here, we provide some quick guides to help you get started with imbens.**  
**we strongly encourage users to check out the [**example gallery**](https://imbalanced-ensemble.readthedocs.io/en/latest/auto_examples/index.html#) for more comprehensive usage examples, which demonstrate many advanced features of imbens.**

![](https://raw.githubusercontent.com/zhiningliu1998/figures/master/imbalanced-ensemble/example_gallery_snapshot.png)

### a minimal working example

taking self-paced ensemble [1] as an example, it only requires less than 10 lines of code to deploy it:

```python
>>> from imbalanced_ensemble.ensemble import selfpacedensembleclassifier
>>> from sklearn.datasets import make_classification
>>> from sklearn.model_selection import train_test_split
>>> 
>>> x, y = make_classification(n_samples=1000, n_classes=3,
...                            n_informative=4, weights=[0.2, 0.3, 0.5],
...                            random_state=0)
>>> x_train, x_test, y_train, y_test = train_test_split(
...                            x, y, test_size=0.2, random_state=42)
>>> clf = selfpacedensembleclassifier(random_state=0)
>>> clf.fit(x_train, y_train)
selfpacedensembleclassifier(...)
>>> clf.predict(x_test)  
array([...])
```

### visualize ensemble classifiers

the [`imbalanced_ensemble.visualizer`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/visualizer/api.html) sub-module provide an [`imbalancedensemblevisualizer`](https://imbalanced-ensemble.readthedocs.io/en/latest/api/visualizer/_autosummary/imbalanced_ensemble.visualizer.imbalancedensemblevisualizer.html).
it can be used to visualize the ensemble estimator(s) for further information or comparison.
please refer to [**visualizer documentation**](https://imbalanced-ensemble.readthedocs.io/en/latest/api/visualizer/_autosummary/imbalanced_ensemble.visualizer.imbalancedensemblevisualizer.html) and [**examples**](https://imbalanced-ensemble.readthedocs.io/en/latest/auto_examples/index.html) for more details.

**fit an imbalancedensemblevisualizer**
```python
from imbalanced_ensemble.ensemble import selfpacedensembleclassifier
from imbalanced_ensemble.ensemble import rusboostclassifier
from imbalanced_ensemble.ensemble import easyensembleclassifier
from sklearn.tree import decisiontreeclassifier

# fit ensemble classifiers
init_kwargs = {'base_estimator': decisiontreeclassifier()}
ensembles = {
    'spe': selfpacedensembleclassifier(**init_kwargs).fit(x_train, y_train),
    'rusboost': rusboostclassifier(**init_kwargs).fit(x_train, y_train),
    'easyens': easyensembleclassifier(**init_kwargs).fit(x_train, y_train),
}

# fit visualizer
from imbalanced_ensemble.visualizer import imbalancedensemblevisualizer
visualizer = imbalancedensemblevisualizer().fit(ensembles=ensembles)
```
**plot performance curves**
```python
fig, axes = visualizer.performance_lineplot()
```
![](https://raw.githubusercontent.com/zhiningliu1998/figures/master/imbalanced-ensemble/examples/visualize_performance_example.png)

**plot confusion matrices**
```python
fig, axes = visualizer.confusion_matrix_heatmap()
```
![](https://raw.githubusercontent.com/zhiningliu1998/figures/master/imbalanced-ensemble/examples/visualize_confusion_matrix_example.png)

### customizing training log

all ensemble classifiers in imbens support customizable training logging.
the training log is controlled by 3 parameters `eval_datasets`, `eval_metrics`, and `training_verbose` of the `fit()` method.
read more details in the [**fit documentation**](https://imbalanced-ensemble.readthedocs.io/en/latest/api/ensemble/_autosummary/imbalanced_ensemble.ensemble.under_sampling.selfpacedensembleclassifier.html#imbalanced_ensemble.ensemble.under_sampling.selfpacedensembleclassifier.fit).

**enable auto training log**
```python
clf.fit(..., train_verbose=true)
```
```
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ             ‚îÉ                          ‚îÉ            data: train             ‚îÉ
‚îÉ #estimators ‚îÉ    class distribution    ‚îÉ               metric               ‚îÉ
‚îÉ             ‚îÉ                          ‚îÉ  acc    balanced_acc   weighted_f1 ‚îÉ
‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´
‚îÉ      1      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.838      0.877          0.839    ‚îÉ
‚îÉ      5      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.924      0.949          0.924    ‚îÉ
‚îÉ     10      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.954      0.970          0.954    ‚îÉ
‚îÉ     15      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.979      0.986          0.979    ‚îÉ
‚îÉ     20      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.990      0.993          0.990    ‚îÉ
‚îÉ     25      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.994      0.996          0.994    ‚îÉ
‚îÉ     30      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.988      0.992          0.988    ‚îÉ
‚îÉ     35      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.999      0.999          0.999    ‚îÉ
‚îÉ     40      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.995      0.997          0.995    ‚îÉ
‚îÉ     45      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.995      0.997          0.995    ‚îÉ
‚îÉ     50      ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.993      0.995          0.993    ‚îÉ
‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´
‚îÉ    final    ‚îÉ {0: 150, 1: 150, 2: 150} ‚îÉ 0.993      0.995          0.993    ‚îÉ
‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ
```


**customize granularity and content of the training log**
```python
clf.fit(..., 
        train_verbose={
            'granularity': 10,
            'print_distribution': false,
            'print_metrics': true,
        })
```

<details><summary> click to view example output </summary>

```
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ             ‚îÉ            data: train             ‚îÉ
‚îÉ #estimators ‚îÉ               metric               ‚îÉ
‚îÉ             ‚îÉ  acc    balanced_acc   weighted_f1 ‚îÉ
‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´
‚îÉ      1      ‚îÉ 0.964      0.970          0.964    ‚îÉ
‚îÉ     10      ‚îÉ 1.000      1.000          1.000    ‚îÉ
‚îÉ     20      ‚îÉ 1.000      1.000          1.000    ‚îÉ
‚îÉ     30      ‚îÉ 1.000      1.000          1.000    ‚îÉ
‚îÉ     40      ‚îÉ 1.000      1.000          1.000    ‚îÉ
‚îÉ     50      ‚îÉ 1.000      1.000          1.000    ‚îÉ
‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´
‚îÉ    final    ‚îÉ 1.000      1.000          1.000    ‚îÉ
‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ
```

</details>

**add evaluation dataset(s)**
```python
  clf.fit(..., 
          eval_datasets={
              'valid': (x_valid, y_valid)
          })
```

<details><summary> click to view example output </summary>

```
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ             ‚îÉ            data: train             ‚îÉ            data: valid             ‚îÉ
‚îÉ #estimators ‚îÉ               metric               ‚îÉ               metric               ‚îÉ
‚îÉ             ‚îÉ  acc    balanced_acc   weighted_f1 ‚îÉ  acc    balanced_acc   weighted_f1 ‚îÉ
‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´
‚îÉ      1      ‚îÉ 0.939      0.961          0.940    ‚îÉ 0.935      0.933          0.936    ‚îÉ
‚îÉ     10      ‚îÉ 1.000      1.000          1.000    ‚îÉ 0.971      0.974          0.971    ‚îÉ
‚îÉ     20      ‚îÉ 1.000      1.000          1.000    ‚îÉ 0.982      0.981          0.982    ‚îÉ
‚îÉ     30      ‚îÉ 1.000      1.000          1.000    ‚îÉ 0.983      0.983          0.983    ‚îÉ
‚îÉ     40      ‚îÉ 1.000      1.000          1.000    ‚îÉ 0.983      0.982          0.983    ‚îÉ
‚îÉ     50      ‚îÉ 1.000      1.000          1.000    ‚îÉ 0.983      0.982          0.983    ‚îÉ
‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´
‚îÉ    final    ‚îÉ 1.000      1.000          1.000    ‚îÉ 0.983      0.982          0.983    ‚îÉ
‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ
```

</details>

**customize evaluation metric(s)**
```python
from sklearn.metrics import accuracy_score, f1_score
clf.fit(..., 
        eval_metrics={
            'acc': (accuracy_score, {}),
            'weighted_f1': (f1_score, {'average':'weighted'}),
        })
```

<details><summary> click to view example output </summary>

```
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ             ‚îÉ     data: train      ‚îÉ     data: valid      ‚îÉ
‚îÉ #estimators ‚îÉ        metric        ‚îÉ        metric        ‚îÉ
‚îÉ             ‚îÉ  acc    weighted_f1  ‚îÉ  acc    weighted_f1  ‚îÉ
‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´
‚îÉ      1      ‚îÉ 0.942      0.961     ‚îÉ 0.919      0.936     ‚îÉ
‚îÉ     10      ‚îÉ 1.000      1.000     ‚îÉ 0.976      0.976     ‚îÉ
‚îÉ     20      ‚îÉ 1.000      1.000     ‚îÉ 0.977      0.977     ‚îÉ
‚îÉ     30      ‚îÉ 1.000      1.000     ‚îÉ 0.981      0.980     ‚îÉ
‚îÉ     40      ‚îÉ 1.000      1.000     ‚îÉ 0.980      0.979     ‚îÉ
‚îÉ     50      ‚îÉ 1.000      1.000     ‚îÉ 0.981      0.980     ‚îÉ
‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´
‚îÉ    final    ‚îÉ 1.000      1.000     ‚îÉ 0.981      0.980     ‚îÉ
‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ
```

</details>

## about imbalanced learning

**class-imbalance** (also known as the **long-tail problem**) is the fact that the classes are not represented equally in a classification problem, which is quite common in practice. for instance, fraud detection, prediction of rare adverse drug reactions and prediction gene families. failure to account for the class imbalance often causes inaccurate and decreased predictive performance of many classification algorithms. **imbalanced learning** aims to tackle the class imbalance problem to learn an unbiased model from imbalanced data.

for more resources on imbalanced learning, please refer to [**awesome-imbalanced-learning**](https://github.com/zhiningliu1998/awesome-imbalanced-learning).

## acknowledgements

***many samplers and utilities are adapted from* [imbalanced-learn](https://imbalanced-learn.org/), *which is an amazing project!***


## references

| #   | reference |
|-----|-------|
| [1] | zhining liu, wei cao, zhifeng gao, jiang bian, hechang chen, yi chang, and tie-yan liu. 2019. self-paced ensemble for highly imbalanced massive data classification. 2020 ieee 36th international conference on data engineering (icde). ieee, 2020, pp. 841-852. |
| [2] | x.-y. liu, j. wu, and z.-h. zhou, exploratory undersampling for class-imbalance learning. ieee transactions on systems, man, and cybernetics, part b (cybernetics), vol. 39, no. 2, pp. 539‚Äì550, 2009. |
| [3] | chen, chao, andy liaw, and leo breiman. ‚Äúusing random forest to learn imbalanced data.‚Äù university of california, berkeley 110 (2004): 1-12. |
| [4] | c. seiffert, t. m. khoshgoftaar, j. van hulse, and a. napolitano, rusboost: a hybrid approach to alleviating class imbalance. ieee transactions on systems, man, and cybernetics-part a: systems and humans, vol. 40, no. 1, pp. 185‚Äì197, 2010. |
| [5] | maclin, r., & opitz, d. (1997). an empirical evaluation of bagging and boosting. aaai/iaai, 1997, 546-551. |
| [6] | n. v. chawla, a. lazarevic, l. o. hall, and k. w. bowyer, smoteboost: improving prediction of the minority class in boosting. in european conference on principles of data mining and knowledge discovery. springer, 2003, pp. 107‚Äì119|
| [7] | s. wang and x. yao, diversity analysis on imbalanced data sets by using ensemble models. in 2009 ieee symposium on computational intelligence and data mining. ieee, 2009, pp. 324‚Äì331.|
| [8] | fan, w., stolfo, s. j., zhang, j., & chan, p. k. (1999, june). adacost: misclassification cost-sensitive boosting. in icml (vol. 99, pp. 97-105). |
| [9] | shawe-taylor, g. k. j., & karakoulas, g. (1999). optimizing classifiers for imbalanced training sets. advances in neural information processing systems, 11(11), 253. |
| [10] | viola, p., & jones, m. (2001). fast and robust classification using asymmetric adaboost and a detector cascade. advances in neural information processing system, 14. |
| [11] | freund, y., & schapire, r. e. (1997). a decision-theoretic generalization of on-line learning and an application to boosting. journal of computer and system sciences, 55(1), 119-139. |
| [12] | breiman, l. (1996). bagging predictors. machine learning, 24(2), 123-140. |
| [13] | guillaume lema√Ætre, fernando nogueira, and christos k. aridas. imbalanced-learn: a python toolbox to tackle the curse of imbalanced datasets in machine learning. journal of machine learning research, 18(17):1‚Äì5, 2017. |

## related projects

**check out [zhining](https://zhiningliu.com/)'s other open-source projects!**  
<table style=""font-size:15px;"">
  <tr>
    <td align=""center""><a href=""https://github.com/zhiningliu1998/awesome-imbalanced-learning""><img src=""https://raw.githubusercontent.com/zhiningliu1998/figures/master/thumbnails/awesomeil-thumb.png"" height=""80px"" alt=""""/><br /><sub><b>imbalanced learning [awesome]</b></sub></a><br />
      <a href=""https://github.com/zhiningliu1998/awesome-imbalanced-learning/stargazers"">
      <img alt=""github stars"" src=""https://img.shields.io/github/stars/zhiningliu1998/awesome-imbalanced-learning?style=social"">
      </a>
    </td>
    <td align=""center""><a href=""https://github.com/zhiningliu1998/awesome-awesome-machine-learning""><img src=""https://raw.githubusercontent.com/zhiningliu1998/figures/master/thumbnails/awesomeml-thumb.png"" height=""80px"" alt=""""/><br /><sub><b>machine learning [awesome]</b></sub></a><br />
      <a href=""https://github.com/zhiningliu1998/awesome-awesome-machine-learning/stargazers"">
      <img alt=""github stars"" src=""https://img.shields.io/github/stars/zhiningliu1998/awesome-awesome-machine-learning?style=social"">
      </a>
    </td>
    <td align=""center""><a href=""https://github.com/zhiningliu1998/self-paced-ensemble""><img src=""https://raw.githubusercontent.com/zhiningliu1998/figures/master/thumbnails/spe-thumb-1.png"" height=""80px"" alt=""""/><br /><sub><b>self-paced ensemble [icde]</b></sub></a><br />
      <a href=""https://github.com/zhiningliu1998/self-paced-ensemble/stargazers"">
      <img alt=""github stars"" src=""https://img.shields.io/github/stars/zhiningliu1998/self-paced-ensemble?style=social"">
      </a>
    </td>
    <td align=""center""><a href=""https://github.com/zhiningliu1998/mesa""><img src=""https://raw.githubusercontent.com/zhiningliu1998/figures/master/thumbnails/mesa-thumb.png"" height=""80px"" alt=""""/><br /><sub><b>meta-sampler [neurips]</b></sub></a><br />
      <a href=""https://github.com/zhiningliu1998/mesa/stargazers"">
      <img alt=""github stars"" src=""https://img.shields.io/github/stars/zhiningliu1998/mesa?style=social"">
      </a>
    </td>
  </tr>
</table>


## contributors ‚ú®

thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

<!-- all-contributors-list:start - do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tr>
    <td align=""center""><a href=""http://zhiningliu.com""><img src=""https://avatars.githubusercontent.com/u/26108487?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>zhining liu</b></sub></a><br /><a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/commits?author=zhiningliu1998"" title=""code"">üíª</a> <a href=""#ideas-zhiningliu1998"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""#maintenance-zhiningliu1998"" title=""maintenance"">üöß</a> <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/issues?q=author%3azhiningliu1998"" title=""bug reports"">üêõ</a> <a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/commits?author=zhiningliu1998"" title=""documentation"">üìñ</a></td>
    <td align=""center""><a href=""https://github.com/leaphan""><img src=""https://avatars.githubusercontent.com/u/35593707?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>leaphan</b></sub></a><br /><a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/issues?q=author%3aleaphan"" title=""bug reports"">üêõ</a></td>
    <td align=""center""><a href=""https://github.com/hannanhtang""><img src=""https://avatars.githubusercontent.com/u/23587399?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>hannanhtang</b></sub></a><br /><a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/issues?q=author%3ahannanhtang"" title=""bug reports"">üêõ</a></td>
    <td align=""center""><a href=""https://github.com/huajuanren""><img src=""https://avatars.githubusercontent.com/u/37321841?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>h.j.ren</b></sub></a><br /><a href=""https://github.com/zhiningliu1998/imbalanced-ensemble/issues?q=author%3ahuajuanren"" title=""bug reports"">üêõ</a></td>
  </tr>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- all-contributors-list:end -->

this project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. contributions of any kind welcome!
"
"pyhsmm","[![build
status](https://travis-ci.org/mattjj/pyhsmm.svg?branch=master)](https://travis-ci.org/mattjj/pyhsmm)

# bayesian inference in hsmms and hmms #

this is a python library for approximate unsupervised inference in
bayesian hidden markov models (hmms) and explicit-duration hidden semi-markov
models (hsmms), focusing on the bayesian nonparametric extensions, the hdp-hmm
and hdp-hsmm, mostly with weak-limit approximations.

there are also some extensions:

* [autoregressive models](https://github.com/mattjj/pyhsmm-autoregressive)
* [switching linear dynamical systems](https://github.com/mattjj/pyhsmm-slds)
* [factorial models](https://github.com/mattjj/pyhsmm-factorial)

## installing from pypi ##

give this a shot:

```bash
pip install pyhsmm
```

you may need to install a compiler with `-std=c++11` support, like gcc-4.7 or higher.

to install manually from the git repo, you'll need `cython`. then try this:

```bash
python setup.py install
```

it might also help to look at the [travis file](https://raw.githubusercontent.com/mattjj/pyhsmm/master/.travis.yml) to
see how to set up a working install from scratch.

## running ##

see the examples directory.

for the python interpreter to be able to import pyhsmm, you'll need it on your
python path. since the current working directory is usually included in the
python path, you can probably run the examples from the same directory in which
you run the git clone with commands like `python pyhsmm/examples/hsmm.py`. you
might also want to add pyhsmm to your global python path (e.g. by copying it to
your site-packages directory).

## a simple demonstration ##

here's how to draw from the hdp-hsmm posterior over hsmms given a sequence of
observations. (the same example, along with the code to generate the synthetic
data loaded in this example, can be found in `examples/basic.py`.)

let's say we have some 2d data in a data.txt file:

```bash
$ head -5 data.txt
-3.711962552600095444e-02 1.456401745267922598e-01
7.553818775915704942e-02 2.457422192223903679e-01
-2.465977987699214502e+00 5.537627981813508793e-01
-7.031638516485749779e-01 1.536468304146855757e-01
-9.224669847039665971e-01 3.680035337673161489e-01
```

in python, we can plot the data in a 2d plot, collapsing out the time dimension:

```python
import numpy as np
from matplotlib import pyplot as plt

data = np.loadtxt('data.txt')
plt.plot(data[:,0],data[:,1],'kx')
```

![2d data](https://raw.githubusercontent.com/mattjj/pyhsmm/master/images/data.png)

we can also make a plot of time versus the first principal component:

```python
from pyhsmm.util.plot import pca_project_data
plt.plot(pca_project_data(data,1))
```

![data first principal component vs time](https://raw.githubusercontent.com/mattjj/pyhsmm/master/images/data_vs_time.png)

to learn an hsmm, we'll use `pyhsmm` to create a `weaklimithdphsmm` instance
using some reasonable hyperparameters. we'll ask this model to infer the number
of states as well, so we'll give it an `nmax` parameter:

```python
import pyhsmm
import pyhsmm.basic.distributions as distributions

obs_dim = 2
nmax = 25

obs_hypparams = {'mu_0':np.zeros(obs_dim),
                'sigma_0':np.eye(obs_dim),
                'kappa_0':0.3,
                'nu_0':obs_dim+5}
dur_hypparams = {'alpha_0':2*30,
                 'beta_0':2}

obs_distns = [distributions.gaussian(**obs_hypparams) for state in range(nmax)]
dur_distns = [distributions.poissonduration(**dur_hypparams) for state in range(nmax)]

posteriormodel = pyhsmm.models.weaklimithdphsmm(
        alpha=6.,gamma=6., # better to sample over these; see concentration-resampling.py
        init_state_concentration=6., # pretty inconsequential
        obs_distns=obs_distns,
        dur_distns=dur_distns)
```

(the first two arguments set the ""new-table"" proportionality constant for the
meta-chinese restaurant process and the other crps, respectively, in the hdp
prior on transition matrices. for this example, they really don't matter at
all, but on real data it's much better to infer these parameters, as in
`examples/concentration_resampling.py`.)

then, we add the data we want to condition on:

```python
posteriormodel.add_data(data,trunc=60)
```

the `trunc` parameter is an optional argument that can speed up inference: it
sets a truncation limit on the maximum duration for any state. if you don't
pass in the `trunc` argument, no truncation is used and all possible state
duration lengths are considered. (pyhsmm has fancier ways to speed up message
passing over durations, but they aren't documented.)

if we had multiple observation sequences to learn from, we could add them to the
model just by calling `add_data()` for each observation sequence.

now we run a resampling loop. for each iteration of the loop, all the latent
variables of the model will be resampled by gibbs sampling steps, including the
transition matrix, the observation means and covariances, the duration
parameters, and the hidden state sequence. we'll also copy some samples so that
we can plot them.

```python
models = []
for idx in progprint_xrange(150):
    posteriormodel.resample_model()
    if (idx+1) % 10 == 0:
        models.append(copy.deepcopy(posteriormodel))
```

now we can plot our saved samples:

```python
fig = plt.figure()
for idx, model in enumerate(models):
    plt.clf()
    model.plot()
    plt.gcf().suptitle('hdp-hsmm sampled after %d iterations' % (10*(idx+1)))
    plt.savefig('iter_%.3d.png' % (10*(idx+1)))
```

![sampled models](https://raw.githubusercontent.com/mattjj/pyhsmm/master/images/posterior_animation.gif)

i generated these data from an hsmm that looked like this:

![randomly-generated model and data](https://raw.githubusercontent.com/mattjj/pyhsmm/master/images/truth.png)

so the posterior samples look pretty good!

a convenient shortcut to build a list of sampled models is to write

```python
model_samples = [model.resample_and_copy() for itr in progprint_xrange(150)]
```

that will build a list of model objects (each of which can be inspected,
plotted, pickled, etc, independently) in a way that won't duplicate data that
isn't changed (like the observations or hyperparameter arrays) so that memory
usage is minimized. it also minimizes file size if you save samples like

```python
import cpickle
with open('sampled_models.pickle','w') as outfile:
    cpickle.dump(model_samples,outfile,protocol=-1)
```

## extending the code ##
to add your own observation or duration distributions, implement the interfaces
defined in `basic/abstractions.py`. to get a flavor of
the style, see [pybasicbayes](https://github.com/mattjj/pybasicbayes).

## references ##
* matthew j. johnson. [bayesian time series models and scalable
  inference](http://www.mit.edu/~mattjj/thesis.pdf). mit phd thesis, may 2014.

* matthew j. johnson and alan s. willsky. [bayesian nonparametric hidden
  semi-markov models](http://www.jmlr.org/papers/volume14/johnson13a/johnson13a.pdf).
  journal of machine learning research (jmlr), 14:673‚Äì701, february 2013.

* matthew j. johnson and alan s. willsky, [the hierarchical dirichlet process
  hidden semi-markov model](http://www.mit.edu/~mattjj/papers/uai2010.pdf). 26th
  conference on uncertainty in artificial intelligence (uai 2010), avalon,
  california, july 2010.

```bibtex
@article{johnson2013hdphsmm,
    title={bayesian nonparametric hidden semi-markov models},
    author={johnson, matthew j. and willsky, alan s.},
    journal={journal of machine learning research},
    pages={673--701},
    volume={14},
    month={february},
    year={2013},
}
```

## authors ##

[matt johnson](https://github.com/mattjj), [alex wiltschko](https://github.com/alexbw), [yarden katz](https://github.com/yarden), [chia-ying (jackie) lee](https://github.com/jacquelinecelia), [scott linderman](https://github.com/slinderman), [kevin squire](https://github.com/kmsquire), [nick foti](https://github.com/nfoti).

"
"SKLL","scikit-learn laboratory
-----------------------

.. image:: https://gitlab.com/educationaltestingservice/skll/badges/main/pipeline.svg
   :target: https://gitlab.com/educationaltestingservice/skll/-/pipelines
   :alt: gitlab ci status

.. image:: https://dev.azure.com/educationaltestingservice/skll/_apis/build/status/educationaltestingservice.skll
   :target: https://dev.azure.com/educationaltestingservice/skll/_build?view=runs
   :alt: azure pipelines status

.. image:: https://codecov.io/gh/educationaltestingservice/skll/branch/main/graph/badge.svg
  :target: https://codecov.io/gh/educationaltestingservice/skll

.. image:: https://img.shields.io/pypi/v/skll.svg
   :target: https://pypi.org/project/skll/
   :alt: latest version on pypi

.. image:: https://img.shields.io/pypi/l/skll.svg
   :alt: license

.. image:: https://img.shields.io/conda/v/ets/skll.svg
   :target: https://anaconda.org/ets/skll
   :alt: conda package for skll

.. image:: https://img.shields.io/pypi/pyversions/skll.svg
   :target: https://pypi.org/project/skll/
   :alt: supported python versions for skll

.. image:: https://img.shields.io/badge/doi-10.5281%2fzenodo.12825-blue.svg
   :target: http://dx.doi.org/10.5281/zenodo.12825
   :alt: doi for citing skll 1.0.0

.. image:: https://mybinder.org/badge_logo.svg
 :target: https://mybinder.org/v2/gh/educationaltestingservice/skll/main?filepath=examples%2ftutorial.ipynb


this python package provides command-line utilities to make it easier to run
machine learning experiments with scikit-learn.  one of the primary goals of
our project is to make it so that you can run scikit-learn experiments without
actually needing to write any code other than what you used to generate/extract
the features.

installation
~~~~~~~~~~~~

you can install using either ``pip`` or ``conda``. see details `here <https://skll.readthedocs.io/en/latest/getting_started.html>`__.

requirements
~~~~~~~~~~~~

-  python 3.8, 3.9, or 3.10
-  `beautifulsoup4 <http://www.crummy.com/software/beautifulsoup/>`__
-  `gridmap <https://pypi.org/project/gridmap/>`__ (only required if you plan
   to run things in parallel on a drmaa-compatible cluster)
-  `joblib <https://pypi.org/project/joblib/>`__
-  `pandas <http://pandas.pydata.org>`__
-  `ruamel.yaml <http://yaml.readthedocs.io/en/latest/overview.html>`__
-  `scikit-learn <http://scikit-learn.org/stable/>`__
-  `seaborn <http://seaborn.pydata.org>`__
-  `tabulate <https://pypi.org/project/tabulate/>`__

command-line interface
~~~~~~~~~~~~~~~~~~~~~~

the main utility we provide is called ``run_experiment`` and it can be used to
easily run a series of learners on datasets specified in a configuration file
like:

.. code:: ini

  [general]
  experiment_name = titanic_evaluate_tuned
  # valid tasks: cross_validate, evaluate, predict, train
  task = evaluate

  [input]
  # these directories could also be absolute paths
  # (and must be if you're not running things in local mode)
  train_directory = train
  test_directory = dev
  # can specify multiple sets of feature files that are merged together automatically
  featuresets = [[""family.csv"", ""misc.csv"", ""socioeconomic.csv"", ""vitals.csv""]]
  # list of scikit-learn learners to use
  learners = [""randomforestclassifier"", ""decisiontreeclassifier"", ""svc"", ""multinomialnb""]
  # column in csv containing labels to predict
  label_col = survived
  # column in csv containing instance ids (if any)
  id_col = passengerid

  [tuning]
  # should we tune parameters of all learners by searching provided parameter grids?
  grid_search = true
  # function to maximize when performing grid search
  objectives = ['accuracy']

  [output]
  # also compute the area under the roc curve as an additional metric
  metrics = ['roc_auc']
  # the following can also be absolute paths
  logs = output
  results = output
  predictions = output
  probability = true
  models = output

for more information about getting started with ``run_experiment``, please check
out `our tutorial <https://skll.readthedocs.org/en/latest/tutorial.html>`__, or
`our config file specs <https://skll.readthedocs.org/en/latest/run_experiment.html>`__.

you can also follow this `interactive jupyter tutorial <https://mybinder.org/v2/gh/avajpayeejr/skll/feature/448-interactive-binder?filepath=examples>`__.

we also provide utilities for:

-  `converting between machine learning toolkit formats <https://skll.readthedocs.org/en/latest/utilities.html#skll-convert>`__
   (e.g., arff, csv)
-  `filtering feature files <https://skll.readthedocs.org/en/latest/utilities.html#filter-features>`__
-  `joining feature files <https://skll.readthedocs.org/en/latest/utilities.html#join-features>`__
-  `other common tasks <https://skll.readthedocs.org/en/latest/utilities.html>`__


python api
~~~~~~~~~~

if you just want to avoid writing a lot of boilerplate learning code, you can
also use our simple python api which also supports pandas dataframes.
the main way you'll want to use the api is through
the ``learner`` and ``reader`` classes. for more details on our api, see
`the documentation <https://skll.readthedocs.org/en/latest/api.html>`__.

while our api can be broadly useful, it should be noted that the command-line
utilities are intended as the primary way of using skll.  the api is just a nice
side-effect of our developing the utilities.


a note on pronunciation
~~~~~~~~~~~~~~~~~~~~~~~

.. image:: doc/skll.png
   :alt: skll logo
   :align: right

.. container:: clear

  .. image:: doc/spacer.png

scikit-learn laboratory (skll) is pronounced ""skull"": that's where the learning
happens.

talks
~~~~~

-  *simpler machine learning with skll 1.0*, dan blanchard, pydata nyc 2014 (`video <https://www.youtube.com/watch?v=veo2shbuorc&feature=youtu.be&t=1s>`__ | `slides <http://www.slideshare.net/danielblanchard2/py-data-nyc-2014>`__)
-  *simpler machine learning with skll*, dan blanchard, pydata nyc 2013 (`video <http://vimeo.com/79511496>`__ | `slides <http://www.slideshare.net/danielblanchard2/simple-machine-learning-with-skll>`__)

citing
~~~~~~
if you are using skll in your work, you can cite it as follows: ""we used scikit-learn (pedragosa et al, 2011) via the skll toolkit (https://github.com/educationaltestingservice/skll).""

books
~~~~~

skll is featured in `data science at the command line <http://datascienceatthecommandline.com>`__
by `jeroen janssens <http://jeroenjanssens.com>`__.

changelog
~~~~~~~~~

see `github releases <https://github.com/educationaltestingservice/skll/releases>`__.

contribute
~~~~~~~~~~

thank you for your interest in contributing to skll! see `contributing.md <https://github.com/educationaltestingservice/skll/blob/main/contributing.md>`__ for instructions on how to get started.
"
"TensorFlow","<div align=""center"">
  <img src=""https://www.tensorflow.org/images/tf_logo_horizontal.png"">
</div>

[![python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)
[![pypi](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)
[![doi](https://zenodo.org/badge/doi/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)
[![cii best practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)
[![openssf scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow/badge)](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow)
[![fuzzing status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)
[![fuzzing status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow-py.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow-py)
[![contributor covenant](https://img.shields.io/badge/contributor%20covenant-v1.4%20adopted-ff69b4.svg)](code_of_conduct.md)

**`documentation`** |
------------------- |
[![documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |

[tensorflow](https://www.tensorflow.org/) is an end-to-end open source platform
for machine learning. it has a comprehensive, flexible ecosystem of
[tools](https://www.tensorflow.org/resources/tools),
[libraries](https://www.tensorflow.org/resources/libraries-extensions), and
[community](https://www.tensorflow.org/community) resources that lets
researchers push the state-of-the-art in ml and developers easily build and
deploy ml-powered applications.

tensorflow was originally developed by researchers and engineers working on the
google brain team within google's machine intelligence research organization to
conduct machine learning and deep neural networks research. the system is
general enough to be applicable in a wide variety of other domains, as well.

tensorflow provides stable [python](https://www.tensorflow.org/api_docs/python)
and [c++](https://www.tensorflow.org/api_docs/cc) apis, as well as
non-guaranteed backward compatible api for
[other languages](https://www.tensorflow.org/api_docs).

keep up-to-date with release announcements and security updates by subscribing
to
[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).
see all the [mailing lists](https://www.tensorflow.org/community/forums).

## install

see the [tensorflow install guide](https://www.tensorflow.org/install) for the
[pip package](https://www.tensorflow.org/install/pip), to
[enable gpu support](https://www.tensorflow.org/install/gpu), use a
[docker container](https://www.tensorflow.org/install/docker), and
[build from source](https://www.tensorflow.org/install/source).

to install the current release, which includes support for
[cuda-enabled gpu cards](https://www.tensorflow.org/install/gpu) *(ubuntu and
windows)*:

```
$ pip install tensorflow
```

other devices (directx and macos-metal) are supported using
[device plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).

a smaller cpu-only package is also available:

```
$ pip install tensorflow-cpu
```

to update tensorflow to the latest version, add `--upgrade` flag to the above
commands.

*nightly binaries are available for testing using the
[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and
[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on pypi.*

#### *try your first tensorflow program*

```shell
$ python
```

```python
>>> import tensorflow as tf
>>> tf.add(1, 2).numpy()
3
>>> hello = tf.constant('hello, tensorflow!')
>>> hello.numpy()
b'hello, tensorflow!'
```

for more examples, see the
[tensorflow tutorials](https://www.tensorflow.org/tutorials/).

## contribution guidelines

**if you want to contribute to tensorflow, be sure to review the
[contribution guidelines](contributing.md). this project adheres to tensorflow's
[code of conduct](code_of_conduct.md). by participating, you are expected to
uphold this code.**

**we use [github issues](https://github.com/tensorflow/tensorflow/issues) for
tracking requests and bugs, please see
[tensorflow discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)
for general questions and discussion, and please direct specific questions to
[stack overflow](https://stackoverflow.com/questions/tagged/tensorflow).**

the tensorflow project strives to abide by generally accepted best practices in
open-source software development.

## patching guidelines

follow these steps to patch a specific version of tensorflow, for example, to
apply fixes to bugs or security vulnerabilities:

*   clone the tensorflow repo and switch to the corresponding branch for your
    desired tensorflow version, for example, branch `r2.8` for version 2.8.
*   apply (that is, cherry pick) the desired changes and resolve any code
    conflicts.
*   run tensorflow tests and ensure they pass.
*   [build](https://www.tensorflow.org/install/source) the tensorflow pip
    package from source.

## continuous build status

you can find more community-supported platforms and configurations in the
[tensorflow sig build community builds table](https://github.com/tensorflow/build#community-supported-tensorflow-builds).

### official builds

build type                    | status                                                                                                                                                                           | artifacts
----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------
**linux cpu**                 | [![status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)           | [pypi](https://pypi.org/project/tf-nightly/)
**linux gpu**                 | [![status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [pypi](https://pypi.org/project/tf-nightly-gpu/)
**linux xla**                 | [![status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)         | tba
**macos**                     | [![status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)     | [pypi](https://pypi.org/project/tf-nightly/)
**windows cpu**               | [![status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)       | [pypi](https://pypi.org/project/tf-nightly/)
**windows gpu**               | [![status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)       | [pypi](https://pypi.org/project/tf-nightly-gpu/)
**android**                   | [![status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)               | [download](https://bintray.com/google/tensorflow/tensorflow/_latestversion)
**raspberry pi 0 and 1**      | [![status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html)           | [py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)
**raspberry pi 2 and 3**      | [![status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html)           | [py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)
**libtensorflow macos cpu**   | status temporarily unavailable                                                                                                                                                   | [nightly binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz) [official gcs](https://storage.googleapis.com/tensorflow/)
**libtensorflow linux cpu**   | status temporarily unavailable                                                                                                                                                   | [nightly binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz) [official gcs](https://storage.googleapis.com/tensorflow/)
**libtensorflow linux gpu**   | status temporarily unavailable                                                                                                                                                   | [nightly binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz) [official gcs](https://storage.googleapis.com/tensorflow/)
**libtensorflow windows cpu** | status temporarily unavailable                                                                                                                                                   | [nightly binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz) [official gcs](https://storage.googleapis.com/tensorflow/)
**libtensorflow windows gpu** | status temporarily unavailable                                                                                                                                                   | [nightly binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz) [official gcs](https://storage.googleapis.com/tensorflow/)

## resources

*   [tensorflow.org](https://www.tensorflow.org)
*   [tensorflow tutorials](https://www.tensorflow.org/tutorials/)
*   [tensorflow official models](https://github.com/tensorflow/models/tree/master/official)
*   [tensorflow examples](https://github.com/tensorflow/examples)
*   [tensorflow codelabs](https://codelabs.developers.google.com/?cat=tensorflow)
*   [tensorflow blog](https://blog.tensorflow.org)
*   [learn ml with tensorflow](https://www.tensorflow.org/resources/learn-ml)
*   [tensorflow twitter](https://twitter.com/tensorflow)
*   [tensorflow youtube](https://www.youtube.com/channel/uc0rqucbdtuftjjiefw5t-iq)
*   [tensorflow model optimization roadmap](https://www.tensorflow.org/model_optimization/guide/roadmap)
*   [tensorflow white papers](https://www.tensorflow.org/about/bib)
*   [tensorboard visualization toolkit](https://github.com/tensorflow/tensorboard)
*   [tensorflow code search](https://cs.opensource.google/tensorflow/tensorflow)

learn more about the
[tensorflow community](https://www.tensorflow.org/community) and how to
[contribute](https://www.tensorflow.org/community/contribute).

## courses

*   [deep learning with tensorflow from edx](https://www.edx.org/course/deep-learning-with-tensorflow)
*   [deeplearning.ai tensorflow developer professional certificate from coursera](https://www.coursera.org/specializations/tensorflow-in-practice)
*   [tensorflow: data and deployment from coursera](https://www.coursera.org/specializations/tensorflow-data-and-deployment)
*   [getting started with tensorflow 2 from coursera](https://www.coursera.org/learn/getting-started-with-tensor-flow2)
*   [tensorflow: advanced techniques from coursera](https://www.coursera.org/specializations/tensorflow-advanced-techniques)
*   [tensorflow 2 for deep learning specialization from coursera](https://www.coursera.org/specializations/tensorflow2-deeplearning)
*   [intro to tensorflow for a.i, m.l, and d.l from coursera](https://www.coursera.org/learn/introduction-tensorflow)
*   [machine learning with tensorflow on gcp from coursera](https://www.coursera.org/specializations/machine-learning-tensorflow-gcp)
*   [intro to tensorflow for deep learning from udacity](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)
*   [introduction to tensorflow lite from udacity](https://www.udacity.com/course/intro-to-tensorflow-lite--ud190)

## license

[apache license 2.0](license)
"
"deap","# deap

[![build status](https://travis-ci.org/deap/deap.svg?branch=master)](https://travis-ci.org/deap/deap) [![download](https://img.shields.io/pypi/dm/deap.svg)](https://pypi.python.org/pypi/deap) [![join the chat at https://gitter.im/deap/deap](https://badges.gitter.im/join%20chat.svg)](https://gitter.im/deap/deap?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![build status](https://dev.azure.com/fderainville/deap/_apis/build/status/deap.deap?branchname=master)](https://dev.azure.com/fderainville/deap/_build/latest?definitionid=1&branchname=master) [![documentation status](https://readthedocs.org/projects/deap/badge/?version=master)](https://deap.readthedocs.io/en/master/?badge=master)

deap is a novel evolutionary computation framework for rapid prototyping and testing of 
ideas. it seeks to make algorithms explicit and data structures transparent. it works in perfect harmony with parallelisation mechanisms such as multiprocessing and [scoop](https://github.com/soravux/scoop).

deap includes the following features:

  * genetic algorithm using any imaginable representation
    * list, array, set, dictionary, tree, numpy array, etc.
  * genetic programming using prefix trees
    * loosely typed, strongly typed
    * automatically defined functions
  * evolution strategies (including cma-es)
  * multi-objective optimisation (nsga-ii, nsga-iii, spea2, mo-cma-es)
  * co-evolution (cooperative and competitive) of multiple populations
  * parallelization of the evaluations (and more)
  * hall of fame of the best individuals that lived in the population
  * checkpoints that take snapshots of a system regularly
  * benchmarks module containing most common test functions
  * genealogy of an evolution (that is compatible with [networkx](https://github.com/networkx/networkx))
  * examples of alternative algorithms : particle swarm optimization, differential evolution, estimation of distribution algorithm

## downloads

following acceptance of [pep 438](http://www.python.org/dev/peps/pep-0438/) by the python community, we have moved deap's source releases on [pypi](https://pypi.python.org).

you can find the most recent releases at: https://pypi.python.org/pypi/deap/.

## documentation
see the [deap user's guide](http://deap.readthedocs.org/) for deap documentation.

in order to get the tip documentation, change directory to the `doc` subfolder and type in `make html`, the documentation will be under `_build/html`. you will need [sphinx](http://sphinx.pocoo.org) to build the documentation.

### notebooks
also checkout our new [notebook examples](https://github.com/deap/notebooks). using [jupyter notebooks](http://jupyter.org)  you'll be able to navigate and execute each block of code individually and tell what every line is doing. either, look at the notebooks online using the notebook viewer links at the botom of the page or download the notebooks, navigate to the you download directory and run

```bash
jupyter notebook
```

## installation
we encourage you to use easy_install or pip to install deap on your system. other installation procedure like apt-get, yum, etc. usually provide an outdated version.
```bash
pip install deap
```

the latest version can be installed with 
```bash
pip install git+https://github.com/deap/deap@master
```

if you wish to build from sources, download or clone the repository and type

```bash
python setup.py install
```

## build status
deap build status is available on travis-ci https://travis-ci.org/deap/deap.

## requirements
the most basic features of deap requires python2.6. in order to combine the toolbox and the multiprocessing module python2.7 is needed for its support to pickle partial functions. cma-es requires numpy, and we recommend matplotlib for visualization of results as it is fully compatible with deap's api.

since version 0.8, deap is compatible out of the box with python 3.

## example

the following code gives a quick overview how simple it is to implement the onemax problem optimization with genetic algorithm using deap.  more examples are provided [here](http://deap.readthedocs.org/en/master/examples/index.html).

```python
import random
from deap import creator, base, tools, algorithms

creator.create(""fitnessmax"", base.fitness, weights=(1.0,))
creator.create(""individual"", list, fitness=creator.fitnessmax)

toolbox = base.toolbox()

toolbox.register(""attr_bool"", random.randint, 0, 1)
toolbox.register(""individual"", tools.initrepeat, creator.individual, toolbox.attr_bool, n=100)
toolbox.register(""population"", tools.initrepeat, list, toolbox.individual)

def evalonemax(individual):
    return sum(individual),

toolbox.register(""evaluate"", evalonemax)
toolbox.register(""mate"", tools.cxtwopoint)
toolbox.register(""mutate"", tools.mutflipbit, indpb=0.05)
toolbox.register(""select"", tools.seltournament, tournsize=3)

population = toolbox.population(n=300)

ngen=40
for gen in range(ngen):
    offspring = algorithms.varand(population, toolbox, cxpb=0.5, mutpb=0.1)
    fits = toolbox.map(toolbox.evaluate, offspring)
    for fit, ind in zip(fits, offspring):
        ind.fitness.values = fit
    population = toolbox.select(offspring, k=len(population))
top10 = tools.selbest(population, k=10)
```

## how to cite deap
authors of scientific papers including results generated using deap are encouraged to cite the following paper.

```xml
@article{deap_jmlr2012, 
    author    = "" f\'elix-antoine fortin and fran\c{c}ois-michel {de rainville} and marc-andr\'e gardner and marc parizeau and christian gagn\'e "",
    title     = { {deap}: evolutionary algorithms made easy },
    pages    = { 2171--2175 },
    volume    = { 13 },
    month     = { jul },
    year      = { 2012 },
    journal   = { journal of machine learning research }
}
```

## publications on deap

  * fran√ßois-michel de rainville, f√©lix-antoine fortin, marc-andr√© gardner, marc parizeau and christian gagn√©, ""deap -- enabling nimbler evolutions"", sigevolution, vol. 6, no 2, pp. 17-26, february 2014. [paper](http://goo.gl/torxtp)
  * f√©lix-antoine fortin, fran√ßois-michel de rainville, marc-andr√© gardner, marc parizeau and christian gagn√©, ""deap: evolutionary algorithms made easy"", journal of machine learning research, vol. 13, pp. 2171-2175, jul 2012. [paper](http://goo.gl/amj3x)
  * fran√ßois-michel de rainville, f√©lix-antoine fortin, marc-andr√© gardner, marc parizeau and christian gagn√©, ""deap: a python framework for evolutionary algorithms"", in !evosoft workshop, companion proc. of the genetic and evolutionary computation conference (gecco 2012), july 07-11 2012. [paper](http://goo.gl/pxxug)

## projects using deap
  * ribaric, t., & houghten, s. (2017, june). genetic programming for improved cryptanalysis of elliptic curve cryptosystems. in 2017 ieee congress on evolutionary computation (cec) (pp. 419-426). ieee.
  * ellefsen, kai olav, herman augusto lepikson, and jan c. albiez. ""multiobjective coverage path planning: enabling automated inspection of complex, real-world structures."" applied soft computing 61 (2017): 264-282.
  * s. chardon, b. brangeon, e. bozonnet, c. inard (2016), construction cost and energy performance of single family houses : from integrated design to automated optimization, automation in construction, volume 70, p.1-13.
  * b. brangeon, e. bozonnet, c. inard (2016), integrated refurbishment of collective housing and optimization process with real products databases, building simulation optimization, pp. 531‚Äì538 newcastle, england.
  * randal s. olson, ryan j. urbanowicz, peter c. andrews, nicole a. lavender, la creis kidd, and jason h. moore (2016). automating biomedical data science through tree-based pipeline optimization. applications of evolutionary computation, pages 123-137.
  * randal s. olson, nathan bartley, ryan j. urbanowicz, and jason h. moore (2016). evaluation of a tree-based pipeline optimization tool for automating data science. proceedings of gecco 2016, pages 485-492.
  * van geit w, gevaert m, chindemi g, r√∂ssert c, courcol j, muller eb, sch√ºrmann f, segev i and markram h (2016). bluepyopt: leveraging open source software and cloud infrastructure to optimise model parameters in neuroscience. front. neuroinform. 10:17. doi: 10.3389/fninf.2016.00017 https://github.com/bluebrain/bluepyopt
  * lara-cabrera, r., cotta, c. and fern√°ndez-leiva, a.j. (2014). geometrical vs topological measures for the evolution of aesthetic maps in a rts game, entertainment computing,
  * macret, m. and pasquier, p. (2013). automatic tuning of the op-1 synthesizer using a multi-objective genetic algorithm. in proceedings of the 10th sound and music computing conference (smc). (pp 614-621).
  * fortin, f. a., grenier, s., & parizeau, m. (2013, july). generalizing the improved run-time complexity algorithm for non-dominated sorting. in proceeding of the fifteenth annual conference on genetic and evolutionary computation conference (pp. 615-622). acm.
  * fortin, f. a., & parizeau, m. (2013, july). revisiting the nsga-ii crowding-distance computation. in proceeding of the fifteenth annual conference on genetic and evolutionary computation conference (pp. 623-630). acm.
  * marc-andr√© gardner, christian gagn√©, and marc parizeau. estimation of distribution algorithm based on hidden markov models for combinatorial optimization. in comp. proc. genetic and evolutionary computation conference (gecco 2013), july 2013.
  * j. t. zhai, m. a. bamakhrama, and t. stefanov. ""exploiting just-enough parallelism when mapping streaming applications in hard real-time systems"". design automation conference (dac 2013), 2013.
  * v. akbarzadeh, c. gagn√©, m. parizeau, m. argany, m. a mostafavi, ""probabilistic sensing model for sensor placement optimization based on line-of-sight coverage"", accepted in ieee transactions on instrumentation and measurement, 2012.
  * m. reif, f. shafait, and a. dengel. ""dataset generation for meta-learning"". proceedings of the german conference on artificial intelligence (ki'12). 2012. 
  * m. t. ribeiro, a. lacerda, a. veloso, and n. ziviani. ""pareto-efficient hybridization for multi-objective recommender systems"". proceedings of the conference on recommanders systems (!recsys'12). 2012.
  * m. p√©rez-ortiz, a. arauzo-azofra, c. herv√°s-mart√≠nez, l. garc√≠a-hern√°ndez and l. salas-morera. ""a system learning user preferences for multiobjective optimization of facility layouts"". pr,oceedings on the int. conference on soft computing models in industrial and environmental applications (soco'12). 2012.
  * l√©vesque, j.c., durand, a., gagn√©, c., and sabourin, r., multi-objective evolutionary optimization for generating ensembles of classifiers in the roc space, genetic and evolutionary computation conference (gecco 2012), 2012.
  * marc-andr√© gardner, christian gagn√©, and marc parizeau, ""bloat control in genetic programming with histogram-based accept-reject method"", in proc. genetic and evolutionary computation conference (gecco 2011), 2011.
  * vahab akbarzadeh, albert ko, christian gagn√©, and marc parizeau, ""topography-aware sensor deployment optimization with cma-es"", in proc. of parallel problem solving from nature (ppsn 2010), springer, 2010.
  * deap is used in [tpot](https://github.com/rhiever/tpot), an open source tool that uses genetic programming to optimize machine learning pipelines.
  * deap is also used in ros as an optimization package http://www.ros.org/wiki/deap.
  * deap is an optional dependency for [pyxrd](https://github.com/mathijs-dumon/pyxrd), a python implementation of the matrix algorithm developed for the x-ray diffraction analysis of disordered lamellar structures.
  * deap is used in [glyph](https://github.com/ambrosys/glyph), a library for symbolic regression with applications to [mlc](https://en.wikipedia.org/wiki/machine_learning_control).
  * deap is used in [sklearn-genetic-opt](https://github.com/rodrigo-arenas/sklearn-genetic-opt), an open source tool that uses evolutionary programming to fine tune machine learning hyperparameters.

if you want your project listed here, send us a link and a brief description and we'll be glad to add it.
"
"mlxtend","[![doi](https://joss.theoj.org/papers/10.21105/joss.00638/status.svg)](https://doi.org/10.21105/joss.00638)
[![pypi version](https://badge.fury.io/py/mlxtend.svg)](http://badge.fury.io/py/mlxtend)
[![anaconda-server badge](https://anaconda.org/conda-forge/mlxtend/badges/version.svg)](https://anaconda.org/conda-forge/mlxtend)
[![build status](https://ci.appveyor.com/api/projects/status/7vx20e0h5dxcyla2/branch/master?svg=true)](https://ci.appveyor.com/project/rasbt/mlxtend/branch/master)
[![codecov](https://codecov.io/gh/rasbt/mlxtend/branch/master/graph/badge.svg)](https://codecov.io/gh/rasbt/mlxtend)
![python 3](https://img.shields.io/badge/python-3-blue.svg)
![license](https://img.shields.io/badge/license-bsd-blue.svg)
[![discuss](https://img.shields.io/badge/discuss-github-blue.svg)](https://github.com/rasbt/mlxtend/discussions)

![](./docs/sources/img/logo.png)

**mlxtend (machine learning extensions) is a python library of useful tools for the day-to-day data science tasks.**

<br>

sebastian raschka 2014-2022

<br>

## links

- **documentation:** [http://rasbt.github.io/mlxtend](http://rasbt.github.io/mlxtend)
- pypi: [https://pypi.python.org/pypi/mlxtend](https://pypi.python.org/pypi/mlxtend)
- changelog: [http://rasbt.github.io/mlxtend/changelog](http://rasbt.github.io/mlxtend/changelog)
- contributing: [http://rasbt.github.io/mlxtend/contributing](http://rasbt.github.io/mlxtend/contributing)
- questions? check out the [github discussions board](https://github.com/rasbt/mlxtend/discussions)

<br>
<br>

## installing mlxtend

#### pypi

to install mlxtend, just execute  

```bash
pip install mlxtend  
```

alternatively, you could download the package manually from the python package index [https://pypi.python.org/pypi/mlxtend](https://pypi.python.org/pypi/mlxtend), unzip it, navigate into the package, and use the command:

```bash
python setup.py install
```

#### conda
if you use conda, to install mlxtend just execute

```bash
conda install -c conda-forge mlxtend 
```

#### dev version

the mlxtend version on pypi may always be one step behind; you can install the latest development version from the github repository by executing

```bash
pip install git+git://github.com/rasbt/mlxtend.git#egg=mlxtend
```

or, you can fork the github repository from https://github.com/rasbt/mlxtend and install mlxtend from your local drive via

```bash
python setup.py install
```

<br>
<br>

## examples

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import itertools
from sklearn.linear_model import logisticregression
from sklearn.svm import svc
from sklearn.ensemble import randomforestclassifier
from mlxtend.classifier import ensemblevoteclassifier
from mlxtend.data import iris_data
from mlxtend.plotting import plot_decision_regions

# initializing classifiers
clf1 = logisticregression(random_state=0)
clf2 = randomforestclassifier(random_state=0)
clf3 = svc(random_state=0, probability=true)
eclf = ensemblevoteclassifier(clfs=[clf1, clf2, clf3], weights=[2, 1, 1], voting='soft')

# loading some example data
x, y = iris_data()
x = x[:,[0, 2]]

# plotting decision regions
gs = gridspec.gridspec(2, 2)
fig = plt.figure(figsize=(10, 8))

for clf, lab, grd in zip([clf1, clf2, clf3, eclf],
                         ['logistic regression', 'random forest', 'rbf kernel svm', 'ensemble'],
                         itertools.product([0, 1], repeat=2)):
    clf.fit(x, y)
    ax = plt.subplot(gs[grd[0], grd[1]])
    fig = plot_decision_regions(x=x, y=y, clf=clf, legend=2)
    plt.title(lab)
plt.show()
```

![](./docs/sources/img/ensemble_decision_regions_2d.png)

---

if you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following doi:


```
@article{raschkas_2018_mlxtend,
  author       = {sebastian raschka},
  title        = {mlxtend: providing machine learning and data science 
                  utilities and extensions to python‚Äôs  
                  scientific computing stack},
  journal      = {the journal of open source software},
  volume       = {3},
  number       = {24},
  month        = apr,
  year         = 2018,
  publisher    = {the open journal},
  doi          = {10.21105/joss.00638},
  url          = {http://joss.theoj.org/papers/10.21105/joss.00638}
}
```

- raschka, sebastian (2018) mlxtend: providing machine learning and data science utilities and extensions to python's scientific computing stack.
j open source softw 3(24).

---

## license

- this project is released under a permissive new bsd open source license ([license-bsd3.txt](https://github.com/rasbt/mlxtend/blob/master/license-bsd3.txt)) and commercially usable. there is no warranty; not even for merchantability or fitness for a particular purpose.
- in addition, you may use, copy, modify and redistribute all artistic creative works (figures and images) included in this distribution under the directory
according to the terms and conditions of the creative commons attribution 4.0 international license.  see the file [license-cc-by.txt](https://github.com/rasbt/mlxtend/blob/master/license-cc-by.txt) for details. (computer-generated graphics such as the plots produced by matplotlib fall under the bsd license mentioned above).

## contact

the best way to ask questions is via the [github discussions channel](https://github.com/rasbt/mlxtend/discussions). in case you encounter usage bugs, please don't hesitate to use the [github's issue tracker](https://github.com/rasbt/mlxtend/issues) directly. 
"
"neon","**discontinuation of project.**  this project will no longer be maintained by intel.  intel will not provide or guarantee development of or support for this project, including but not limited to, maintenance, bug fixes, new releases or updates.  patches to this project are no longer accepted by intel. if you have an ongoing need to use this project, are interested in independently developing it, or would like to maintain patches for the community, please create your own fork of the project.

# neon

[neon](https://github.com/nervanasystems/neon) is intel's reference deep learning framework committed to [best performance](https://github.com/soumith/convnet-benchmarks) on all hardware. designed for ease-of-use and extensibility.

* [tutorials](http://neon.nervanasys.com/docs/latest/tutorials.html) and [ipython notebooks](https://github.com/nervanasystems/meetup) to get users started with using neon for deep learning.
* support for commonly used layers: convolution, rnn, lstm, gru, batchnorm, and more.
* [model zoo](https://github.com/nervanasystems/modelzoo) contains pre-trained weights and example scripts for start-of-the-art models, including: [vgg](https://github.com/nervanasystems/modelzoo/tree/master/imageclassification/ilsvrc2012/vgg), [reinforcement learning](https://github.com/nervanasystems/modelzoo/tree/master/deepreinforcement), [deep residual networks](https://github.com/nervanasystems/modelzoo/tree/master/sceneclassification/deepresnet), [image captioning](https://github.com/nervanasystems/modelzoo/tree/master/imagecaptioning), [sentiment analysis](https://github.com/nervanasystems/modelzoo/tree/master/nlp/sentimentclassification/imdb), and [more](http://neon.nervanasys.com/docs/latest/model_zoo.html).
* swappable hardware backends: write code once and then deploy on cpus, gpus, or nervana hardware

for fast iteration and model exploration, neon has the fastest performance among deep learning libraries (2x speed of cudnnv4, see [benchmarks](https://github.com/soumith/convnet-benchmarks)).
* 2.5s/macrobatch (3072 images) on alexnet on titan x (full run on 1 gpu ~ 26 hrs)
* training vgg with 16-bit floating point on 1 titan x takes ~10 days (original paper: 4 gpus for 2-3 weeks)

we use neon internally at intel nervana to solve our customers' problems across many
[domains](http://www.nervanasys.com/solutions/). we are hiring across several
roles. apply [here](http://www.nervanasys.com/careers/)!

see the [new features](https://github.com/nervanasystems/neon/blob/master/changelog) in our latest release.
we want to highlight that neon v2.0.0+ has been optimized for much better performance on cpus by enabling intel math kernel library (mkl). the dnn (deep neural networks) component of mkl that is used by neon is provided free of charge and downloaded automatically as part of the neon installation.

## quick install

* [local install and dependencies](http://neon.nervanasys.com/docs/latest/installation.html)

on a mac osx or linux machine, enter the following to download and install
neon (conda users see the [guide](http://neon.nervanasys.com/docs/latest/installation.html)), and use it to train your first multi-layer perceptron. to force a python2 or python3 install, replace `make` below with either `make python2` or `make python3`.

```bash
    git clone https://github.com/nervanasystems/neon.git
    cd neon
    make
    . .venv/bin/activate
```

starting after neon v2.2.0, the master branch of neon will be updated weekly with work-in-progress toward the next release. check out a release tag (e.g., ""git checkout v2.2.0"") for a stable release. or simply check out the ""latest"" release tag to get the latest stable release (i.e., ""git checkout latest"")

* [install via pypi](https://pypi.python.org/pypi/nervananeon)

from version 2.4.0, we re-enabled pip install. neon can be installed using package name nervananeon. 

```bash
    pip install nervananeon
```

it is noted that [aeon](https://aeon.nervanasys.com/index.html/getting_started.html) needs to be installed separately. the latest release v2.6.0 uses aeon v1.3.0.

**warning**

> between neon v2.1.0 and v2.2.0, the aeon manifest file format has been changed. when updating from neon < v2.2.0 manifests have to be recreated using ingest scripts (in examples folder) or updated using [this](neon/data/convert_manifest.py) script.

### use a script to run an example

```bash
    python examples/mnist_mlp.py 
```

#### selecting a backend engine from the command line

the gpu backend is selected by default, so the above command is equivalent to if a compatible gpu resource is found on the system:

```bash
    python examples/mnist_mlp.py -b gpu
```

when no gpu is available, the **optimized** cpu (mkl) backend is now selected by default as of neon v2.1.0, which means the above command is now equivalent to:

```bash
    python examples/mnist_mlp.py -b mkl
```

if you are interested in comparing the default mkl backend with the non-optimized cpu backend, use the following command:

```bash
    python examples/mnist_mlp.py -b cpu
```

### use a yaml file to run an example

alternatively, a yaml file may be used run an example.

```bash
    neon examples/mnist_mlp.yaml
```

to select a specific backend in a yaml file, add or modify a line that contains ``backend: mkl`` to enable mkl backend, or ``backend: cpu`` to enable cpu backend.  the gpu backend is selected by default if a gpu is available.

## recommended settings for neon with mkl on intel architectures

the intel math kernel library takes advantages of the parallelization and vectorization capabilities of intel xeon and xeon phi systems. when hyperthreading is enabled on the system, we recommend 
the following kmp_affinity setting to make sure parallel threads are 1:1 mapped to the available physical cores. 

```bash
    export omp_num_threads=<number of physical cores>
    export kmp_affinity=compact,1,0,granularity=fine  
```
or 
```bash
    export omp_num_threads=<number of physical cores>
    export kmp_affinity=verbose,granularity=fine,proclist=[0-<number of physical cores>],explicit
```
for more information about kmp_affinity, please check [here](https://software.intel.com/en-us/node/522691).
we encourage users to set out trying and establishing their own best performance settings. 


## documentation

the complete documentation for neon is available
[here](http://neon.nervanasys.com/docs/latest). some useful starting points are:

* [tutorials](http://neon.nervanasys.com/docs/latest/tutorials.html) for neon
* [overview](http://neon.nervanasys.com/docs/latest/overview.html) of the neon workflow
* [api](http://neon.nervanasys.com/docs/latest/api.html) documentation
* [resources](http://neon.nervanasys.com/docs/latest/resources.html) for neon and deep learning


## support

for any bugs or feature requests please:

1. search the open and closed
   [issues list](https://github.com/nervanasystems/neon/issues) to see if we're
   already working on what you have uncovered.
2. check that your issue/request hasn't already been addressed in our
   [frequently asked questions (faq)](http://neon.nervanasys.com/docs/latest/faq.html)
   or [neon-users](https://groups.google.com/forum/#!forum/neon-users) google
   group.
3. file a new [issue](https://github.com/nervanasystems/neon/issues) or submit
   a new [pull request](https://github.com/nervanasystems/neon/pulls) if you
   have some code you'd like to contribute

for other questions and discussions please post a message to the
   [neon-users](https://groups.google.com/forum/?hl=en#!forum/neon-users)
   google group

## license

we are releasing [neon](https://github.com/nervanasystems/neon) under an open source
[apache 2.0](https://www.apache.org/licenses/license-2.0) license. we welcome you to [contact us](mailto:info@nervanasys.com) with your use cases.
"
"TPOT","master status: [![master build status - mac/linux](https://travis-ci.com/epistasislab/tpot.svg?branch=master)](https://travis-ci.com/epistasislab/tpot)
[![master build status - windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/master?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=master)
[![master coverage status](https://coveralls.io/repos/github/epistasislab/tpot/badge.svg?branch=master)](https://coveralls.io/github/epistasislab/tpot?branch=master)

development status: [![development build status - mac/linux](https://travis-ci.com/epistasislab/tpot.svg?branch=development)](https://travis-ci.com/epistasislab/tpot/branches)
[![development build status - windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/development?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=development)
[![development coverage status](https://coveralls.io/repos/github/epistasislab/tpot/badge.svg?branch=development)](https://coveralls.io/github/epistasislab/tpot?branch=development)

package information: [![python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/)
[![license: lgpl v3](https://img.shields.io/badge/license-lgpl%20v3-blue.svg)](http://www.gnu.org/licenses/lgpl-3.0)
[![pypi version](https://badge.fury.io/py/tpot.svg)](https://badge.fury.io/py/tpot)

<p align=""center"">
<img src=""https://raw.githubusercontent.com/epistasislab/tpot/master/images/tpot-logo.jpg"" width=300 />
</p>

**tpot** stands for **t**ree-based **p**ipeline **o**ptimization **t**ool. consider tpot your **data science assistant**. tpot is a python automated machine learning tool that optimizes machine learning pipelines using genetic programming.

![tpot demo](https://github.com/epistasislab/tpot/blob/master/images/tpot-demo.gif ""tpot demo"")

tpot will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.

![an example machine learning pipeline](https://github.com/epistasislab/tpot/blob/master/images/tpot-ml-pipeline.png ""an example machine learning pipeline"")

<p align=""center""><strong>an example machine learning pipeline</strong></p>

once tpot is finished searching (or you get tired of waiting), it provides you with the python code for the best pipeline it found so you can tinker with the pipeline from there.

![an example tpot pipeline](https://github.com/epistasislab/tpot/blob/master/images/tpot-pipeline-example.png ""an example tpot pipeline"")

tpot is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.

**tpot is still under active development** and we encourage you to check back on this repository regularly for updates.

for further information about tpot, please see the [project documentation](http://epistasislab.github.io/tpot/).

## license

please see the [repository license](https://github.com/epistasislab/tpot/blob/master/license) for the licensing and usage information for tpot.

generally, we have licensed tpot to make it as widely usable as possible.

## installation

we maintain the [tpot installation instructions](http://epistasislab.github.io/tpot/installing/) in the documentation. tpot requires a working installation of python.

## usage

tpot can be used [on the command line](http://epistasislab.github.io/tpot/using/#tpot-on-the-command-line) or [with python code](http://epistasislab.github.io/tpot/using/#tpot-with-code).

click on the corresponding links to find more information on tpot usage in the documentation.

## examples

### classification

below is a minimal working example with the optical recognition of handwritten digits dataset.

```python
from tpot import tpotclassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25, random_state=42)

tpot = tpotclassifier(generations=5, population_size=50, verbosity=2, random_state=42)
tpot.fit(x_train, y_train)
print(tpot.score(x_test, y_test))
tpot.export('tpot_digits_pipeline.py')
```

running this code should discover a pipeline that achieves about 98% testing accuracy, and the corresponding python code should be exported to the `tpot_digits_pipeline.py` file and look similar to the following:

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import randomforestclassifier
from sklearn.linear_model import logisticregression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import polynomialfeatures
from tpot.builtins import stackingestimator
from tpot.export_utils import set_param_recursive

# note: make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('path/to/data/file', sep='column_separator', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=42)

# average cv score on the training set was: 0.9799428471757372
exported_pipeline = make_pipeline(
    polynomialfeatures(degree=2, include_bias=false, interaction_only=false),
    stackingestimator(estimator=logisticregression(c=0.1, dual=false, penalty=""l1"")),
    randomforestclassifier(bootstrap=true, criterion=""entropy"", max_features=0.35000000000000003, min_samples_leaf=20, min_samples_split=19, n_estimators=100)
)
# fix random state for all the steps in exported pipeline
set_param_recursive(exported_pipeline.steps, 'random_state', 42)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
```

### regression

similarly, tpot can optimize pipelines for regression problems. below is a minimal working example with the practice boston housing prices data set.

```python
from tpot import tpotregressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

housing = load_boston()
x_train, x_test, y_train, y_test = train_test_split(housing.data, housing.target,
                                                    train_size=0.75, test_size=0.25, random_state=42)

tpot = tpotregressor(generations=5, population_size=50, verbosity=2, random_state=42)
tpot.fit(x_train, y_train)
print(tpot.score(x_test, y_test))
tpot.export('tpot_boston_pipeline.py')
```

which should result in a pipeline that achieves about 12.77 mean squared error (mse), and the python code in `tpot_boston_pipeline.py` should look similar to:

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import extratreesregressor
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import polynomialfeatures
from tpot.export_utils import set_param_recursive

# note: make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('path/to/data/file', sep='column_separator', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=42)

# average cv score on the training set was: -10.812040755234403
exported_pipeline = make_pipeline(
    polynomialfeatures(degree=2, include_bias=false, interaction_only=false),
    extratreesregressor(bootstrap=false, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=100)
)
# fix random state for all the steps in exported pipeline
set_param_recursive(exported_pipeline.steps, 'random_state', 42)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
```

check the documentation for [more examples and tutorials](http://epistasislab.github.io/tpot/examples/).

## contributing to tpot

we welcome you to [check the existing issues](https://github.com/epistasislab/tpot/issues/) for bugs or enhancements to work on. if you have an idea for an extension to tpot, please [file a new issue](https://github.com/epistasislab/tpot/issues/new) so we can discuss it.

before submitting any contributions, please review our [contribution guidelines](http://epistasislab.github.io/tpot/contributing/).

## having problems or have questions about tpot?

please [check the existing open and closed issues](https://github.com/epistasislab/tpot/issues?utf8=%e2%9c%93&q=is%3aissue) to see if your issue has already been attended to. if it hasn't, [file a new issue](https://github.com/epistasislab/tpot/issues/new) on this repository so we can review your issue.

## citing tpot

if you use tpot in a scientific publication, please consider citing at least one of the following papers:

trang t. le, weixuan fu and jason h. moore (2020). [scaling tree-based automated machine learning to biomedical big data with a feature set selector](https://academic.oup.com/bioinformatics/article/36/1/250/5511404). *bioinformatics*.36(1): 250-256.

bibtex entry:

```bibtex
@article{le2020scaling,
  title={scaling tree-based automated machine learning to biomedical big data with a feature set selector},
  author={le, trang t and fu, weixuan and moore, jason h},
  journal={bioinformatics},
  volume={36},
  number={1},
  pages={250--256},
  year={2020},
  publisher={oxford university press}
}
```


randal s. olson, ryan j. urbanowicz, peter c. andrews, nicole a. lavender, la creis kidd, and jason h. moore (2016). [automating biomedical data science through tree-based pipeline optimization](http://link.springer.com/chapter/10.1007/978-3-319-31204-0_9). *applications of evolutionary computation*, pages 123-137.

bibtex entry:

```bibtex
@inbook{olson2016evobio,
    author={olson, randal s. and urbanowicz, ryan j. and andrews, peter c. and lavender, nicole a. and kidd, la creis and moore, jason h.},
    editor={squillero, giovanni and burelli, paolo},
    chapter={automating biomedical data science through tree-based pipeline optimization},
    title={applications of evolutionary computation: 19th european conference, evoapplications 2016, porto, portugal, march 30 -- april 1, 2016, proceedings, part i},
    year={2016},
    publisher={springer international publishing},
    pages={123--137},
    isbn={978-3-319-31204-0},
    doi={10.1007/978-3-319-31204-0_9},
    url={http://dx.doi.org/10.1007/978-3-319-31204-0_9}
}
```

randal s. olson, nathan bartley, ryan j. urbanowicz, and jason h. moore (2016). [evaluation of a tree-based pipeline optimization tool for automating data science](http://dl.acm.org/citation.cfm?id=2908918). *proceedings of gecco 2016*, pages 485-492.

bibtex entry:

```bibtex
@inproceedings{olsongecco2016,
    author = {olson, randal s. and bartley, nathan and urbanowicz, ryan j. and moore, jason h.},
    title = {evaluation of a tree-based pipeline optimization tool for automating data science},
    booktitle = {proceedings of the genetic and evolutionary computation conference 2016},
    series = {gecco '16},
    year = {2016},
    isbn = {978-1-4503-4206-3},
    location = {denver, colorado, usa},
    pages = {485--492},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/2908812.2908918},
    doi = {10.1145/2908812.2908918},
    acmid = {2908918},
    publisher = {acm},
    address = {new york, ny, usa},
}
```

alternatively, you can cite the repository directly with the following doi:

[![doi](https://zenodo.org/badge/20747/rhiever/tpot.svg)](https://zenodo.org/badge/latestdoi/20747/rhiever/tpot)

## support for tpot

tpot was developed in the [computational genetics lab](http://epistasis.org/) at the [university of pennsylvania](https://www.upenn.edu/) with funding from the [nih](http://www.nih.gov/) under grant r01 ai117694. we are incredibly grateful for the support of the nih and the university of pennsylvania during the development of this project.

the tpot logo was designed by todd newmuis, who generously donated his time to the project.
"
"Orange","<p align=""center"">
    <a href=""https://orange.biolab.si/download"">
    <img src=""https://raw.githubusercontent.com/irgolic/orange3/readme-shields/distribute/orange-title.png"" alt=""orange data mining"" height=""200"">
    </a>
</p>
<p align=""center"">
    <a href=""https://orange.biolab.si/download"" alt=""latest release"">
        <img src=""https://img.shields.io/github/v/release/biolab/orange3?label=download"" />
    </a>
    <a href=""https://orange3.readthedocs.io/en/latest/?badge=latest"" alt=""documentation"">
        <img src=""https://readthedocs.org/projects/orange3/badge/?version=latest"">
    </a>
    <a href=""https://discord.gg/fwrfexv"" alt=""discord"">
        <img src=""https://img.shields.io/discord/633376992607076354?logo=discord&color=7389d8&logocolor=white&label=discord"">                                                                                                                                                                                                                                                  </a>
</p>

# orange data mining
[orange] is a data mining and visualization toolbox for novice and expert alike. to explore data with orange, one requires __no programming or in-depth mathematical knowledge__. we believe that workflow-based data science tools democratize data science by hiding complex underlying mechanics and exposing intuitive concepts. anyone who owns data, or is motivated to peek into data, should have the means to do so.

<p align=""center"">
    <a href=""https://orange.biolab.si/download"">
    <img src=""https://raw.githubusercontent.com/irgolic/orange3/readme-shields/distribute/orange-example-tall.png"" alt=""example workflow"">
    </a>
</p>

[orange]: https://orange.biolab.si/


## installing

### easy installation

for easy installation, [download](https://orange.biolab.si/download) the latest released orange version from our website. to install an add-on, head to `options -> add-ons...` in the menu bar.

### installing with conda

first, install [miniconda](https://docs.conda.io/en/latest/miniconda.html) for your os. 

then, create a new conda environment, and install orange3:

```shell
# add conda-forge to your channels for access to the latest release
conda config --add channels conda-forge

# perhaps enforce strict conda-forge priority
conda config --set channel_priority strict

# create and activate an environment for orange
conda create python=3 --yes --name orange3
conda activate orange3

# install pyqt and pyqtwebengine
conda install pyqt
# install orange
conda install orange3
```

for installation of an add-on, use:
```shell
conda install orange3-<addon name>
```
[see specific add-on repositories for details.](https://github.com/biolab/)


### installing with pip

we recommend using our [standalone installer](https://orange.biolab.si/download) or conda, but orange is also installable with pip. you will need a c/c++ compiler (on windows we suggest using microsoft visual studio build tools).
orange needs pyqt to run. install either:
- pyqt5 and pyqtwebengine: `pip install -r requirements-pyqt.txt` 
- pyqt6 and pyqt6-webengine: `pip install pyqt6 pyqt6-webengine`

### installing with winget (windows only)

to install orange with [winget](https://docs.microsoft.com/en-us/windows/package-manager/winget/), run:

```shell
winget install --id  universityofljubljana.orange 
```

## running

ensure you've activated the correct virtual environment. if following the above conda instructions:

```shell
conda activate orange3
``` 

run `orange-canvas` or `python3 -m orange.canvas`. add `--help` for a list of program options.

starting up for the first time may take a while.


## developing

[![github actions](https://img.shields.io/endpoint.svg?url=https%3a%2f%2factions-badge.atrox.dev%2fbiolab%2forange3%2fbadge&label=build)](https://actions-badge.atrox.dev/biolab/orange3/goto) [![codecov](https://img.shields.io/codecov/c/github/biolab/orange3)](https://codecov.io/gh/biolab/orange3) [![contributor count](https://img.shields.io/github/contributors-anon/biolab/orange3)](https://github.com/biolab/orange3/graphs/contributors) [![latest github commit](https://img.shields.io/github/last-commit/biolab/orange3)](https://github.com/biolab/orange3/commits/master)

want to write a widget? [use the orange3 example add-on template.](https://github.com/biolab/orange3-example-addon)

want to get involved? join us on [discord](https://discord.gg/fwrfexv), introduce yourself in #general! 

take a look at our [contributing guide](https://github.com/irgolic/orange3/blob/readme-shields/contributing.md) and [style guidelines](https://github.com/biolab/orange-widget-base/wiki/widget-ui).

check out our widget development [docs](https://orange-widget-base.readthedocs.io/en/latest/?badge=latest) for a comprehensive guide on writing orange widgets.

### the orange ecosystem

the development of core orange is primarily split into three repositories:

[biolab/orange-canvas-core](https://www.github.com/biolab/orange-canvas-core) implements the canvas,  
[biolab/orange-widget-base](https://www.github.com/biolab/orange-widget-base) is a handy widget gui library,  
[biolab/orange3](https://www.github.com/biolab/orange3) brings it all together and implements the base data mining toolbox.	

additionally, add-ons implement additional widgets for more specific use cases. [anyone can write an add-on.](https://github.com/biolab/orange3-example-addon) some of our first-party add-ons:

- [biolab/orange3-text](https://www.github.com/biolab/orange3-text)
- [biolab/orange3-bioinformatics](https://www.github.com/biolab/orange3-bioinformatics)
- [biolab/orange3-timeseries](https://www.github.com/biolab/orange3-timeseries)    
- [biolab/orange3-single-cell](https://www.github.com/biolab/orange3-single-cell)    
- [biolab/orange3-imageanalytics](https://www.github.com/biolab/orange3-imageanalytics)    
- [biolab/orange3-educational](https://www.github.com/biolab/orange3-educational)    
- [biolab/orange3-geo](https://www.github.com/biolab/orange3-geo)    
- [biolab/orange3-associate](https://www.github.com/biolab/orange3-associate)    
- [biolab/orange3-network](https://www.github.com/biolab/orange3-network)
- [biolab/orange3-explain](https://www.github.com/biolab/orange3-explain)

### setting up for core orange development

first, fork the repository by pressing the fork button in the top-right corner of this page.

set your github username,

```shell
export my_github_username=replaceme
```

create a conda environment, clone your fork, and install it:

```shell
conda create python=3 --yes --name orange3
conda activate orange3

git clone ssh://git@github.com/$my_github_username/orange3

# install pyqt and pyqtwebengine. you can also use pyqt6
pip install -r requirements-pyqt.txt
pip install -e orange3
```

now you're ready to work with git. see github's guides on [pull requests](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/proposing-changes-to-your-work-with-pull-requests), [forks](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/working-with-forks) if you're unfamiliar. if you're having trouble, get in touch on [discord](https://discord.gg/fwrfexv).

#### running

run orange with `python -m orange.canvas` (after activating the conda environment).

`python -m orange.canvas -l 2 --no-splash --no-welcome` will skip the splash screen and welcome window, and output more debug info. use `-l 4` for more.

add `--clear-widget-settings` to clear the widget settings before start.

to explore the dark side of the orange, try `--style=fusion:breeze-dark`

argument `--help` lists all available options.

to run tests, use `unittest orange.tests orange.widgets.tests`


### setting up for development of all components

should you wish to contribute orange's base components (the widget base and the canvas), you must also clone these two repositories from github instead of installing them as dependencies of orange3.

first, fork all the repositories to which you want to contribute. 

set your github username,

```shell
export my_github_username=replaceme
```

create a conda environment, clone your forks, and install them:

```shell
conda create python=3 --yes --name orange3
conda activate orange3

# install pyqt and pyqtwebengine. you can also use pyqt6
pip install -r requirements-pyqt.txt

git clone ssh://git@github.com/$my_github_username/orange-widget-base
pip install -e orange-widget-base

git clone ssh://git@github.com/$my_github_username/orange-canvas-core
pip install -e orange-canvas-core

git clone ssh://git@github.com/$my_github_username/orange3
pip install -e orange3

# repeat for any add-on repositories
```

it's crucial to install `orange-base-widget` and `orange-canvas-core` before `orange3` to ensure that `orange3` will use your local versions.
"
"fuku-ml","`fukuml`_
=========
.. _fukuml: http://www.fukuball.com/fuku-ml/

.. image:: https://travis-ci.org/fukuball/fuku-ml.svg?branch=master
    :target: https://travis-ci.org/fukuball/fuku-ml

.. image:: https://codecov.io/github/fukuball/fuku-ml/coverage.svg?branch=master
    :target: https://codecov.io/github/fukuball/fuku-ml?branch=master

.. image:: https://badge.fury.io/py/fukuml.svg
    :target: https://badge.fury.io/py/fukuml

.. image:: https://api.codacy.com/project/badge/grade/afc87eff27ab47d6b960ea7b3088c469
    :target: https://www.codacy.com/app/fukuball/fuku-ml

.. image:: https://img.shields.io/badge/made%20with-%e2%9d%a4-ff69b4.svg
    :target: http://www.fukuball.com

simple machine learning library / Á∞°ÂñÆÊòìÁî®ÁöÑÊ©üÂô®Â≠∏ÁøíÂ•ó‰ª∂

installation
============

.. code-block:: bash

    $ pip install fukuml

tutorial
============

- lesson 1: `perceptron binary classification learning algorithm`_

- appendix 1: `play with your own dataset`_

- appendix 2: `indievox open data/api Êô∫ÊÖßÈü≥Ê®ÇÊáâÁî®Ôºöan introduce to indievox open data/api and the intelligent music application`_

.. _perceptron binary classification learning algorithm: https://github.com/fukuball/fukuml-tutorial/blob/master/perceptron%20binary%20classification%20learning%20algorithm%20tutorial.ipynb

.. _play with your own dataset: https://github.com/fukuball/fukuml-tutorial/blob/master/play%20with%20your%20own%20dataset%20tutorial.ipynb

.. _indievox open data/api Êô∫ÊÖßÈü≥Ê®ÇÊáâÁî®Ôºöan introduce to indievox open data/api and the intelligent music application: https://speakerdeck.com/fukuball/api-and-the-intelligent-music-application

algorithm
============

- perceptron
    - perceptron binary classification learning algorithm
    - perceptron multi classification learning algorithm
    - pocket perceptron binary classification learning algorithm
    - pocket perceptron multi classification learning algorithm
- regression
    - linear regression learning algorithm
    - linear regression binary classification learning algorithm
    - linear regression multi classification learning algorithm
    - ridge regression learning algorithm
    - ridge regression binary classification learning algorithm
    - ridge regression multi classification learning algorithm
    - kernel ridge regression learning algorithm
    - kernel ridge regression binary classification learning algorithm
    - kernel ridge regression multi classification learning algorithm
- logistic regression
    - logistic regression learning algorithm
    - logistic regression binary classification learning algorithm
    - logistic regression one vs all multi classification learning algorithm
    - logistic regression one vs one multi classification learning algorithm
    - l2 regularized logistic regression learning algorithm
    - l2 regularized logistic regression binary classification learning algorithm
    - kernel logistic regression learning algorithm
- support vector machine
    - primal hard margin support vector machine binary classification learning algorithm
    - dual hard margin support vector machine binary classification learning algorithm
    - polynomial kernel support vector machine binary classification learning algorithm
    - gaussian kernel support vector machine binary classification learning algorithm
    - soft polynomial kernel support vector machine binary classification learning algorithm
    - soft gaussian kernel support vector machine binary classification learning algorithm
    - polynomial kernel support vector machine multi classification learning algorithm
    - gaussian kernel support vector machine multi classification learning algorithm
    - soft polynomial kernel support vector machine multi classification learning algorithm
    - soft gaussian kernel support vector machine multi classification learning algorithm
    - probabilistic support vector machine learning algorithm
    - least squares support vector machine binary classification learning algorithm
    - least squares support vector machine multi classification learning algorithm
    - support vector regression learning algorithm
- decision tree
    - decision stump binary classification learning algorithm
    - adaboost stump binary classification learning algorithm
    - adaboost decision tree classification learning algorithm
    - gradient boost decision tree regression learning algorithm
    - decision tree classification learning algorithm
    - decision tree regression learning algorithm
    - random forest classification learning algorithm
    - random forest regression learning algorithm
- neural network
    - neural network learning algorithm
    - neural network binary classification learning algorithm
- accelerator
    - linear regression accelerator
- feature transform
    - polynomial feature transform
    - legendre feature transform
- validation
    - 10 fold cross validation
- blending
    - uniform blending for classification
    - linear blending for classification
    - uniform blending for regression
    - linear blending for regression

usage
============

.. code-block:: py

    >>> import numpy as np
    # we need numpy as a base libray

    >>> import fukuml.pla as pla
    # import fukuml.pla to do perceptron learning

    >>> your_input_data_file = '/path/to/your/data/file'
    # assign your input data file, please check the data format: https://github.com/fukuball/fuku-ml/blob/master/fukuml/dataset/pla_binary_train.dat

    >>> pla_bc = pla.binaryclassifier()
    # new a pla binary classifier

    >>> pla_bc.load_train_data(your_input_data_file)
    # load train data

    >>> pla_bc.set_param()
    # set parameter

    >>> pla_bc.init_w()
    # init the w

    >>> w = pla_bc.train()
    # train by perceptron learning algorithm to find best w

    >>> test_data = 'each feature of data x separated with spaces. and the ground truth y put in the end of line separated by a space'
    # assign test data, format like this '0.97681 0.10723 0.64385 ........ 0.29556 1'

    >>> prediction = pla_bc.prediction(test_data)
    # prediction by trained w

    >>> print prediction['input_data_x']
    # print test data x

    >>> print prediction['input_data_y']
    # print test data y

    >>> print prediction['prediction']
    # print the prediction, will find out prediction is the same as pla_bc.test_data_y

for detail, please check https://github.com/fukuball/fuku-ml/blob/master/doc/sample_code.rst

tests
=========

.. code-block:: shell

   python test_fuku_ml.py

pep8
=========

.. code-block:: shell

   pep8 fukuml/*.py --ignore=e501

donate
=========

if you find fuku-ml useful, please consider a donation. thank you!

- bitcoin: 1bbihqu3czsdylsp9bvqq7pi1z1jtdaaq9
- eth: 0x92da3f837bf2f79d422bb8ceac632208f94cde33


license
=========
the mit license (mit)

copyright (c) 2016 fukuball

permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""software""), to deal
in the software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the software, and to permit persons to whom the software is
furnished to do so, subject to the following conditions:

the above copyright notice and this permission notice shall be included in all
copies or substantial portions of the software.

the software is provided ""as is"", without warranty of any kind, express or
implied, including but not limited to the warranties of merchantability,
fitness for a particular purpose and noninfringement. in no event shall the
authors or copyright holders be liable for any claim, damages or other
liability, whether in an action of contract, tort or otherwise, arising from,
out of or in connection with the software or the use or other dealings in the
software."
"PyTorch Lightning","<div align=""center"">

**news: pytorch lightning has been renamed lightning!**

<img src=""https://github.com/likethecognac/images/blob/main/icon_color_whitebolt.png"" width=""85px"">

**the deep learning framework to train, deploy, and ship ai products lightning fast.**

______________________________________________________________________

<p align=""center"">
  <a href=""https://www.lightning.ai/"">lightning gallery</a> ‚Ä¢
  <a href=""#key-features"">key features</a> ‚Ä¢
  <a href=""#how-to-use"">how to use</a> ‚Ä¢
  <a href=""https://pytorch-lightning.readthedocs.io/en/stable/"">docs</a> ‚Ä¢
  <a href=""#examples"">examples</a> ‚Ä¢
  <a href=""#community"">community</a> ‚Ä¢
  <a href=""https://pytorch-lightning.readthedocs.io/en/stable/generated/contributing.html"">contribute</a> ‚Ä¢
  <a href=""#license"">license</a>
</p>

<!-- do not add conda downloads... readme changes must be approved by eden or will -->

[![pypi - python version](https://img.shields.io/pypi/pyversions/pytorch-lightning)](https://pypi.org/project/pytorch-lightning/)
[![pypi status](https://badge.fury.io/py/pytorch-lightning.svg)](https://badge.fury.io/py/pytorch-lightning)
[![pypi status](https://pepy.tech/badge/pytorch-lightning)](https://pepy.tech/project/pytorch-lightning)
[![conda](https://img.shields.io/conda/v/conda-forge/pytorch-lightning?label=conda&color=success)](https://anaconda.org/conda-forge/pytorch-lightning)
[![dockerhub](https://img.shields.io/docker/pulls/pytorchlightning/pytorch_lightning.svg)](https://hub.docker.com/r/pytorchlightning/pytorch_lightning)
[![codecov](https://codecov.io/gh/lightning-ai/lightning/branch/master/graph/badge.svg?token=smzx8mnkla)](https://codecov.io/gh/lightning-ai/lightning)

[![readthedocs](https://readthedocs.org/projects/pytorch-lightning/badge/?version=stable)](https://pytorch-lightning.readthedocs.io/en/stable/)
[![slack](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://www.pytorchlightning.ai/community)
[![license](https://img.shields.io/badge/license-apache%202.0-blue.svg)](https://github.com/lightning-ai/lightning/blob/master/license)

<!--
[![codefactor](https://www.codefactor.io/repository/github/lightning-ai/lightning/badge)](https://www.codefactor.io/repository/github/lightning-ai/lightning)
-->

</div>

###### \*codecov is > 90%+ but build delays may show less

______________________________________________________________________

## pytorch lightning is just organized pytorch

lightning disentangles pytorch code to decouple the science from the engineering.
![pt to pl](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)

## build ai products with lightning apps

once you're done building models, publish a paper demo or build a full production end-to-end ml system with lightning apps. lightning apps remove the cloud infrastructure boilerplate so you can focus on solving the research or business problems. lightning apps can run on the lightning cloud, your own cluster or a private cloud.

[browse available lightning apps here](https://lightning.ai/)

<div align=""center"">
    <img src=""https://pl-public-data.s3.amazonaws.com/assets_lightning/lightning-apps-teaser.png"" width=""80%"">
</div>

### [learn more about lightning apps](src/lightning_app/readme.md)

______________________________________________________________________

## lightning design philosophy

lightning structures pytorch code with these principles:

<div align=""center"">
  <img src=""https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/philosophies.jpg"" max-height=""250px"">
</div>

lightning forces the following structure to your code which makes it reusable and shareable:

- research code (the lightningmodule).
- engineering code (you delete, and is handled by the trainer).
- non-essential research code (logging, etc... this goes in callbacks).
- data (use pytorch dataloaders or organize them into a lightningdatamodule).

once you do this, you can train on multiple-gpus, tpus, cpus and even in 16-bit precision without changing your code!

[get started in just 15 minutes](https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html)

______________________________________________________________________

## continuous integration

lightning is rigorously tested across multiple cpus, gpus, tpus, ipus, and hpus and against major python and pytorch versions.

<details>
  <summary>current build statuses</summary>

<center>

|       system / pytorch ver.        |                                                                                              1.11                                                                                               |                                                                                                              1.12                                                                                                               | 1.13                                                                                                                                                                                                                            | 2.0  |
| :--------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
|        linux py3.9 \[gpus\]        |                                                                                                -                                                                                                | [![build status](<https://dev.azure.com/lightning-ai/lightning/_apis/build/status/pytorch-lightning%20(gpus)?branchname=master>)](https://dev.azure.com/lightning-ai/lightning/_build/latest?definitionid=24&branchname=master) | [![build status](<https://dev.azure.com/lightning-ai/lightning/_apis/build/status/pytorch-lightning%20(gpus)?branchname=master>)](https://dev.azure.com/lightning-ai/lightning/_build/latest?definitionid=24&branchname=master) | soon |
|        linux py3.9 \[tpus\]        |                                                                                                -                                                                                                |                     [![test pytorch - tpu](https://github.com/lightning-ai/lightning/actions/workflows/tpu-tests.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/tpu-tests.yml)                     |                                                                                                                                                                                                                                 | soon |
|        linux py3.8 \[ipus\]        |                                                                                                -                                                                                                |                                                                                                                -                                                                                                                | [![build status](<https://dev.azure.com/lightning-ai/lightning/_apis/build/status/pytorch-lightning%20(ipus)?branchname=master>)](https://dev.azure.com/lightning-ai/lightning/_build/latest?definitionid=25&branchname=master) | soon |
|        linux py3.8 \[hpus\]        |                                                                                                -                                                                                                |                                                                                                                -                                                                                                                | [![build status](<https://dev.azure.com/lightning-ai/lightning/_apis/build/status/pytorch-lightning%20(hpus)?branchname=master>)](https://dev.azure.com/lightning-ai/lightning/_build/latest?definitionid=26&branchname=master) | soon |
|  linux (multiple python versions)  | [![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml) |                 [![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml)                 | [![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | soon |
|   osx (multiple python versions)   | [![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml) |                 [![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml)                 | [![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | soon |
| windows (multiple python versions) | [![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml) |                 [![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml)                 | [![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | soon |

</center>
</details>

______________________________________________________________________

## how to use

### step 0: install

simple installation from pypi

```bash
pip install pytorch-lightning
```

<!-- following section will be skipped from pypi description -->

<details>
  <summary>other installation options</summary>
    <!-- following section will be skipped from pypi description -->

#### install with optional dependencies

```bash
pip install pytorch-lightning['extra']
```

#### conda

```bash
conda install pytorch-lightning -c conda-forge
```

#### install stable version

install future release from the source

```bash
pip install https://github.com/lightning-ai/lightning/archive/refs/heads/release/stable.zip -u
```

#### install bleeding-edge

install nightly from the source (no guarantees)

```bash
pip install https://github.com/lightning-ai/lightning/archive/refs/heads/master.zip -u
```

or from testing pypi

```bash
pip install -iu https://test.pypi.org/simple/ pytorch-lightning
```

</details>
<!-- end skipping pypi description -->

### step 1: add these imports

```python
import os
import torch
from torch import nn
import torch.nn.functional as f
from torchvision.datasets import mnist
from torch.utils.data import dataloader, random_split
from torchvision import transforms
import pytorch_lightning as pl
```

### step 2: define a lightningmodule (nn.module subclass)

a lightningmodule defines a full *system* (ie: a gan, autoencoder, bert or a simple image classifier).

```python
class litautoencoder(pl.lightningmodule):
    def __init__(self):
        super().__init__()
        self.encoder = nn.sequential(nn.linear(28 * 28, 128), nn.relu(), nn.linear(128, 3))
        self.decoder = nn.sequential(nn.linear(3, 128), nn.relu(), nn.linear(128, 28 * 28))

    def forward(self, x):
        # in lightning, forward defines the prediction/inference actions
        embedding = self.encoder(x)
        return embedding

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop. it is independent of forward
        x, y = batch
        x = x.view(x.size(0), -1)
        z = self.encoder(x)
        x_hat = self.decoder(z)
        loss = f.mse_loss(x_hat, x)
        self.log(""train_loss"", loss)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.adam(self.parameters(), lr=1e-3)
        return optimizer
```

**note: training_step defines the training loop. forward defines how the lightningmodule behaves during inference/prediction.**

### step 3: train!

```python
dataset = mnist(os.getcwd(), download=true, transform=transforms.totensor())
train, val = random_split(dataset, [55000, 5000])

autoencoder = litautoencoder()
trainer = pl.trainer()
trainer.fit(autoencoder, dataloader(train), dataloader(val))
```

## advanced features

lightning has over [40+ advanced features](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags) designed for professional ai research at scale.

here are some examples:

<div align=""center"">
  <img src=""https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/features_2.jpg"" max-height=""600px"">
</div>

<details>
  <summary>highlighted feature code snippets</summary>

```python
# 8 gpus
# no code changes needed
trainer = trainer(max_epochs=1, accelerator=""gpu"", devices=8)

# 256 gpus
trainer = trainer(max_epochs=1, accelerator=""gpu"", devices=8, num_nodes=32)
```

<summary>train on tpus without code changes</summary>

```python
# no code changes needed
trainer = trainer(accelerator=""tpu"", devices=8)
```

<summary>16-bit precision</summary>

```python
# no code changes needed
trainer = trainer(precision=16)
```

<summary>experiment managers</summary>

```python
from pytorch_lightning import loggers

# tensorboard
trainer = trainer(logger=tensorboardlogger(""logs/""))

# weights and biases
trainer = trainer(logger=loggers.wandblogger())

# comet
trainer = trainer(logger=loggers.cometlogger())

# mlflow
trainer = trainer(logger=loggers.mlflowlogger())

# neptune
trainer = trainer(logger=loggers.neptunelogger())

# ... and dozens more
```

<summary>earlystopping</summary>

```python
es = earlystopping(monitor=""val_loss"")
trainer = trainer(callbacks=[es])
```

<summary>checkpointing</summary>

```python
checkpointing = modelcheckpoint(monitor=""val_loss"")
trainer = trainer(callbacks=[checkpointing])
```

<summary>export to torchscript (jit) (production use)</summary>

```python
# torchscript
autoencoder = litautoencoder()
torch.jit.save(autoencoder.to_torchscript(), ""model.pt"")
```

<summary>export to onnx (production use)</summary>

```python
# onnx
with tempfile.namedtemporaryfile(suffix="".onnx"", delete=false) as tmpfile:
    autoencoder = litautoencoder()
    input_sample = torch.randn((1, 64))
    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=true)
    os.path.isfile(tmpfile.name)
```

</details>

### pro-level control of optimization (advanced users)

for complex/professional level work, you have optional full control of the optimizers.

```python
class litautoencoder(pl.lightningmodule):
    def __init__(self):
        super().__init__()
        self.automatic_optimization = false

    def training_step(self, batch, batch_idx):
        # access your optimizers with use_pl_optimizer=false. default is true
        opt_a, opt_b = self.optimizers(use_pl_optimizer=true)

        loss_a = ...
        self.manual_backward(loss_a, opt_a)
        opt_a.step()
        opt_a.zero_grad()

        loss_b = ...
        self.manual_backward(loss_b, opt_b, retain_graph=true)
        self.manual_backward(loss_b, opt_b)
        opt_b.step()
        opt_b.zero_grad()
```

______________________________________________________________________

## advantages over unstructured pytorch

- models become hardware agnostic
- code is clear to read because engineering code is abstracted away
- easier to reproduce
- make fewer mistakes because lightning handles the tricky engineering
- keeps all the flexibility (lightningmodules are still pytorch modules), but removes a ton of boilerplate
- lightning has dozens of integrations with popular machine learning tools.
- [tested rigorously with every new pr](https://github.com/lightning-ai/lightning/tree/master/tests). we test every combination of pytorch and python supported versions, every os, multi gpus and even tpus.
- minimal running speed overhead (about 300 ms per epoch compared with pure pytorch).

______________________________________________________________________

## examples

###### self-supervised learning

- [cpc transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#cpc-transforms)
- [moco v2 tranforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#moco-v2-transforms)
- [simclr transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#simclr-transforms)

###### convolutional architectures

- [gpt-2](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#gpt-2)
- [unet](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#unet)

###### reinforcement learning

- [dqn loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#dqn-loss)
- [double dqn loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#double-dqn-loss)
- [per dqn loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#per-dqn-loss)

###### gans

- [basic gan](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#basic-gan)
- [dcgan](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#dcgan)

###### classic ml

- [logistic regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#logistic-regression)
- [linear regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#linear-regression)

______________________________________________________________________

## community

the lightning community is maintained by

- [10+ core contributors](https://pytorch-lightning.readthedocs.io/en/latest/governance.html) who are all a mix of professional engineers, research scientists, and ph.d. students from top ai labs.
- 590+ active community contributors.

want to help us build lightning and reduce boilerplate for thousands of researchers? [learn how to make your first contribution here](https://pytorch-lightning.readthedocs.io/en/stable/generated/contributing.html)

lightning is also part of the [pytorch ecosystem](https://pytorch.org/ecosystem/) which requires projects to have solid testing, documentation and support.

### asking for help

if you have any questions please:

1. [read the docs](https://pytorch-lightning.rtfd.io/en/latest).
1. [search through existing discussions](https://github.com/lightning-ai/lightning/discussions), or [add a new question](https://github.com/lightning-ai/lightning/discussions/new)
1. [join our slack](https://www.pytorchlightning.ai/community).
"
"PyTorch Lightning Bolts","<div align=""center"">

<img src=""docs/source/_images/logos/bolts_logo.png"" width=""400px"">

**deep learning components for extending pytorch lightning**

______________________________________________________________________

<p align=""center"">
  <a href=""#install"">installation</a> ‚Ä¢
  <a href=""https://lightning-bolts.readthedocs.io/en/latest/"">latest docs</a> ‚Ä¢
  <a href=""https://lightning-bolts.readthedocs.io/en/stable/"">stable docs</a> ‚Ä¢
  <a href=""#what-is-bolts"">about</a> ‚Ä¢
  <a href=""#team"">community</a> ‚Ä¢
  <a href=""https://www.pytorchlightning.ai/"">website</a> ‚Ä¢
  <a href=""https://www.grid.ai/"">grid ai</a> ‚Ä¢
  <a href=""#license"">license</a>
</p>

[![pypi status](https://badge.fury.io/py/lightning-bolts.svg)](https://badge.fury.io/py/lightning-bolts)
[![pypi status](https://pepy.tech/badge/lightning-bolts)](https://pepy.tech/project/lightning-bolts)
[![build status](https://dev.azure.com/lightning-ai/lightning%20bolts/_apis/build/status/lightning-ai.lightning-bolts?branchname=master)](https://dev.azure.com/lightning-ai/lightning%20bolts/_build?definitionid=31&_a=summary&repositoryfilter=13&branchfilter=4923%2c4923)
[![codecov](https://codecov.io/gh/lightning-ai/lightning-bolts/branch/master/graph/badge.svg?token=o8p0qhvj90)](https://codecov.io/gh/lightning-ai/lightning-bolts)

[![documentation status](https://readthedocs.org/projects/lightning-bolts/badge/?version=latest)](https://lightning-bolts.readthedocs.io/en/latest/)
[![slack](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://www.pytorchlightning.ai/community)
[![license](https://img.shields.io/badge/license-apache%202.0-blue.svg)](https://github.com/pytorchlightning/lightning-bolts/blob/master/license)
[![doi](https://zenodo.org/badge/250025410.svg)](https://zenodo.org/badge/latestdoi/250025410)

</div>

______________________________________________________________________

## getting started

pip / conda

```bash
pip install lightning-bolts
```

<details>
  <summary>other installations</summary>

install bleeding-edge (no guarantees)

```bash
pip install git+https://github.com/pytorchlightning/lightning-bolts.git@master --upgrade
```

to install all optional dependencies

```bash
pip install lightning-bolts[""extra""]
```

</details>

## what is bolts

bolts provides a variety of components to extend pytorch lightning such as callbacks & datasets, for applied research and production.

## news

- sept 22: [leverage sparsity for faster inference with lightning flash and sparseml](https://devblog.pytorchlightning.ai/leverage-sparsity-for-faster-inference-with-lightning-flash-and-sparseml-cdda1165622b)
- aug 26: [fine-tune transformers faster with lightning flash and torch ort](https://devblog.pytorchlightning.ai/fine-tune-transformers-faster-with-lightning-flash-and-torch-ort-ec2d53789dc3)

#### example 1: accelerate lightning training with the torch ort callback

torch ort converts your model into an optimized onnx graph, speeding up training & inference when using nvidia or amd gpus. see the [documentation](https://lightning-bolts.readthedocs.io/en/latest/callbacks/torch_ort.html) for more details.

```python
from pytorch_lightning import lightningmodule, trainer
import torchvision.models as models
from pl_bolts.callbacks import ortcallback


class visionmodel(lightningmodule):
    def __init__(self):
        super().__init__()
        self.model = models.vgg19_bn(pretrained=true)

    ...


model = visionmodel()
trainer = trainer(gpus=1, callbacks=ortcallback())
trainer.fit(model)
```

#### example 2: introduce sparsity with the sparsemlcallback to accelerate inference

we can introduce sparsity during fine-tuning with [sparseml](https://github.com/neuralmagic/sparseml), which ultimately allows us to leverage the [deepsparse](https://github.com/neuralmagic/deepsparse) engine to see performance improvements at inference time.

```python
from pytorch_lightning import lightningmodule, trainer
import torchvision.models as models
from pl_bolts.callbacks import sparsemlcallback


class visionmodel(lightningmodule):
    def __init__(self):
        super().__init__()
        self.model = models.vgg19_bn(pretrained=true)

    ...


model = visionmodel()
trainer = trainer(gpus=1, callbacks=sparsemlcallback(recipe_path=""recipe.yaml""))
trainer.fit(model)
```

## are specific research implementations supported?

we'd like to encourage users to contribute general components that will help a broad range of problems, however components that help specifics domains will also be welcomed!

for example a callback to help train ssl models would be a great contribution, however the next greatest ssl model from your latest paper would be a good contribution to [lightning flash](https://github.com/pytorchlightning/lightning-flash).

use [lightning flash](https://github.com/pytorchlightning/lightning-flash) to train, predict and serve state-of-the-art models for applied research. we suggest looking at our [vissl](https://lightning-flash.readthedocs.io/en/latest/integrations/vissl.html) flash integration for ssl based tasks.

## contribute!

bolts is supported by the pytorch lightning team and the pytorch lightning community!

join our slack and/or read our [contributing](./.github/contributing.md) guidelines to get help becoming a contributor!

______________________________________________________________________

## license

please observe the apache 2.0 license that is listed in this repository.
in addition the lightning framework is patent pending.
"
"skorch",".. image:: https://github.com/skorch-dev/skorch/blob/master/assets/skorch_bordered.svg
   :width: 30%

------------

|build| |coverage| |docs| |huggingface| |powered|

a scikit-learn compatible neural network library that wraps pytorch.

.. |build| image:: https://github.com/skorch-dev/skorch/workflows/tests/badge.svg
    :alt: test status
    :scale: 100%

.. |coverage| image:: https://github.com/skorch-dev/skorch/blob/master/assets/coverage.svg
    :alt: test coverage
    :scale: 100%

.. |docs| image:: https://readthedocs.org/projects/skorch/badge/?version=latest
    :alt: documentation status
    :scale: 100%
    :target: https://skorch.readthedocs.io/en/latest/?badge=latest

.. |huggingface| image:: https://github.com/skorch-dev/skorch/actions/workflows/test-hf-integration.yml/badge.svg
    :alt: hugging face integration
    :scale: 100%
    :target: https://github.com/skorch-dev/skorch/actions/workflows/test-hf-integration.yml

.. |powered| image:: https://github.com/skorch-dev/skorch/blob/master/assets/powered.svg
    :alt: powered by
    :scale: 100%
    :target: https://github.com/ottogroup/

=========
resources
=========

- `documentation <https://skorch.readthedocs.io/en/latest/?badge=latest>`_
- `source code <https://github.com/skorch-dev/skorch/>`_
- `installation <https://github.com/skorch-dev/skorch#installation>`_

========
examples
========

to see more elaborate examples, look `here
<https://github.com/skorch-dev/skorch/tree/master/notebooks/readme.md>`__.

.. code:: python

    import numpy as np
    from sklearn.datasets import make_classification
    from torch import nn
    from skorch import neuralnetclassifier

    x, y = make_classification(1000, 20, n_informative=10, random_state=0)
    x = x.astype(np.float32)
    y = y.astype(np.int64)

    class mymodule(nn.module):
        def __init__(self, num_units=10, nonlin=nn.relu()):
            super().__init__()

            self.dense0 = nn.linear(20, num_units)
            self.nonlin = nonlin
            self.dropout = nn.dropout(0.5)
            self.dense1 = nn.linear(num_units, num_units)
            self.output = nn.linear(num_units, 2)
            self.softmax = nn.softmax(dim=-1)

        def forward(self, x, **kwargs):
            x = self.nonlin(self.dense0(x))
            x = self.dropout(x)
            x = self.nonlin(self.dense1(x))
            x = self.softmax(self.output(x))
            return x

    net = neuralnetclassifier(
        mymodule,
        max_epochs=10,
        lr=0.1,
        # shuffle training data on each epoch
        iterator_train__shuffle=true,
    )

    net.fit(x, y)
    y_proba = net.predict_proba(x)

in an `sklearn pipeline <https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.pipeline.html>`_:

.. code:: python

    from sklearn.pipeline import pipeline
    from sklearn.preprocessing import standardscaler

    pipe = pipeline([
        ('scale', standardscaler()),
        ('net', net),
    ])

    pipe.fit(x, y)
    y_proba = pipe.predict_proba(x)

with `grid search <https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.gridsearchcv.html>`_:

.. code:: python

    from sklearn.model_selection import gridsearchcv

    # deactivate skorch-internal train-valid split and verbose logging
    net.set_params(train_split=false, verbose=0)
    params = {
        'lr': [0.01, 0.02],
        'max_epochs': [10, 20],
        'module__num_units': [10, 20],
    }
    gs = gridsearchcv(net, params, refit=false, cv=3, scoring='accuracy', verbose=2)

    gs.fit(x, y)
    print(""best score: {:.3f}, best params: {}"".format(gs.best_score_, gs.best_params_))


skorch also provides many convenient features, among others:

- `learning rate schedulers <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.lrscheduler>`_ (warm restarts, cyclic lr and many more)
- `scoring using sklearn (and custom) scoring functions <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.epochscoring>`_
- `early stopping <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.earlystopping>`_
- `checkpointing <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.checkpoint>`_
- `parameter freezing/unfreezing <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.freezer>`_
- `progress bar <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.progressbar>`_ (for cli as well as jupyter)
- `automatic inference of cli parameters <https://github.com/skorch-dev/skorch/tree/master/examples/cli>`_
- `integration with gpytorch for gaussian processes <https://skorch.readthedocs.io/en/latest/user/probabilistic.html>`_
- `integration with hugging face ü§ó <https://skorch.readthedocs.io/en/stable/user/huggingface.html>`_

============
installation
============

skorch requires python 3.7 or higher.

conda installation
==================

you need a working conda installation. get the correct miniconda for
your system from `here <https://conda.io/miniconda.html>`__.

to install skorch, you need to use the conda-forge channel:

.. code:: bash

    conda install -c conda-forge skorch

we recommend to use a `conda virtual environment <https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html>`_.

**note**: the conda channel is *not* managed by the skorch
maintainers. more information is available `here
<https://github.com/conda-forge/skorch-feedstock>`__.

pip installation
================

to install with pip, run:

.. code:: bash

    python -m pip install -u skorch

again, we recommend to use a `virtual environment
<https://docs.python.org/3/tutorial/venv.html>`_ for this.

from source
===========

if you would like to use the most recent additions to skorch or
help development, you should install skorch from source.

using conda
-----------

to install skorch from source using conda, proceed as follows:

.. code:: bash

    git clone https://github.com/skorch-dev/skorch.git
    cd skorch
    conda create -n skorch-env python=3.10
    conda activate skorch-env
    conda install -c pytorch pytorch
    python -m pip install -r requirements.txt
    python -m pip install .

if you want to help developing, run:

.. code:: bash

    git clone https://github.com/skorch-dev/skorch.git
    cd skorch
    conda create -n skorch-env python=3.10
    conda activate skorch-env
    conda install -c pytorch pytorch
    python -m pip install -r requirements.txt
    python -m pip install -r requirements-dev.txt
    python -m pip install -e .

    py.test  # unit tests
    pylint skorch  # static code checks

you may adjust the python version to any of the supported python versions.

using pip
---------

for pip, follow these instructions instead:

.. code:: bash

    git clone https://github.com/skorch-dev/skorch.git
    cd skorch
    # create and activate a virtual environment
    python -m pip install -r requirements.txt
    # install pytorch version for your system (see below)
    python -m pip install .

if you want to help developing, run:

.. code:: bash

    git clone https://github.com/skorch-dev/skorch.git
    cd skorch
    # create and activate a virtual environment
    python -m pip install -r requirements.txt
    # install pytorch version for your system (see below)
    python -m pip install -r requirements-dev.txt
    python -m pip install -e .

    py.test  # unit tests
    pylint skorch  # static code checks

pytorch
=======

pytorch is not covered by the dependencies, since the pytorch version
you need is dependent on your os and device. for installation
instructions for pytorch, visit the `pytorch website
<http://pytorch.org/>`__. skorch officially supports the last four
minor pytorch versions, which currently are:

- 1.10.2
- 1.11.0
- 1.12.1
- 1.13.0

however, that doesn't mean that older versions don't work, just that
they aren't tested. since skorch mostly relies on the stable part of
the pytorch api, older pytorch versions should work fine.

in general, running this to install pytorch should work:

.. code:: bash

    # using conda:
    conda install pytorch pytorch-cuda -c pytorch
    # using pip
    python -m pip install torch

==================
external resources
==================

- @jakubczakon: `blog post
  <https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem>`_
  ""8 creators and core contributors talk about their model training
  libraries from pytorch ecosystem"" 2020
- @benjaminbossan: `talk 1
  <https://www.youtube.com/watch?v=qbu_dcbjvek>`_ ""skorch: a
  scikit-learn compatible neural network library"" at pycon/pydata 2019
- @githubnemo: `poster <https://github.com/githubnemo/skorch-poster>`_
  for the pytorch developer conference 2019
- @thomasjpfan: `talk 2 <https://www.youtube.com/watch?v=0j7falk0bmq>`_
  ""skorch: a union of scikit learn and pytorch"" at scipy 2019
- @thomasjpfan: `talk 3 <https://www.youtube.com/watch?v=yaxsxf2cq8m>`_
  ""skorch - a union of scikit-learn and pytorch"" at pydata 2018

=============
communication
=============

- `github issues <https://github.com/skorch-dev/skorch/issues>`_: bug
  reports, feature requests, install issues, rfcs, thoughts, etc.

- slack: we run the #skorch channel on the `pytorch slack server
  <https://pytorch.slack.com/>`_, for which you can `request access
  here <https://bit.ly/ptslack>`_.
"
"Edward","[![edward](../master/docs/images/edward_200.png?raw=true)](http://edwardlib.org)

[![build status](https://travis-ci.org/blei-lab/edward.svg?branch=master)](https://travis-ci.org/blei-lab/edward)
[![coverage status](https://coveralls.io/repos/github/blei-lab/edward/badge.svg?branch=master&cachebuster=1)](https://coveralls.io/github/blei-lab/edward?branch=master)
[![join the chat at https://gitter.im/blei-lab/edward](https://badges.gitter.im/blei-lab/edward.svg)](https://gitter.im/blei-lab/edward?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

[edward](http://edwardlib.org) is a python library for probabilistic modeling,
inference, and criticism. it is a testbed for fast experimentation and research
with probabilistic models, ranging from classical hierarchical models on small
data sets to complex deep probabilistic models on large data sets. edward fuses
three fields: bayesian statistics and machine learning, deep learning, and
probabilistic programming.

it supports __modeling__ with

+ directed graphical models
+ neural networks (via libraries such as
    [`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers)
    and
    [keras](http://keras.io))
+ implicit generative models
+ bayesian nonparametrics and probabilistic programs

it supports __inference__ with

+ variational inference
  + black box variational inference
  + stochastic variational inference
  + generative adversarial networks
  + maximum a posteriori estimation
+ monte carlo
  + gibbs sampling
  + hamiltonian monte carlo
  + stochastic gradient langevin dynamics
+ compositions of inference
  + expectation-maximization
  + pseudo-marginal and abc methods
  + message passing algorithms

it supports __criticism__ of the model and inference with

+ point-based evaluations
+ posterior predictive checks

edward is built on top of [tensorflow](https://www.tensorflow.org).
it enables features such as computational graphs, distributed
training, cpu/gpu integration, automatic differentiation, and
visualization with tensorboard.

## resources

+ [edward website](http://edwardlib.org)
+ [edward forum](http://discuss.edwardlib.org)
+ [edward gitter channel](http://gitter.im/blei-lab/edward)
+ [edward releases](https://github.com/blei-lab/edward/releases)
+ [edward papers, posters, and slides](https://github.com/edwardlib/papers)

see [getting started](http://edwardlib.org/getting-started) for how to install edward.
"
"modAL","<img src=""https://modal-python.readthedocs.io/en/latest/_static/modal_b.png"" alt=""modal"" style=""width: 400px;"">

modular active learning framework for python3

[![travis-ci-master](https://travis-ci.org/modal-python/modal.svg?branch=master)](https://travis-ci.org/modal-python/modal) [![codecov-master](https://codecov.io/gh/modal-python/modal/branch/master/graph/badge.svg)](https://codecov.io/gh/modal-python/modal) [![readthedocs](https://readthedocs.org/projects/modal-python/badge/?version=latest)](http://modal-python.readthedocs.io/en/latest/?badge=latest)

## page contents
- [introduction](#introduction)  
- [active learning from bird's-eye view](#active-learning)  
- [modal in action](#modal-in-action)
  - [from zero to one in a few lines of code](#initialization)  
  - [replacing parts quickly](#replacing-parts)  
  - [replacing parts with your own solutions](#replacing-parts-with-your-own-solutions)  
  - [an example with active regression](#active-regression)
  - [additional examples](#additional-examples)  
- [installation](#installation)  
- [documentation](#documentation)  
- [citing](#citing)  
- [about the developer](#about-the-developer)

# introduction<a name=""introduction""></a>
modal is an active learning framework for python3, designed with *modularity, flexibility* and *extensibility* in mind. built on top of scikit-learn, it allows you to rapidly create active learning workflows with nearly complete freedom. what is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease.

# active learning from bird's-eye view<a name=""active-learning""></a>
with the recent explosion of available data, you have can have millions of unlabelled examples with a high cost to obtain labels. for instance, when trying to predict the sentiment of tweets, obtaining a training set can require immense manual labour. but worry not, active learning comes to the rescue! in general, al is a framework allowing you to increase classification performance by intelligently querying you to label the most informative instances. to give an example, suppose that you have the following data and classifier with shaded regions signifying the classification probability.

<p align=""center"">
  <img src=""https://modal-python.readthedocs.io/en/latest/_images/motivating-example.png"" height=""600px"" width=""600px""/>
</p>

suppose that you can query the label of an unlabelled instance, but it costs you a lot. which one would you choose? by querying an instance in the uncertain region, surely you obtain more information than querying by random. active learning gives you a set of tools to handle problems like this. in general, an active learning workflow looks like the following.

<p align=""center"">
 <img src=""https://modal-python.readthedocs.io/en/latest/_images/active-learning.png""/>
</p>

the key components of any workflow are the **model** you choose, the **uncertainty** measure you use and the **query** strategy you apply to request labels. with modal, instead of choosing from a small set of built-in components, you have the freedom to seamlessly integrate scikit-learn or keras models into your algorithm and easily tailor your custom query strategies and uncertainty measures.

# modal in action<a name=""modal-in-action""></a>
let's see what modal can do for you!

## from zero to one in a few lines of code<a name=""initialization""></a>
active learning with a scikit-learn classifier, for instance randomforestclassifier, can be as simple as the following.
```python
from modal.models import activelearner
from sklearn.ensemble import randomforestclassifier

# initializing the learner
learner = activelearner(
    estimator=randomforestclassifier(),
    x_training=x_training, y_training=y_training
)

# query for labels
query_idx, query_inst = learner.query(x_pool)

# ...obtaining new labels from the oracle...

# supply label for queried instance
learner.teach(x_pool[query_idx], y_new)
```

## replacing parts quickly<a name=""replacing-parts""></a>
if you would like to use different uncertainty measures and query strategies than the default uncertainty sampling, you can either replace them with several built-in strategies or you can design your own by following a few very simple design principles. for instance, replacing the default uncertainty measure to classification entropy looks the following.
```python
from modal.models import activelearner
from modal.uncertainty import entropy_sampling
from sklearn.ensemble import randomforestclassifier

learner = activelearner(
    estimator=randomforestclassifier(),
    query_strategy=entropy_sampling,
    x_training=x_training, y_training=y_training
)
```

## replacing parts with your own solutions<a name=""replacing-parts-with-your-own-solutions""></a>
modal was designed to make it easy for you to implement your own query strategy. for example, implementing and using a simple random sampling strategy is as easy as the following.
```python
import numpy as np

def random_sampling(classifier, x_pool):
    n_samples = len(x_pool)
    query_idx = np.random.choice(range(n_samples))
    return query_idx, x_pool[query_idx]

learner = activelearner(
    estimator=randomforestclassifier(),
    query_strategy=random_sampling,
    x_training=x_training, y_training=y_training
)
```
for more details on how to implement your custom strategies, visit the page [extending modal](https://modal-python.readthedocs.io/en/latest/content/overview/extending-modal.html)!

## an example with active regression<a name=""active-regression""></a>
to see modal in *real* action, let's consider an active regression problem with gaussian processes! in this example, we shall try to learn the *noisy sine* function:
```python
import numpy as np

x = np.random.choice(np.linspace(0, 20, 10000), size=200, replace=false).reshape(-1, 1)
y = np.sin(x) + np.random.normal(scale=0.3, size=x.shape)
```
for active learning, we shall define a custom query strategy tailored to gaussian processes. in a nutshell, a *query stategy* in modal is a function taking (at least) two arguments (an estimator object and a pool of examples), outputting the index of the queried instance. in our case, the arguments are ```regressor``` and ```x```.
```python
def gp_regression_std(regressor, x):
    _, std = regressor.predict(x, return_std=true)
    return np.argmax(std)
```
after setting up the query strategy and the data, the active learner can be initialized.
```python
from modal.models import activelearner
from sklearn.gaussian_process import gaussianprocessregressor
from sklearn.gaussian_process.kernels import whitekernel, rbf

n_initial = 5
initial_idx = np.random.choice(range(len(x)), size=n_initial, replace=false)
x_training, y_training = x[initial_idx], y[initial_idx]

kernel = rbf(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \
         + whitekernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))

regressor = activelearner(
    estimator=gaussianprocessregressor(kernel=kernel),
    query_strategy=gp_regression_std,
    x_training=x_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)
)
```
the initial regressor is not very accurate.
<p align=""center"">
  <img src=""https://modal-python.readthedocs.io/en/latest/_images/gp-initial.png"">
</p>

the blue band enveloping the regressor represents the standard deviation of the gaussian process at the given point. now we are ready to do active learning!
```python
# active learning
n_queries = 10
for idx in range(n_queries):
    query_idx, query_instance = regressor.query(x)
    regressor.teach(x[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))
```
after a few queries, we can see that the prediction is much improved.

<p align=""center"">
 <img src=""https://modal-python.readthedocs.io/en/latest/_images/gp-final.png"">
</p>

## additional examples<a name=""additional-examples""></a>
including this, many examples are available:
- [pool-based sampling](https://modal-python.readthedocs.io/en/latest/content/examples/pool-based_sampling.html)  
- [stream-based sampling](https://modal-python.readthedocs.io/en/latest/content/examples/stream-based_sampling.html)  
- [active regression](https://modal-python.readthedocs.io/en/latest/content/examples/active_regression.html)  
- [ensemble regression](https://modal-python.readthedocs.io/en/latest/content/examples/ensemble_regression.html)  
- [bayesian optimization](https://modal-python.readthedocs.io/en/latest/content/examples/bayesian_optimization.html)  
- [query by committee](https://modal-python.readthedocs.io/en/latest/content/examples/query_by_committee.html)  
- [bootstrapping and bagging](https://modal-python.readthedocs.io/en/latest/content/examples/bootstrapping_and_bagging.html)  
- [keras integration](https://modal-python.readthedocs.io/en/latest/content/examples/keras_integration.html)

# installation<a name=""installation""></a>
modal requires
- python >= 3.5
- numpy >= 1.13
- scipy >= 0.18
- scikit-learn >= 0.18

you can install modal directly with pip:  
```
pip install modal
```
alternatively, you can install modal directly from source:  
```
pip install git+https://github.com/modal-python/modal.git
```

# documentation<a name=""documentation""></a>
you can find the documentation of modal at [https://modal-python.github.io](https://modal-python.github.io), where several tutorials and working examples are available, along with a complete api reference. for running the examples, matplotlib >= 2.0 is recommended.

# citing<a name=""citing""></a>
if you use modal in your projects, you can cite it as
```
@article{modal2018,
    title={mod{al}: {a} modular active learning framework for {p}ython},
    author={tivadar danka and peter horvath},
    url={https://github.com/modal-python/modal},
    note={available on arxiv at \url{https://arxiv.org/abs/1805.00979}}
}
```

# about the developer<a name=""about-the-developer"">
modal is developed by me, [tivadar danka](https://www.tivadardanka.com) (aka [cosmic-cortex](https://github.com/cosmic-cortex) in github). i have a phd in pure mathematics, but i fell in love with biology and machine learning right after i finished my phd. i have changed fields and now i work in the [bioimage analysis and machine learning group of peter horvath](http://group.szbk.u-szeged.hu/sysbiol/horvath-peter-lab-index.html), where i am working to develop active learning strategies for intelligent sample analysis in biology. during my work i realized that in python, creating and prototyping active learning workflows can be made really easy and fast with scikit-learn, so i ended up developing a general framework for this. the result is modal :) if you have any questions, requests or suggestions, you can contact me at <a href=""mailto:85a5187a@opayq.com"">85a5187a@opayq.com</a>! i hope you'll find modal useful!
"
"Cogitare","<p align=""center""><img width=""80%"" src=""https://raw.githubusercontent.com/cogitare-ai/cogitare/master/docs/source/art/logo-line.png"" /></p>

<h3 align=""center""><b>cogitare</b> is a modern, fast, and modular deep learning and machine learning framework for python. a friendly interface for beginners and a powerful toolset for experts.</h3>

<h3 align=""center""><b>cogitare</b> is built on top of pytorch.</h3>

<p align=""center"">
  <a href=""http://docs.cogitare-ai.org/"">documentation</a> ‚Ä¢
  <a href=""http://tutorials.cogitare-ai.org/"">tutorials</a> ‚Ä¢
  <a href=""#1-about"">about</a> ‚Ä¢
  <a href=""#2-install"">install</a> ‚Ä¢
  <a href=""#3-quickstart"">quickstart</a> ‚Ä¢
  <a href=""#4-contribution"">contribution</a>
</p>


<p align=""center"">
	<a href=""https://travis-ci.org/cogitare-ai/cogitare"">
		<img src=""https://travis-ci.org/cogitare-ai/cogitare.svg?branch=master"" />
	</a>
	<a href=""https://codecov.io/gh/cogitare-ai/cogitare"">
		<img src=""https://codecov.io/gh/cogitare-ai/cogitare/branch/master/graph/badge.svg"" />
	</a>
	<a href=""https://badge.fury.io/py/cogitare"">
		<img src=""https://badge.fury.io/py/cogitare.svg"" alt=""pypi version"" height=""18"">
	</a>
</p>


# 1. about

it uses the best of [pytorch](http://pytorch.org/), [dask](https://dask.pydata.org/), [numpy](http://www.numpy.org/), and others tools through a simple interface to train, to evaluate, to test
models and more.

with cogitare, you can use classical machine learning algorithms with high
performance and develop state-of-the-art models quickly.

check the tutorials at http://tutorials.cogitare-ai.org/

the primary objectives of cogitare are:

- provide an easy-to-use interface to train and evaluate models;
- provide tools to debug and analyze the model;
- provide implementations of state-of-the-art models (models for common tasks, ready
  to train and ready to use);
- provide ready-to-use implementations of straightforward and classical models (such as
  logisticregression);
- be compatible with models for a broad range of problems;
- be compatible with other tools (scikit-learn, etcs);
- keep growing with the community: accept as many new features as possible;
- provide a friendly interface to beginners, and powerful features for experts;
- take the best of the hardware through multi-processing and multi-threading;
- and others.

currently, it's a work in progress project that aims to provide a complete
toolchain for machine learning and deep learning development, taking the best
of cuda and multi-core processing.

# 2. install

- install pytorch from http://pytorch.org/
- install cogitare from pip:

      pip install cogitare

- cogitare is in active development, so it's recommended to get the latest version from github. to install directly from github, use:

      pip install -e git+https://github.com/cogitare-ai/cogitare#egg=cogitare

# 3. quickstart


this is a simple tutorial to get started with cogitare main functionalities.

in this tutorial, we will write a convolutional neural network (cnn) to
classify handwritten digits (mnist).

### 3.1 model

we start by defining our cnn model.

when developing a model with cogitare, your model must extend the ``cogitare.model`` class. this class provides the model interface, which allows you to train and evaluate the model efficiently.

to implement a model, you must extend the ``cogitare.model`` class and implement the ``forward()`` and ``loss()`` methods. the forward method will receive the batch. in this way, it is necessary to implement the forward pass through the network in this method, and then return the output of the net. the loss method will receive the output of the ``forward()`` and the batch received from the iterator, apply a loss function, compute and return it.

the model interface will iterate over the dataset, and execute each batch on ``forward``, ``loss``, and ``backward``.


```python
# adapted from https://github.com/pytorch/examples/blob/master/mnist/main.py
from cogitare import model
from cogitare import utils
from cogitare.data import dataset, asyncdataloader
from cogitare.plugins import earlystopping
from cogitare.metrics.classification import accuracy
import cogitare

import torch.nn as nn
import torch
import torch.nn.functional as f
from torch.nn.utils import clip_grad_norm
import torch.optim as optim

from sklearn.datasets import fetch_mldata

import numpy as np

cuda = true


cogitare.utils.set_cuda(cuda)
```


```python
class cnn(model):
    
    def __init__(self):
        super(cnn, self).__init__()
        
        # define the model
        self.conv1 = nn.conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.dropout2d()
        self.fc1 = nn.linear(320, 50)
        self.fc2 = nn.linear(50, 10)
    
    def forward(self, batch):
        # in this sample, each batch will be a tuple containing (input_batch, expected_batch)
        # in forward in are only interested in input so that we can ignore the second item of the tuple
        input, _ = batch
        
        # batch x flat tensor -> batch x 1 channel (gray) x width x heigth
        input = input.view(32, 1, 28, 28)
        
        # pass the data in the net
        x = f.relu(f.max_pool2d(self.conv1(input), 2))
        x = f.relu(f.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = f.relu(self.fc1(x))
        x = f.dropout(x, training=self.training)
        x = self.fc2(x)

        # return the model output
        return f.log_softmax(x, dim=1)
    
    def loss(self, output, batch):
        # in this sample, each batch will be a tuple containing (input_batch, expected_batch)
        # in loss in are only interested in expected so that we can ignore the first item of the tuple
        _, expected = batch
        
        return f.nll_loss(output, expected)
```

the model class is simple; it only requires de forward and loss methods. by default, cogitare will backward the loss returned by the ``loss()`` method, and optimize the model parameters. if you want to disable the cogitare backward and optimization steps, just return ``none`` in the loss function. if you return none, you are responsible by backwarding and optimizing the parameters.

### 3.2 data loading
in this step, we will load the data from sklearn package.


```python
mnist = fetch_mldata('mnist original')
mnist.data = (mnist.data / 255).astype(np.float32)
```

cogitare provides a toolbox to load and pre-process data for your models. in this introduction, we will use the ``dataset`` and the ``asyncdataloader`` as examples.

the ``dataset`` is responsible by iterating over multiples data iterators (in our case, we'll have two data iterators: input samples, expected samples).


```python
# as input, the dataset is expected a list of iterators. in our case, the first iterator is the input 
# data and the second iterator is the target data

# also, we set the batch size to 32 and enable the shuffling

# drop the last batch if its size is different of 32
data = dataset([mnist.data, mnist.target.astype(int)], batch_size=32, shuffle=true, drop_last=true)

# then, we split our dataset into a train and into a validation sets, by a ratio of 0.8
data_train, data_validation = data.split(0.8)
```

notice that cogitare accepts any iterator as input. instead of using our dataset, you can use the mnist.data itself, pytorch's data loaders, or any other input that acts as an iterator.

in some cases, we can increase the model performance by loading the data using multiples threads/processes or by pre-loading the data before being requested by the model.

with the ``asyncdataloader``, we can load n batches ahead of the model execution in parallel. we present this technique in this sample because it can increase performance in a wide range of models (when the data loading or pre-processing is slower than the model execution).


```python
def pre_process(batch):
    input, expected = batch
    
    # the data is a numpy.ndarray (loaded from sklearn), so we need to convert it to variable
    input = utils.to_variable(input, dtype=torch.floattensor)  # converts to a torch variable of longtensor
    expected = utils.to_variable(expected, dtype=torch.longtensor)  # converts to a torch variable of longtensor
    return input, expected


# we wrap our data_train and data_validation iterators over the async data loader.
# each loader will load 16 batches ahead of the model execution using 8 workers (8 threads, in this case).
# for each batch, it will be pre-processed in parallel with the preprocess function, that will load the data
# on gpu
data_train = asyncdataloader(data_train, buffer_size=16, mode='threaded', workers=8, on_batch_loaded=pre_process)
data_validation = asyncdataloader(data_validation, buffer_size=16, mode='threaded', workers=8, on_batch_loaded=pre_process)
```

to cache the async buffer before training, we can:


```python
data_train.cache()
data_validation.cache()
```

## 3.3 training

now, we can train our model.

first, lets create the model instance and add the default plugins to watch the training status.
the default plugin includes:

- progress bar per batch and epoch
- plot training and validation losses (if validation_dataset is present)
- log training loss


```python
model = cnn()
model.register_default_plugins()
```

besides that, we may want to add some extra plugins, such as the earlystopping. so, if the model is not decreasing the loss after n epochs, the training stops and the best model is used.

to add the early stopping algorithm, you can use:


```python
early = earlystopping(max_tries=10, path='/tmp/model.pt')
# after 10 epochs without decreasing the loss, stop the training and the best model is saved at /tmp/model.pt

# the plugin will execute in the end of each epoch
model.register_plugin(early, 'on_end_epoch')
```

also, a common technique is to clip the gradient during training. if you want to clip the grad, you can use:


```python
model.register_plugin(lambda *args, **kw: clip_grad_norm(model.parameters(), 1.0), 'before_step')
# will execute the clip_grad_norm before each optimization step
```

now, we define the optimizator, and then start the model training:


```python
optimizer = optim.adam(model.parameters(), lr=0.001)

if cuda:
    model = model.cuda()
model.learn(data_train, optimizer, data_validation, max_epochs=100)
```

    2018-02-02 20:59:23 sprawl cogitare.core.model[2443] info model: 
    
    cnn(
      (conv1): conv2d (1, 10, kernel_size=(5, 5), stride=(1, 1))
      (conv2): conv2d (10, 20, kernel_size=(5, 5), stride=(1, 1))
      (conv2_drop): dropout2d(p=0.5)
      (fc1): linear(in_features=320, out_features=50)
      (fc2): linear(in_features=50, out_features=10)
    )
    
    2018-02-02 20:59:23 sprawl cogitare.core.model[2443] info training data: 
    
    dataset with:
        containers: [
            tensorholder with 1750x32 samples
    	tensorholder with 1750x32 samples
        ],
        batch size: 32
    
    
    2018-02-02 20:59:23 sprawl cogitare.core.model[2443] info number of trainable parameters: 21,840
    2018-02-02 20:59:23 sprawl cogitare.core.model[2443] info number of non-trainable parameters: 0
    2018-02-02 20:59:23 sprawl cogitare.core.model[2443] info total number of parameters: 21,840
    2018-02-02 20:59:23 sprawl cogitare.core.model[2443] info starting the training ...
    2018-02-02 21:02:04 sprawl cogitare.core.model[2443] info training finished
    
    stopping training after 10 tries. best score 0.0909
    model restored from: /tmp/model.pt

![](http://docs.cogitare-ai.org/_images/quickstart_23_3.png)


to check the model loss and accuracy on the validation dataset:


```python
def model_accuracy(output, data):
    _, indices = torch.max(output, 1)
    
    return accuracy(indices, data[1])

# evaluate the model loss and accuracy over the validation dataset
metrics = model.evaluate_with_metrics(data_validation, {'loss': model.metric_loss, 'accuracy': model_accuracy})

# the metrics is an dict mapping the metric name (loss or accuracy, in this sample) to a list of the accuracy output
# we have a measurement per batch. so, to have a value of the full dataset, we take the mean value:

metrics_mean = {'loss': 0, 'accuracy': 0}
for loss, acc in zip(metrics['loss'], metrics['accuracy']):
    metrics_mean['loss'] += loss
    metrics_mean['accuracy'] += acc.item()

qtd = len(metrics['loss'])

print('loss: {}'.format(metrics_mean['loss'] / qtd))
print('accuracy: {}'.format(metrics_mean['accuracy'] / qtd))
```

    loss: 0.10143917564566948
    accuracy: 0.9846252860411899


one of the advantages of cogitare is the plug-and-play apis, which let you add/remove functionalities easily. with this sample, we trained a model with training progress bar, error plotting, early stopping, grad clipping, and model evaluation easily.

# 4. contribution

cogitare is a work in progress project, and any contribution is welcome.

you can contribute testing and providing bug reports, proposing feature ideas,
fixing bugs, pushing code, etcs.

1. you want to propose a new feature and implement it
	- post about your intended feature, and we shall discuss the design and implementation. once we agree that the plan looks good, go ahead and implement it.
2. you want to implement a feature or bug-fix for an outstanding issue
    - look at the outstanding issues here: https://github.com/cogitare-ai/cogitare/issues
    - pick an issue and comment on the task that you want to work on this feature
    - if you need more context on a particular issue, please ask and we shall provide.


once you finish implementing a feature or bugfix, please send a pull request to
https://github.com/cogitare-ai/cogitare

if you are not familiar with creating a pull request, here are some guides:
- http://stackoverflow.com/questions/14680711/how-to-do-a-github-pull-request
- https://help.github.com/articles/creating-a-pull-request/
"
"mlens","<div align=""center"">
<img src=""docs/source/_static/img/logo.png"" width=""50%""><br><br>
</div>
<hr>

![python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)
![python 2.7](https://img.shields.io/badge/python-2.7-blue.svg)
[![pypi version](https://badge.fury.io/py/mlens.svg)](http://badge.fury.io/py/mlens)
[![build status](https://travis-ci.org/flennerhag/mlens.svg?branch=master)](https://travis-ci.org/flennerhag/mlens)
[![build status](https://ci.appveyor.com/api/projects/status/99g65kvraic8w2la/branch/master?svg=true)](https://ci.appveyor.com/project/flennerhag/mlens/branch/master)
[![code health](https://landscape.io/github/flennerhag/mlens/master/landscape.svg?style=flat)](https://landscape.io/github/flennerhag/mlens/master)
[![coverage status](https://coveralls.io/repos/github/flennerhag/mlens/badge.svg?branch=master)](https://coveralls.io/github/flennerhag/mlens?branch=master)
![license](https://img.shields.io/badge/license-mit-blue.svg)
[![doi](https://zenodo.org/badge/78573084.svg)](https://zenodo.org/badge/latestdoi/78573084)

**a python library for high performance ensemble learning**

ml-ensemble combines a scikit-learn high-level api with a low-level
computational graph framework to build memory efficient, 
maximally parallelized ensemble networks in as few lines of codes as possible.

ml-ensemble is thread safe as long as base learners are and can fall back on
memory mapped multiprocessing for memory-neutral process-based concurrency.
for tutorials and full documentation, visit the project
 [website](http://ml-ensemble.com/).

## ensembles as computational graphs

an ensemble is built on top of a computational graph,
allowing users great design freedom. ensembles can be built with recursion,
dynamic evaluation (e.g. ``if-else``) and much more. a high-level api wraps common
ensemble architectures into scikit-learn estimators.

<div align=""center"">
<img src=""docs/source/_static/img/network.png"" width=""60%""><br><br>
</div>

*example computational graph of a layer in an ensemble*

### memory efficient parallelized learning

ml-ensemble is optimized for speed and minimal memory consumption. no
serialization of data takes place, regardless of whether multithreading or
multiprocessing is used. additionally, multithreading is pickle-free. 

### ease of use

ready-made ensembles are built by adding layers to an instance. no matter how
complex the ensemble, to train it call the ``fit`` method:

```python
ensemble = subsemble()

# first layer
ensemble.add(list_of_estimators)

# second layer
ensemble.add(list_of_estimators)

# final meta estimator
ensemble.add_meta(estimator)

# train ensemble
ensemble.fit(x, y)
```

similarly, it's straightforward to modify an existing ensemble:

```python
# remove layer
ensemble.remove(2)

# change layer parameters
ensemble.replace(0, new_list_of_estimators)
```

and to create differentiated preprocessing pipelines for different subsets
of estimators within a given layer, simple pass a mapping to the ``add``
method:

```python

preprocessing = {'pipeline-1': list_of_transformers_1,
                 'pipeline-2': list_of_transformers_2}

estimators = {'pipeline-1': list_of_estimators_1,
              'pipeline-2': list_of_estimators_2}

ensemble.add(estimators, preprocessing)
```

### dedicated diagnostics

ml ensemble implements a dedicated diagnostics and model selection suite
for intuitive and speedy ensemble evaluation. the ``evaluator``
allows you to evaluate several preprocessing pipelines and several
estimators in one go, giving you birds-eye view of how different candidates
for the ensemble perform.

moreover, entire *ensembles* can be used as a preprocessing pipeline, to
leverage model selection for higher-level layers. simply set ``model_selection``
to ``true`` in the ensemble (don't forget to turn it off when done).

```python

preprocessing_dict = {'pipeline-1': list_of_transformers_1,
                      'pipeline-2': list_of_transformers_2}


evaluator = evaluator(scorer=score_func)
evaluator.fit(x, y, list_of_estimators, param_dists_dict, preprocessing_dict)
```

all ensembles and model selection instances provides summary statistics in
tabular format. you can find fit and prediction times in any ensemble by calling
``data``, and cv-scores if you passed a scorer. for the model selection suite,
the ``results`` attribute gives you the outcome of an evaluation::

```python
              fit_time_mean  fit_time_std  train_score_mean  train_score_std  test_score_mean  test_score_std               params
prep-1 est-1       0.001353      0.001316          0.957037         0.005543         0.960000        0.032660                   {}
       est-2       0.000447      0.000012          0.980000         0.004743         0.966667        0.033333  {'n_neighbors': 15}
prep-2 est-1       0.001000      0.000603          0.957037         0.005543         0.960000        0.032660                   {}
       est-2       0.000448      0.000036          0.965185         0.003395         0.960000        0.044222   {'n_neighbors': 8}
prep-3 est-1       0.000735      0.000248          0.791111         0.019821         0.780000        0.133500                   {}
       est-2       0.000462      0.000143          0.837037         0.014815         0.800000        0.126491   {'n_neighbors': 9}
```

## install

#### pypi

ml-ensemble is available on pypi. install with

```bash
pip install mlens
```

#### bleeding edge

fork the github repository:

```bash
git clone https://github.com/flennerhag/mlens.git; cd mlens;
python setup.py install
```

## citation

for scientific publication, ml-ensemble can be cited as 

```
@misc{flennerhag:2017mlens,
  author = {flennerhag, sebastian},
  title  = {ml-ensemble},
  month  = nov,
  year   = 2017,
  doi    = {10.5281/zenodo.1042144},
  url    = {https://dx.doi.org/10.5281/zenodo.1042144}
}
```

## questions

please see [issue tracker](https://github.com/flennerhag/mlens/issues).


## contribute

ml-ensemble is an open-source project that welcome any contribution, small as large.
bug fixes and minor improvements can be pulled as is; larger prs need to be unit tested. 
we generally follow the [pep-8](https://www.python.org/dev/peps/pep-0008/) style guide.

## license

mit license

copyright (c) 2017‚Äì2020 sebastian flennerhag

permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""software""), to deal
in the software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the software, and to permit persons to whom the software is
furnished to do so, subject to the following conditions:

the above copyright notice and this permission notice shall be included in all
copies or substantial portions of the software.

the software is provided ""as is"", without warranty of any kind, express or
implied, including but not limited to the warranties of merchantability,
fitness for a particular purpose and noninfringement. in no event shall the
authors or copyright holders be liable for any claim, damages or other
liability, whether in an action of contract, tort or otherwise, arising from,
out of or in connection with the software or the use or other dealings in the
software.
"
"Microsoft Recommenders","# recommenders

[![documentation status](https://readthedocs.org/projects/microsoft-recommenders/badge/?version=latest)](https://microsoft-recommenders.readthedocs.io/en/latest/?badge=latest)

## what's new (july, 2022)

we have a new release [recommenders 1.1.1](https://github.com/microsoft/recommenders/releases/tag/1.1.1)! 

we have introduced a new way of testing our repository using [azureml](https://azure.microsoft.com/en-us/services/machine-learning/). with azureml we are able to distribute our tests to different machines and run them in parallel. this allows us to test our repository on a wider range of machines and provides us with a much faster test cycle. our total computation time went from around 9h to 35min, and we were able to reduce the costs by half. see more details [here](tests/readme.md).

we also made other improvements like faster evaluation metrics and improving sar algorithm. 

starting with release 0.6.0, recommenders has been available on pypi and can be installed using pip! 

here you can find the pypi page: https://pypi.org/project/recommenders/

here you can find the package documentation: https://microsoft-recommenders.readthedocs.io/en/latest/

## introduction

this repository contains examples and best practices for building recommendation systems, provided as jupyter notebooks. the examples detail our learnings on five key tasks:

- [prepare data](examples/01_prepare_data): preparing and loading data for each recommender algorithm
- [model](examples/00_quick_start): building models using various classical and deep learning recommender algorithms such as alternating least squares ([als](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#als)) or extreme deep factorization machines ([xdeepfm](https://arxiv.org/abs/1803.05170)).
- [evaluate](examples/03_evaluate): evaluating algorithms with offline metrics
- [model select and optimize](examples/04_model_select_and_optimize): tuning and optimizing hyperparameters for recommender models
- [operationalize](examples/05_operationalize): operationalizing models in a production environment on azure

several utilities are provided in [recommenders](recommenders) to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications. see the [recommenders documentation](https://readthedocs.org/projects/microsoft-recommenders/).

for a more detailed overview of the repository, please see the documents on the [wiki page](https://github.com/microsoft/recommenders/wiki/documents-and-presentations).

## getting started

please see the [setup guide](setup.md) for more details on setting up your machine locally, on a [data science virtual machine (dsvm)](https://azure.microsoft.com/en-gb/services/virtual-machines/data-science-virtual-machines/) or on [azure databricks](setup.md#setup-guide-for-azure-databricks).

the installation of the recommenders package has been tested with 
- python versions 3.6 - 3.9 and [venv](https://docs.python.org/3/library/venv.html), [virtualenv](https://virtualenv.pypa.io/en/latest/index.html#) or [conda](https://docs.conda.io/projects/conda/en/latest/glossary.html?highlight=environment#conda-environment)

and currently does not support version 3.10 and above. it is recommended to install the package and its dependencies inside a clean environment (such as [conda](https://docs.conda.io/projects/conda/en/latest/glossary.html?highlight=environment#conda-environment), [venv](https://docs.python.org/3/library/venv.html) or [virtualenv](https://virtualenv.pypa.io/en/latest/index.html#)).

to set up on your local machine:

* to install core utilities, cpu-based algorithms, and dependencies:

    1. ensure software required for compilation and python libraries
       is installed.

       + on linux this can be supported by adding:

         ```bash
         sudo apt-get install -y build-essential libpython<version>
         ``` 

         where `<version>` should be the python version (e.g. `3.6`).

       + on windows you will need [microsoft c++ build tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/).

    2. create a conda or virtual environment.  see the
       [setup guide](setup.md) for more details.

    3. within the created environment, install the package from
       [pypi](https://pypi.org):

       ```bash
       pip install --upgrade pip
       pip install --upgrade setuptools
       pip install recommenders[examples]
       ```

    4. register your (conda or virtual) environment with jupyter:

       ```bash
       python -m ipykernel install --user --name my_environment_name --display-name ""python (reco)""
       ```

    5. start the jupyter notebook server

       ```bash
       jupyter notebook
       ```

    6. run the [sar python cpu movielens](examples/00_quick_start/sar_movielens.ipynb)
       notebook under the `00_quick_start` folder.  make sure to
       change the kernel to ""python (reco)"".

* for additional options to install the package (support for gpu,
  spark etc.) see [this guide](recommenders/readme.md).

**note** - the [alternating least squares (als)](examples/00_quick_start/als_movielens.ipynb) notebooks require a pyspark environment to run. please follow the steps in the [setup guide](setup.md#dependencies-setup) to run these notebooks in a pyspark environment. for the deep learning algorithms, it is recommended to use a gpu machine and to follow the steps in the [setup guide](setup.md#dependencies-setup) to set up nvidia libraries.

**note for dsvm users** - please follow the steps in the [dependencies setup - set pyspark environment variables on linux or macos](setup.md#dependencies-setup) and [troubleshooting for the dsvm](setup.md#troubleshooting-for-the-dsvm) sections if you encounter any issue.

**docker** - another easy way to try the recommenders repository and get started quickly is to build [docker images](tools/docker/readme.md) suitable for different environments. 

## algorithms

the table below lists the recommender algorithms currently available in the repository. notebooks are linked under the example column as quick start, showcasing an easy to run example of the algorithm, or as deep dive, explaining in detail the math and implementation of the algorithm.

| algorithm | type | description | example |
|-----------|------|-------------|---------|
| alternating least squares (als) | collaborative filtering | matrix factorization algorithm for explicit or implicit feedback in large datasets, optimized for scalability and distributed computing capability. it works in the pyspark environment. | [quick start](examples/00_quick_start/als_movielens.ipynb) / [deep dive](examples/02_model_collaborative_filtering/als_deep_dive.ipynb) |
| attentive asynchronous singular value decomposition (a2svd)<sup>*</sup> | collaborative filtering | sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |
| cornac/bayesian personalized ranking (bpr) | collaborative filtering | matrix factorization algorithm for predicting item ranking with implicit feedback. it works in the cpu environment. | [deep dive](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) |
| cornac/bilateral variational autoencoder (bivae) | collaborative filtering | generative model for dyadic data (e.g., user-item interactions). it works in the cpu/gpu environment. | [deep dive](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) |
| convolutional sequence embedding recommendation (caser) | collaborative filtering | algorithm based on convolutions that aim to capture both user‚Äôs general preferences and sequential patterns. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |
| deep knowledge-aware network (dkn)<sup>*</sup> | content-based filtering | deep learning algorithm incorporating a knowledge graph and article embeddings for providing news or article recommendations. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/dkn_mind.ipynb) / [deep dive](examples/02_model_content_based_filtering/dkn_deep_dive.ipynb) |
| extreme deep factorization machine (xdeepfm)<sup>*</sup> | hybrid | deep learning based algorithm for implicit and explicit feedback with user/item features. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/xdeepfm_criteo.ipynb) |
| fastai embedding dot bias (fast) | collaborative filtering | general purpose algorithm with embeddings and biases for users and items. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/fastai_movielens.ipynb) |
| lightfm/hybrid matrix factorization | hybrid | hybrid matrix factorization algorithm for both implicit and explicit feedbacks. it works in the cpu environment. | [quick start](examples/02_model_hybrid/lightfm_deep_dive.ipynb) |
| lightgbm/gradient boosting tree<sup>*</sup> | content-based filtering | gradient boosting tree algorithm for fast training and low memory usage in content-based problems. it works in the cpu/gpu/pyspark environments. | [quick start in cpu](examples/00_quick_start/lightgbm_tinycriteo.ipynb) / [deep dive in pyspark](examples/02_model_content_based_filtering/mmlspark_lightgbm_criteo.ipynb) |
| lightgcn | collaborative filtering | deep learning algorithm which simplifies the design of gcn for predicting implicit feedback. it works in the cpu/gpu environment. | [deep dive](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) |
| geoimc<sup>*</sup> | hybrid | matrix completion algorithm that has into account user and item features using riemannian conjugate gradients optimization and following a geometric approach. it works in the cpu environment. | [quick start](examples/00_quick_start/geoimc_movielens.ipynb) |
| gru4rec | collaborative filtering | sequential-based algorithm that aims to capture both long and short-term user preferences using recurrent neural networks. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |
| multinomial vae | collaborative filtering | generative model for predicting user/item interactions. it works in the cpu/gpu environment. | [deep dive](examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb) |
| neural recommendation with long- and short-term user representations (lstur)<sup>*</sup> | content-based filtering | neural recommendation algorithm for recommending news articles with long- and short-term user interest modeling. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/lstur_mind.ipynb) |
| neural recommendation with attentive multi-view learning (naml)<sup>*</sup> | content-based filtering | neural recommendation algorithm for recommending news articles with attentive multi-view learning. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/naml_mind.ipynb) |
| neural collaborative filtering (ncf) | collaborative filtering | deep learning algorithm with enhanced performance for user/item implicit feedback. it works in the cpu/gpu environment.| [quick start](examples/00_quick_start/ncf_movielens.ipynb) / [deep dive](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) |
| neural recommendation with personalized attention (npa)<sup>*</sup> | content-based filtering | neural recommendation algorithm for recommending news articles with personalized attention network. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/npa_mind.ipynb) |
| neural recommendation with multi-head self-attention (nrms)<sup>*</sup> | content-based filtering | neural recommendation algorithm for recommending news articles with multi-head self-attention. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/nrms_mind.ipynb) |
| next item recommendation (nextitnet) | collaborative filtering | algorithm based on dilated convolutions and residual network that aims to capture sequential patterns. it considers both user/item interactions and features.  it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |
| restricted boltzmann machines (rbm) | collaborative filtering | neural network based algorithm for learning the underlying probability distribution for explicit or implicit user/item feedback. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/rbm_movielens.ipynb) / [deep dive](examples/02_model_collaborative_filtering/rbm_deep_dive.ipynb) |
| riemannian low-rank matrix completion (rlrmc)<sup>*</sup> | collaborative filtering | matrix factorization algorithm using riemannian conjugate gradients optimization with small memory consumption to predict user/item interactions. it works in the cpu environment. | [quick start](examples/00_quick_start/rlrmc_movielens.ipynb) |
| simple algorithm for recommendation (sar)<sup>*</sup> | collaborative filtering | similarity-based algorithm for implicit user/item feedback.  it works in the cpu environment. | [quick start](examples/00_quick_start/sar_movielens.ipynb) / [deep dive](examples/02_model_collaborative_filtering/sar_deep_dive.ipynb) |
| self-attentive sequential recommendation (sasrec) | collaborative filtering | transformer based algorithm for sequential recommendation. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/sasrec_amazon.ipynb) |
| short-term and long-term preference integrated recommender (sli-rec)<sup>*</sup> | collaborative filtering | sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism, a time-aware controller and a content-aware controller. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |
| multi-interest-aware sequential user modeling (sum)<sup>*</sup> | collaborative filtering | an enhanced memory network-based sequential user model which aims to capture users' multiple interests. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |
| sequential recommendation via personalized transformer (ssept) | collaborative filtering | transformer based algorithm for sequential recommendation with user embedding. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/sasrec_amazon.ipynb) |
| standard vae | collaborative filtering | generative model for predicting user/item interactions.  it works in the cpu/gpu environment. | [deep dive](examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) |
| surprise/singular value decomposition (svd) | collaborative filtering | matrix factorization algorithm for predicting explicit rating feedback in small datasets. it works in the cpu/gpu environment. | [deep dive](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) |
| term frequency - inverse document frequency (tf-idf) | content-based filtering | simple similarity-based algorithm for content-based recommendations with text datasets. it works in the cpu environment. | [quick  start](examples/00_quick_start/tfidf_covid.ipynb) |
| vowpal wabbit (vw)<sup>*</sup> | content-based filtering | fast online learning algorithms, great for scenarios where user features / context are constantly changing. it uses the cpu for online learning. | [deep dive](examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb) |
| wide and deep | hybrid | deep learning algorithm that can memorize feature interactions and generalize user features. it works in the cpu/gpu environment. | [quick start](examples/00_quick_start/wide_deep_movielens.ipynb) |
| xlearn/factorization machine (fm) & field-aware fm (ffm) | hybrid | quick and memory efficient algorithm to predict labels with user/item features. it works in the cpu/gpu environment. | [deep dive](examples/02_model_hybrid/fm_deep_dive.ipynb) |

**note**: <sup>*</sup> indicates algorithms invented/contributed by microsoft.

independent or incubating algorithms and utilities are candidates for the [contrib](contrib) folder. this will house contributions which may not easily fit into the core repository or need time to refactor or mature the code and add necessary tests.

| algorithm | type | description | example |
|-----------|------|-------------|---------|
| sarplus <sup>*</sup> | collaborative filtering | optimized implementation of sar for spark |  [quick start](contrib/sarplus/readme.md) |

### algorithm comparison

we provide a [benchmark notebook](examples/06_benchmarks/movielens.ipynb) to illustrate how different algorithms could be evaluated and compared. in this notebook, the movielens dataset is split into training/test sets at a 75/25 ratio using a stratified split. a recommendation model is trained using each of the collaborative filtering algorithms below. we utilize empirical parameter values reported in literature [here](http://mymedialite.net/examples/datasets.html). for ranking metrics we use `k=10` (top 10 recommended items). we run the comparison on a standard nc6s_v2 [azure dsvm](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/) (6 vcpus, 112 gb memory and 1 p100 gpu). spark als is run in local standalone mode. in this table we show the results on movielens 100k, running the algorithms for 15 epochs.

| algo | map | ndcg@k | precision@k | recall@k | rmse | mae | r<sup>2</sup> | explained variance |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| [als](examples/00_quick_start/als_movielens.ipynb) | 0.004732 |	0.044239 |	0.048462 |	0.017796 | 0.965038 |	0.753001 |	0.255647 |	0.251648 |
| [bivae](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) | 0.146126	| 0.475077 |	0.411771 |	0.219145 | n/a |	n/a |	n/a |	n/a |
| [bpr](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) | 0.132478	| 0.441997 |	0.388229 |	0.212522 | n/a |	n/a |	n/a |	n/a |
| [fastai](examples/00_quick_start/fastai_movielens.ipynb) | 0.025503 |	0.147866 |	0.130329 |	0.053824 | 0.943084 |	0.744337 |	0.285308 |	0.287671 |
| [lightgcn](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) | 0.088526 | 0.419846 | 0.379626 | 0.144336 | n/a | n/a | n/a | n/a |
| [ncf](examples/02_model_hybrid/ncf_deep_dive.ipynb) | 0.107720	| 0.396118 |	0.347296 |	0.180775 | n/a | n/a | n/a | n/a |
| [sar](examples/00_quick_start/sar_movielens.ipynb) | 0.110591 |	0.382461 | 	0.330753 | 0.176385 | 1.253805 | 1.048484 |	-0.569363 |	0.030474 |
| [svd](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) | 0.012873	| 0.095930 |	0.091198 |	0.032783 | 0.938681 | 0.742690 | 0.291967 | 0.291971 |

## code of conduct

this project adheres to [microsoft's open source code of conduct](code_of_conduct.md) in order to foster a welcoming and inspiring community for all.

## contributing

this project welcomes contributions and suggestions. before contributing, please see our [contribution guidelines](contributing.md).

## build status

these tests are the nightly builds, which compute the smoke and integration tests. `main` is our principal branch and `staging` is our development branch. we use [pytest](https://docs.pytest.org/) for testing python utilities in [recommenders](recommenders) and [papermill](https://github.com/nteract/papermill) and [scrapbook](https://nteract-scrapbook.readthedocs.io/en/latest/) for the [notebooks](examples). 

for more information about the testing pipelines, please see the [test documentation](tests/readme.md).

### azureml nightly build status

smoke and integration tests are run daily on azureml.

| build type | branch | status |  | branch | status |
| --- | --- | --- | --- | --- | --- |
| **linux cpu** | main | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3amain) | | staging | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3astaging) |
| **linux gpu** | main | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3amain) | | staging | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3astaging) |
| **linux spark** | main | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3amain) | | staging | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3astaging) |

## related projects

- [microsoft ai github](https://github.com/microsoft/ai): find other best practice projects, and azure ai design patterns in our central repository.
- [nlp best practices](https://github.com/microsoft/nlp-recipes): best practices and examples on nlp.
- [computer vision best practices](https://github.com/microsoft/computervision-recipes): best practices and examples on computer vision.
- [forecasting best practices](https://github.com/microsoft/forecasting): best practices and examples on time series forecasting.

## references

- d. li, j. lian, l. zhang, k. ren, d. lu, t. wu, x. xie, ""recommender systems: frontiers and practices"" (in chinese), publishing house of electronics industry, beijing 2022.
- a. argyriou, m. gonz√°lez-fierro, and l. zhang, ""microsoft recommenders: best practices for production-ready recommendation systems"", *www 2020: international world wide web conference taipei*, 2020. available online: https://dl.acm.org/doi/abs/10.1145/3366424.3382692
- l. zhang, t. wu, x. xie, a. argyriou, m. gonz√°lez-fierro and j. lian, ""building production-ready recommendation system at scale"", *acm sigkdd conference on knowledge discovery and data mining 2019 (kdd 2019)*, 2019.
- s. graham,  j.k. min, t. wu, ""microsoft recommenders: tools to accelerate developing recommender systems"", *recsys '19: proceedings of the 13th acm conference on recommender systems*, 2019. available online: https://dl.acm.org/doi/10.1145/3298689.3346967
"
"StellarGraph","![stellargraph machine learning library logo](https://raw.githubusercontent.com/stellargraph/stellargraph/develop/stellar-graph-banner.png)

<p align=""center"">
  <a href=""https://stellargraph.readthedocs.io/"" alt=""docs""><img src=""https://readthedocs.org/projects/stellargraph/badge/?version=latest""/></a>
  <a href=""https://community.stellargraph.io"" alt=""discourse forum""><img src=""https://img.shields.io/badge/help_forum-discourse-blue.svg""/></a>
  <a href=""https://pypi.org/project/stellargraph/"" alt=""pypi""><img src=""https://img.shields.io/pypi/v/stellargraph.svg""/></a>
  <a href=""https://github.com/stellargraph/stellargraph/blob/develop/license"" alt=""license""><img src=""https://img.shields.io/github/license/stellargraph/stellargraph.svg""/></a>
</p>
<p align=""center"">
  <a href=""https://github.com/stellargraph/stellargraph/blob/develop/contributing.md"" alt=""contributions welcome""><img src=""https://img.shields.io/badge/contributions-welcome-brightgreen.svg""/></a>
  <a href=""https://github.com/stellargraph/stellargraph/actions?query=branch%3amaster"" alt=""build status: master""><img src=""https://github.com/stellargraph/stellargraph/workflows/ci/badge.svg?branch=master""/></a>
  <a href=""https://github.com/stellargraph/stellargraph/actions?query=branch%3adevelop"" alt=""build status: develop""><img src=""https://github.com/stellargraph/stellargraph/workflows/ci/badge.svg?branch=develop""/></a>
  <a href=""https://codecov.io/gh/stellargraph/stellargraph""><img src=""https://codecov.io/gh/stellargraph/stellargraph/branch/develop/graph/badge.svg"" /></a>
  <a href=""https://pypi.org/project/stellargraph"" alt=""pypi downloads""><img alt=""pypi downloads"" src=""https://pepy.tech/badge/stellargraph""></a>
</p>


# stellargraph machine learning library

**stellargraph** is a python library for machine learning on [graphs and networks](https://en.wikipedia.org/wiki/graph_%28discrete_mathematics%29).

## table of contents
   * [introduction](#introduction)
   * [getting started](#getting-started)
   * [getting help](#getting-help)
   * [example: gcn](#example-gcn)
   * [algorithms](#algorithms)
   * [installation](#installation)
       * [install stellargraph using pypi](#install-stellargraph-using-pypi)
       * [install stellargraph in anaconda python](#install-stellargraph-in-anaconda-python)
       * [install stellargraph from github source](#install-stellargraph-from-github-source)
   * [citing](#citing)
   * [references](#references)

## introduction

the stellargraph library offers state-of-the-art algorithms for [graph machine learning](https://medium.com/stellargraph/knowing-your-neighbours-machine-learning-on-graphs-9b7c3d0d5896), making it easy to discover patterns and answer questions about graph-structured data. it can solve many machine learning tasks:

- representation learning for nodes and edges, to be used for visualisation and various downstream machine learning tasks;
- [classification and attribute inference of nodes](https://medium.com/stellargraph/can-graph-machine-learning-identify-hate-speech-in-online-social-networks-58e3b80c9f7e) or edges;
- classification of whole graphs;
- link prediction;
- [interpretation of node classification](https://medium.com/stellargraph/https-medium-com-stellargraph-saliency-maps-for-graph-machine-learning-5cca536974da) [8].

graph-structured data represent entities as nodes (or vertices) and relationships between them as edges (or links), and can include data associated with either as attributes. for example, a graph can contain people as nodes and friendships between them as links, with data like a person's age and the date a friendship was established. stellargraph supports analysis of many kinds of graphs:

- homogeneous (with nodes and links of one type),
- heterogeneous (with more than one type of nodes and/or links)
- knowledge graphs (extreme heterogeneous graphs with thousands of types of edges)
- graphs with or without data associated with nodes
- graphs with edge weights

stellargraph is built on [tensorflow 2](https://tensorflow.org/) and its [keras high-level api](https://www.tensorflow.org/guide/keras), as well as [pandas](https://pandas.pydata.org) and [numpy](https://www.numpy.org). it is thus user-friendly, modular and extensible. it interoperates smoothly with code that builds on these, such as the standard keras layers and [scikit-learn](http://scikit-learn.github.io/stable), so it is easy to augment the core graph machine learning algorithms provided by stellargraph. it is thus also [easy to install with `pip` or anaconda](#installation).

## getting started

[the numerous detailed and narrated examples][demos] are a good way to get started with stellargraph. there is likely to be one that is similar to your data or your problem (if not, [let us know](#getting-help)).

[demos]: https://stellargraph.readthedocs.io/en/stable/demos/index.html

you can start working with the examples immediately in google colab or binder by clicking the ![](https://colab.research.google.com/assets/colab-badge.svg) and ![](https://mybinder.org/badge_logo.svg) badges within each jupyter notebook.

alternatively, you can run download a local copy of the demos and run them using `jupyter`. the demos can be downloaded by cloning the `master` branch of this repository, or by using the `curl` command below:
```bash
curl -l https://github.com/stellargraph/stellargraph/archive/master.zip | tar -xz --strip=1 stellargraph-master/demos
```

the dependencies required to run most of our demo notebooks locally can be installed using one of the following:

- using `pip`: `pip install stellargraph[demos]`
- using `conda`: `conda install -c stellargraph stellargraph`

(see [installation](#installation) section for more details and more options.)

## getting help

if you get stuck or have a problem, there's many ways to make progress and get help or support:

- [read the documentation](https://stellargraph.readthedocs.io)
- [consult the examples][demos]
- contact us:
  - [ask questions and discuss problems on the stellargraph discourse forum](https://community.stellargraph.io)
  - [file an issue](https://github.com/stellargraph/stellargraph/issues/new/choose)
  - send us an email at [stellar.admin@csiro.au](mailto:stellar.admin@csiro.au?subject=question%20about%20the%20stellargraph%20library)


## example: gcn

one of the earliest deep machine learning algorithms for graphs is a graph convolution network (gcn) [6]. the following example uses it for node classification: predicting the class from which a node comes. it shows how easy it is to apply using stellargraph, and shows how stellargraph integrates smoothly with pandas and tensorflow and libraries built on them.

#### data preparation

data for stellargraph can be prepared using common libraries like pandas and scikit-learn.

``` python
import pandas as pd
from sklearn import model_selection

def load_my_data():
    # your own code to load data into pandas dataframes, e.g. from csv files or a database
    ...

nodes, edges, targets = load_my_data()

# use scikit-learn to compute training and test sets
train_targets, test_targets = model_selection.train_test_split(targets, train_size=0.5)
```

#### graph machine learning model

this is the only part that is specific to stellargraph. the machine learning model consists of some graph convolution layers followed by a layer to compute the actual predictions as a tensorflow tensor. stellargraph makes it easy to construct all of these layers via the `gcn` model class. it also makes it easy to get input data in the right format via the `stellargraph` graph data type and a data generator.

```python
import stellargraph as sg
import tensorflow as tf

# convert the raw data into stellargraph's graph format for faster operations
graph = sg.stellargraph(nodes, edges)

generator = sg.mapper.fullbatchnodegenerator(graph, method=""gcn"")

# two layers of gcn, each with hidden dimension 16
gcn = sg.layer.gcn(layer_sizes=[16, 16], generator=generator)
x_inp, x_out = gcn.in_out_tensors() # create the input and output tensorflow tensors

# use tensorflow keras to add a layer to compute the (one-hot) predictions
predictions = tf.keras.layers.dense(units=len(ground_truth_targets.columns), activation=""softmax"")(x_out)

# use the input and output tensors to create a tensorflow keras model
model = tf.keras.model(inputs=x_inp, outputs=predictions)
```

#### training and evaluation

the model is a conventional tensorflow keras model, and so tasks such as training and evaluation can use the functions offered by keras. stellargraph's data generators make it simple to construct the required keras sequences for input data.

```python
# prepare the model for training with the adam optimiser and an appropriate loss function
model.compile(""adam"", loss=""categorical_crossentropy"", metrics=[""accuracy""])

# train the model on the train set
model.fit(generator.flow(train_targets.index, train_targets), epochs=5)

# check model generalisation on the test set
(loss, accuracy) = model.evaluate(generator.flow(test_targets.index, test_targets))
print(f""test set: loss = {loss}, accuracy = {accuracy}"")
```

this algorithm is spelled out in more detail in [its extended narrated notebook][gcn-demo]. we provide [many more algorithms, each with a detailed example][demos].

[gcn-demo]: https://stellargraph.readthedocs.io/en/stable/demos/node-classification/gcn-node-classification.html

## algorithms
the stellargraph library currently includes the following algorithms for graph machine learning:

| algorithm | description |
| --- | --- |
| graphsage [1] | supports supervised as well as unsupervised representation learning, node classification/regression, and link prediction for homogeneous networks. the current implementation supports multiple aggregation methods, including mean, maxpool, meanpool, and attentional aggregators. |
| hinsage | extension of graphsage algorithm to heterogeneous networks. supports representation learning, node classification/regression, and link prediction/regression for heterogeneous graphs. the current implementation supports mean aggregation of neighbour nodes, taking into account their types and the types of links between them. |
| attri2vec [4] | supports node representation learning, node classification, and out-of-sample node link prediction for homogeneous graphs with node attributes. |
| graph attention network (gat) [5] | the gat algorithm supports representation learning and node classification for homogeneous graphs. there are versions of the graph attention layer that support both sparse and dense adjacency matrices. |
| graph convolutional network (gcn) [6] | the gcn algorithm supports representation learning and node classification for homogeneous graphs. there are versions of the graph convolutional layer that support both sparse and dense adjacency matrices. |
| cluster graph convolutional network (cluster-gcn) [10] | an extension of the gcn algorithm supporting representation learning and node classification for homogeneous graphs. cluster-gcn scales to larger graphs and can be used to train deeper gcn models using stochastic gradient descent. |
| simplified graph convolutional network (sgc) [7] | the sgc network algorithm supports representation learning and node classification for homogeneous graphs. it is an extension of the gcn algorithm that smooths the graph to bring in more distant neighbours of nodes without using multiple layers. |
| (approximate) personalized propagation of neural predictions (ppnp/appnp) [9] | the (a)ppnp algorithm supports fast and scalable representation learning and node classification for attributed homogeneous graphs. in a semi-supervised setting, first a multilayer neural network is trained using the node attributes as input. the predictions from the latter network are then diffused across the graph using a method based on personalized pagerank. |
| node2vec [2] | the node2vec and deepwalk algorithms perform unsupervised representation learning for homogeneous networks, taking into account network structure while ignoring node attributes. the node2vec algorithm is implemented by combining stellargraph's random walk generator with the word2vec algorithm from [gensim](https://radimrehurek.com/gensim/). learned node representations can be used in downstream machine learning models implemented using [scikit-learn](https://scikit-learn.org/stable/), [keras](https://keras.io/), [tensorflow](https://www.tensorflow.org/) or any other python machine learning library. |
| metapath2vec [3] | the metapath2vec algorithm performs unsupervised, metapath-guided representation learning for heterogeneous networks, taking into account network structure while ignoring node attributes. the implementation combines stellargraph's metapath-guided random walk generator and [gensim](https://radimrehurek.com/gensim/) word2vec algorithm. as with node2vec, the learned node representations (node embeddings) can be used in downstream machine learning models to solve tasks such as node classification, link prediction, etc, for heterogeneous networks. |
| relational graph convolutional network [11] | the rgcn algorithm performs semi-supervised learning for node representation and node classification on knowledge graphs. rgcn extends gcn to directed graphs with multiple edge types and works with both sparse and dense adjacency matrices.|
| complex[12] | the complex algorithm computes embeddings for nodes (entities) and edge types (relations) in knowledge graphs, and can use these for link prediction |
| graphwave [13] | graphwave calculates unsupervised structural embeddings via wavelet diffusion through the graph. |
| supervised graph classification | a model for supervised graph classification based on gcn [6] layers and mean pooling readout. |
| watch your step [14] | the watch your step algorithm computes node embeddings by using adjacency powers to simulate expected random walks. |
| deep graph infomax [15] | deep graph infomax trains unsupervised gnns to maximize the shared information between node level and graph level features. |
| continuous-time dynamic network embeddings (ctdne) [16] | supports time-respecting random walks which can be used in a similar way as in node2vec for unsupervised representation learning. |
| distmult [17] | the distmult algorithm computes embeddings for nodes (entities) and edge types (relations) in knowledge graphs, and can use these for link prediction |
| dgcnn [18] | the deep graph convolutional neural network (dgcnn) algorithm for supervised graph classification. |
| tgcn [19] | the gcn_lstm model in stellargraph follows the temporal graph convolutional network architecture proposed in the tgcn paper with a few enhancements in the layers architecture. |

## installation

stellargraph is a python 3 library and we recommend using python version `3.6`. the required python version
can be downloaded and installed from [python.org](https://python.org/). alternatively, use the anaconda python
environment, available from [anaconda.com](https://www.anaconda.com/download/).

the stellargraph library can be installed from pypi, from anaconda cloud, or directly from github, as described below.

#### install stellargraph using pypi:
to install stellargraph library from [pypi](https://pypi.org) using `pip`, execute the following command:
```
pip install stellargraph
```

[some of the examples][demos] require installing additional dependencies as well as `stellargraph`. to install these dependencies as well as stellargraph using `pip` execute the following command:
```
pip install stellargraph[demos]
```

the community detection demos requires `python-igraph` which is only available on some platforms. to install this in addition to the other demo requirements:
```
pip install stellargraph[demos,igraph]
```

#### install stellargraph in anaconda python:
the stellargraph library is available an [anaconda cloud](https://anaconda.org/stellargraph/stellargraph) and can be installed in [anaconda python](https://anaconda.com) using the command line `conda` tool, execute the following command:
```
conda install -c stellargraph stellargraph
```


#### install stellargraph from github source:
first, clone the stellargraph repository using `git`:
```
git clone https://github.com/stellargraph/stellargraph.git
```

then, `cd` to the stellargraph folder, and install the library by executing the following commands:
```
cd stellargraph
pip install .
```

some of the examples in the `demos` directory require installing additional dependencies as well as `stellargraph`. to install these dependencies as well as stellargraph using `pip` execute the following command:
```
pip install .[demos]
```


## citing
stellargraph is designed, developed and supported by [csiro's data61](https://data61.csiro.au/).
if you use any part of this library in your research, please cite it using the following bibtex entry
```latex
@misc{stellargraph,
  author = {csiro's data61},
  title = {stellargraph machine learning library},
  year = {2018},
  publisher = {github},
  journal = {github repository},
  howpublished = {\url{https://github.com/stellargraph/stellargraph}},
}
```

## references

1. inductive representation learning on large graphs. w.l. hamilton, r. ying, and j. leskovec.
neural information processing systems (nips), 2017. ([link](https://arxiv.org/abs/1706.02216) [webpage](https://snap.stanford.edu/graphsage/))

2. node2vec: scalable feature learning for networks. a. grover, j. leskovec. acm sigkdd international conference on knowledge discovery and data mining (kdd), 2016. ([link](https://snap.stanford.edu/node2vec/))

3. metapath2vec: scalable representation learning for heterogeneous networks. yuxiao dong, nitesh v. chawla, and ananthram swami.
acm sigkdd international conference on knowledge discovery and data mining (kdd), 135‚Äì144, 2017
([link](https://ericdongyx.github.io/metapath2vec/m2v.html))

4. attributed network embedding via subspace discovery. d. zhang, y. jie, x. zhu and c. zhang, data mining and knowledge discovery, 2019. ([link](https://link.springer.com/article/10.1007/s10618-019-00650-2))

5. graph attention networks. p. veliƒçkoviƒá et al.
international conference on learning representations (iclr) 2018 ([link](https://arxiv.org/abs/1710.10903))

6. graph convolutional networks (gcn): semi-supervised classification with graph convolutional networks. thomas n. kipf, max welling.
international conference on learning representations (iclr), 2017
([link](https://github.com/tkipf/gcn))

7. simplifying graph convolutional networks. f. wu, t. zhang, a. h. de souza, c. fifty, t. yu, and k. q. weinberger.
international conference on machine learning (icml), 2019. ([link](https://arxiv.org/abs/1902.07153))

8. adversarial examples on graph data: deep insights into attack and defense. h. wu, c. wang, y. tyshetskiy, a. docherty, k. lu, and l. zhu. ijcai 2019. ([link](https://arxiv.org/abs/1903.01610))

9. predict then propagate: graph neural networks meet personalized pagerank. j. klicpera, a. bojchevski, a., and s. g√ºnnemann, iclr, 2019, arxiv:1810.05997.([link](https://arxiv.org/abs/1810.05997))

10. cluster-gcn: an efficient algorithm for training deep and large graph convolutional networks. w. chiang, x. liu, s. si, y. li, s. bengio, and c. hsiej, kdd, 2019, arxiv:1905.07953.([link](https://arxiv.org/abs/1905.07953))

11. modeling relational data with graph convolutional networks. m. schlichtkrull, t. n. kipf, p. bloem, r. van den berg, i. titov, and m. welling, european semantic web conference (2018), arxiv:1609.02907 ([link](https://arxiv.org/abs/1703.06103)).

12. complex embeddings for simple link prediction. t. trouillon, j. welbl, s. riedel, √©. gaussier and g. bouchard, icml 2016. ([link](http://jmlr.org/proceedings/papers/v48/trouillon16.pdf))

13. learning structural node embeddings via diffusion wavelets. c. donnat, m. zitnik, d. hallac, and j. leskovec, sigkdd, 2018, arxiv:1710.10321 ([link](https://arxiv.org/pdf/1710.10321.pdf))

14. watch your step: learning node embeddings via graph attention. s. abu-el-haija, b. perozzi, r. al-rfou and a. alemi, nips, 2018. arxiv:1710.09599 ([link](https://arxiv.org/abs/1710.09599))

15. deep graph infomax. p. veliƒçkoviƒá, w. fedus, w. l. hamilton, p. lio, y. bengio, r. d. hjelm, iclr, 2019, arxiv:1809.10341 ([link](https://arxiv.org/pdf/1809.10341.pdf)).

16. continuous-time dynamic network embeddings. giang hoang nguyen, john boaz lee, ryan a. rossi, nesreen k. ahmed, eunyee koh, and sungchul kim. proceedings of the 3rd international workshop on learning representations for big networks (www bignet) 2018. ([link](https://dl.acm.org/doi/10.1145/3184558.3191526))

17. embedding entities and relations for learning and inference in knowledge bases. bishan yang, wen-tau yih, xiaodong he, jianfeng gao, and li deng, iclr, 2015. arxiv:1412.6575 ([link](https://arxiv.org/pdf/1412.6575))

18. an end-to-end deep learning architecture for graph classification. muhan zhang, zhicheng cui, marion neumann, and yixin chen, aaai, 2018. ([link](https://www.cse.wustl.edu/~muhan/papers/aaai_2018_dgcnn.pdf))

19. t-gcn: a temporal graph convolutional network for traffic prediction. ling zhao, yujiao song, chao zhang, yu liu, pu wang, tao lin, min deng, and haifeng li.ieee transactions on intelligent transportation systems, 2019. ([link](https://ieeexplore.ieee.org/document/8809901))
"
"Cornac","# cornac

**cornac** is a comparative framework for multimodal recommender systems. it focuses on making it **convenient** to work with models leveraging **auxiliary data** (e.g., item descriptive text and image, social network, etc). **cornac** enables **fast** experiments and **straightforward** implementations of new models. it is **highly compatible** with existing machine learning libraries (e.g., tensorflow, pytorch).

### quick links

[website](https://cornac.preferred.ai/) |
[documentation](https://cornac.readthedocs.io/en/latest/index.html) |
[tutorials](tutorials#tutorials) |
[examples](https://github.com/preferredai/cornac/tree/master/examples#cornac-examples-directory) |
[models](#models) |
[datasets](./cornac/datasets/readme.md#datasets) |
[paper](http://www.jmlr.org/papers/volume21/19-805/19-805.pdf) |
[preferred.ai](https://preferred.ai/)

[![.github/workflows/python-package.yml](https://github.com/preferredai/cornac/actions/workflows/python-package.yml/badge.svg)](https://github.com/preferredai/cornac/actions/workflows/python-package.yml)
[![circleci](https://img.shields.io/circleci/project/github/preferredai/cornac/master.svg?logo=circleci)](https://circleci.com/gh/preferredai/cornac)
[![appveyor](https://ci.appveyor.com/api/projects/status/0yq4td1xg4kkhdwu?svg=true)](https://ci.appveyor.com/project/tqtg/cornac)
[![codecov](https://img.shields.io/codecov/c/github/preferredai/cornac/master.svg?logo=codecov)](https://codecov.io/gh/preferredai/cornac)
[![docs](https://img.shields.io/readthedocs/cornac/latest.svg)](https://cornac.readthedocs.io/en/latest)
<br />
[![release](https://img.shields.io/github/release-pre/preferredai/cornac.svg)](https://github.com/preferredai/cornac/releases)
[![pypi](https://img.shields.io/pypi/v/cornac.svg)](https://pypi.org/project/cornac/)
[![conda](https://img.shields.io/conda/vn/conda-forge/cornac.svg)](https://anaconda.org/conda-forge/cornac)
[![conda recipe](https://img.shields.io/badge/recipe-cornac-green.svg)](https://github.com/conda-forge/cornac-feedstock)
[![downloads](https://static.pepy.tech/personalized-badge/cornac?period=total&units=international_system&left_color=grey&right_color=orange&left_text=downloads)](https://pepy.tech/project/cornac)
<br />
[![python](https://img.shields.io/pypi/pyversions/cornac.svg)](https://cornac.preferred.ai/)
[![conda platforms](https://img.shields.io/conda/pn/conda-forge/cornac.svg)](https://anaconda.org/conda-forge/cornac)
[![license](https://img.shields.io/badge/license-apache%202.0-yellow.svg)](https://opensource.org/licenses/apache-2.0)


## installation

currently, we are supporting python 3. there are several ways to install cornac:

- **from pypi (you may need a c++ compiler):**
  ```bash
  pip3 install cornac
  ```

- **from anaconda:**
  ```bash
  conda install cornac -c conda-forge
  ```

- **from the github source (for latest updates):**
  ```bash
  pip3 install cython
  git clone https://github.com/preferredai/cornac.git
  cd cornac
  python3 setup.py install
  ```

**note:** 

additional dependencies required by models are listed [here](readme.md#models).

some algorithm implementations use `openmp` to support multi-threading. for mac os users, in order to run those algorithms efficiently, you might need to install `gcc` from homebrew to have an openmp compiler:
```bash
brew install gcc | brew link gcc
```

## getting started: your first cornac experiment

![](flow.jpg)
<p align=""center""><i>flow of an experiment in cornac</i></p>

```python
import cornac
from cornac.eval_methods import ratiosplit
from cornac.models import mf, pmf, bpr
from cornac.metrics import mae, rmse, precision, recall, ndcg, auc, map

# load the built-in movielens 100k and split the data based on ratio
ml_100k = cornac.datasets.movielens.load_feedback()
rs = ratiosplit(data=ml_100k, test_size=0.2, rating_threshold=4.0, seed=123)

# initialize models, here we are comparing: biased mf, pmf, and bpr
models = [
    mf(k=10, max_iter=25, learning_rate=0.01, lambda_reg=0.02, use_bias=true, seed=123),
    pmf(k=10, max_iter=100, learning_rate=0.001, lambda_reg=0.001, seed=123),
    bpr(k=10, max_iter=200, learning_rate=0.001, lambda_reg=0.01, seed=123),
]

# define metrics to evaluate the models
metrics = [mae(), rmse(), precision(k=10), recall(k=10), ndcg(k=10), auc(), map()]

# put it together in an experiment, voil√†!
cornac.experiment(eval_method=rs, models=models, metrics=metrics, user_based=true).run()
```

**output:**

|                          |    mae |   rmse |    auc |     map | ndcg@10 | precision@10 | recall@10 |  train (s) | test (s) |
| ------------------------ | -----: | -----: | -----: | ------: | ------: | -----------: | --------: | ---------: | -------: |
| [mf](cornac/models/mf)   | 0.7430 | 0.8998 | 0.7445 |  0.0407 |  0.0479 |       0.0437 |    0.0352 |       0.13 |     1.57 |
| [pmf](cornac/models/pmf) | 0.7534 | 0.9138 | 0.7744 |  0.0491 |  0.0617 |       0.0533 |    0.0479 |       2.18 |     1.64 |
| [bpr](cornac/models/bpr) |    n/a |    n/a | 0.8695 |  0.0753 |  0.0975 |       0.0727 |    0.0891 |       3.74 |     1.49 |


for more details, please take a look at our [examples](examples) as well as [tutorials](tutorials). for learning purposes, this list of [tutorials on recommender systems](https://github.com/preferredai/tutorials/tree/master/recommender-systems) will be more organized and comprehensive. 


## models

the recommender models supported by cornac are listed below. why don't you join us to lengthen the list?

| year | model and paper | additional dependencies | examples |
| :---: | --- | :---: | :---: |
| 2021 | [bilateral variational autoencoder for collaborative filtering (bivaecf)](cornac/models/bivaecf), [paper](https://dl.acm.org/doi/pdf/10.1145/3437963.3441759) | [requirements.txt](cornac/models/bivaecf/requirements.txt) | [preferredai/bi-vae](https://github.com/preferredai/bi-vae)
|      | [causal inference for visual debiasing in visually-aware recommendation (causalrec)](cornac/models/causalrec), [paper](https://arxiv.org/abs/2107.02390) | [requirements.txt](cornac/models/causalrec/requirements.txt) | [causalrec_clothing.py](examples/causalrec_clothing.py)
|      | [explainable recommendation with comparative constraints on product aspects (comparer)](cornac/models/comparer), [paper](https://dl.acm.org/doi/pdf/10.1145/3437963.3441754) | n/a | [preferredai/comparer](https://github.com/preferredai/comparer)
| 2020 | [adversarial training towards robust multimedia recommender system (amr)](cornac/models/amr), [paper](https://ieeexplore.ieee.org/document/8618394) | [requirements.txt](cornac/models/amr/requirements.txt) | [amr_clothing.py](examples/amr_clothing.py)
| 2019 | [embarrassingly shallow autoencoders for sparse data (ease·¥ø)](cornac/models/ease), [paper](https://arxiv.org/pdf/1905.03375.pdf) | n/a | [ease_movielens.py](examples/ease_movielens.py)
| 2018 | [collaborative context poisson factorization (c2pf)](cornac/models/c2pf), [paper](https://www.ijcai.org/proceedings/2018/0370.pdf) | n/a | [c2pf_exp.py](examples/c2pf_example.py)
|      | [multi-task explainable recommendation (mter)](cornac/models/mter), [paper](https://arxiv.org/pdf/1806.03568.pdf) | n/a | [mter_exp.py](examples/mter_example.py)
|      | [neural attention rating regression with review-level explanations (narre)](cornac/models/narre), [paper](http://www.thuir.cn/group/~yqliu/publications/www2018_cc.pdf) | [requirements.txt](cornac/models/narre/requirements.txt) | [narre_example.py](examples/narre_example.py)
|      | [probabilistic collaborative representation learning (pcrl)](cornac/models/pcrl), [paper](http://www.hadylauw.com/publications/uai18.pdf) | [requirements.txt](cornac/models/pcrl/requirements.txt) | [pcrl_exp.py](examples/pcrl_example.py)
|      | [variational autoencoder for collaborative filtering (vaecf)](cornac/models/vaecf), [paper](https://arxiv.org/pdf/1802.05814.pdf) | [requirements.txt](cornac/models/vaecf/requirements.txt) | [vaecf_citeulike.py](examples/vaecf_citeulike.py)
| 2017 | [collaborative variational autoencoder (cvae)](cornac/models/cvae), [paper](http://eelxpeng.github.io/assets/paper/collaborative_variational_autoencoder.pdf) | [requirements.txt](cornac/models/cvae/requirements.txt) | [cvae_exp.py](examples/cvae_example.py)
|      | [conditional variational autoencoder for collaborative filtering (cvaecf)](cornac/models/cvaecf), [paper](https://seslab.kaist.ac.kr/xe2/?module=file&act=procfiledownload&file_srl=18019&sid=4be19b9d0134a4aeacb9ef1ecd81c784&module_srl=1379) | [requirements.txt](cornac/models/cvaecf/requirements.txt) | [cvaecf_filmtrust.py](examples/cvaecf_filmtrust.py)
|      | [generalized matrix factorization (gmf)](cornac/models/ncf), [paper](https://arxiv.org/pdf/1708.05031.pdf) | [requirements.txt](cornac/models/ncf/requirements.txt) | [ncf_exp.py](examples/ncf_example.py)
|      | [indexable bayesian personalized ranking (ibpr)](cornac/models/ibpr), [paper](http://www.hadylauw.com/publications/cikm17a.pdf) | [requirements.txt](cornac/models/ibpr/requirements.txt) | [ibpr_exp.py](examples/ibpr_example.py)
|      | [matrix co-factorization (mcf)](cornac/models/mcf), [paper](http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p1113.pdf) | n/a | [mcf_office.py](examples/mcf_office.py)
|      | [multi-layer perceptron (mlp)](cornac/models/ncf), [paper](https://arxiv.org/pdf/1708.05031.pdf) | [requirements.txt](cornac/models/ncf/requirements.txt) | [ncf_exp.py](examples/ncf_example.py)
|      | [neural matrix factorization (neumf) / neural collaborative filtering (ncf)](cornac/models/ncf), [paper](https://arxiv.org/pdf/1708.05031.pdf) | [requirements.txt](cornac/models/ncf/requirements.txt) | [ncf_exp.py](examples/ncf_example.py)
|      | [online indexable bayesian personalized ranking (online ibpr)](cornac/models/online_ibpr), [paper](http://www.hadylauw.com/publications/cikm17a.pdf) | [requirements.txt](cornac/models/online_ibpr/requirements.txt) |
|      | [visual matrix factorization (vmf)](cornac/models/vmf), [paper](http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p1113.pdf) | [requirements.txt](cornac/models/vmf/requirements.txt) | [vmf_clothing.py](examples/vmf_clothing.py)
| 2016 | [collaborative deep ranking (cdr)](cornac/models/cdr), [paper](http://inpluslab.com/chenliang/homepagefiles/paper/hao-pakdd2016.pdf) | [requirements.txt](cornac/models/cdr/requirements.txt) | [cdr_exp.py](examples/cdr_example.py)
|      | [collaborative ordinal embedding (coe)](cornac/models/coe), [paper](http://www.hadylauw.com/publications/sdm16.pdf) | [requirements.txt](cornac/models/coe/requirements.txt) |
|      | [convolutional matrix factorization (convmf)](cornac/models/conv_mf), [paper](http://uclab.khu.ac.kr/resources/publication/c_351.pdf) | [requirements.txt](cornac/models/conv_mf/requirements.txt) | [convmf_exp.py](examples/conv_mf_example.py)
|      | [spherical k-means (skm)](cornac/models/skm), [paper](https://www.sciencedirect.com/science/article/pii/s092523121501509x) | n/a | [skm_movielens.py](examples/skm_movielens.py)
|      | [visual bayesian personalized ranking (vbpr)](cornac/models/vbpr), [paper](https://arxiv.org/pdf/1510.01784.pdf) | [requirements.txt](cornac/models/vbpr/requirements.txt) | [vbpr_tradesy.py](examples/vbpr_tradesy.py)
| 2015 | [collaborative deep learning (cdl)](cornac/models/cdl), [paper](https://arxiv.org/pdf/1409.2944.pdf) | [requirements.txt](cornac/models/cdl/requirements.txt) | [cdl_exp.py](examples/cdl_example.py)
|      | [hierarchical poisson factorization (hpf)](cornac/models/hpf), [paper](http://jakehofman.com/inprint/poisson_recs.pdf) | n/a | [hpf_movielens.py](examples/hpf_movielens.py)
| 2014 | [explicit factor model (efm)](cornac/models/efm), [paper](http://yongfeng.me/attach/efm-zhang.pdf) | n/a | [efm_exp.py](examples/efm_example.py)
|      | [social bayesian personalized ranking (sbpr)](cornac/models/sbpr), [paper](https://cseweb.ucsd.edu/~jmcauley/pdfs/cikm14.pdf) | n/a | [sbpr_epinions.py](examples/sbpr_epinions.py)
| 2013 | [hidden factors and hidden topics (hft)](cornac/models/hft), [paper](https://cs.stanford.edu/people/jure/pubs/reviews-recsys13.pdf) | n/a | [hft_exp.py](examples/hft_example.py)
| 2012 | [weighted bayesian personalized ranking (wbpr)](cornac/models/bpr), [paper](http://proceedings.mlr.press/v18/gantner12a/gantner12a.pdf) | n/a | [bpr_netflix.py](examples/bpr_netflix.py)
| 2011 | [collaborative topic regression (ctr)](cornac/models/ctr), [paper](http://www.cs.columbia.edu/~blei/papers/wangblei2011.pdf) | n/a | [ctr_citeulike.py](examples/ctr_example_citeulike.py)
| earlier | [baseline only](cornac/models/baseline_only), [paper](http://courses.ischool.berkeley.edu/i290-dm/s11/secure/a1-koren.pdf) | n/a | [svd_exp.py](examples/svd_example.py)
|      | [bayesian personalized ranking (bpr)](cornac/models/bpr), [paper](https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf) | n/a | [bpr_netflix.py](examples/bpr_netflix.py)
|      | [factorization machines (fm)](cornac/models/fm), [paper](https://www.csie.ntu.edu.tw/~b97053/paper/factorization%20machines%20with%20libfm.pdf) | linux only | [fm_example.py](examples/fm_example.py)
|      | [global average (globalavg)](cornac/models/global_avg), [paper](https://datajobs.com/data-science-repo/recommender-systems-[netflix].pdf) | n/a | [biased_mf.py](examples/biased_mf.py)
|      | [item k-nearest-neighbors (itemknn)](cornac/models/knn), [paper](https://dl.acm.org/doi/pdf/10.1145/371920.372071) | n/a | [knn_movielens.py](examples/knn_movielens.py)
|      | [matrix factorization (mf)](cornac/models/mf), [paper](https://datajobs.com/data-science-repo/recommender-systems-[netflix].pdf) | n/a | [biased_mf.py](examples/biased_mf.py), [given_data.py](examples/given_data.py)
|      | [maximum margin matrix factorization (mmmf)](cornac/models/mmmf), [paper](https://link.springer.com/content/pdf/10.1007/s10994-008-5073-7.pdf) | n/a | [mmmf_exp.py](examples/mmmf_exp.py)
|      | [most popular (mostpop)](cornac/models/most_pop), [paper](https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf) | n/a | [bpr_netflix.py](examples/bpr_netflix.py)
|      | [non-negative matrix factorization (nmf)](cornac/models/nmf), [paper](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf) | n/a | [nmf_exp.py](examples/nmf_example.py)
|      | [probabilistic matrix factorization (pmf)](cornac/models/pmf), [paper](https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf) | n/a | [pmf_ratio.py](examples/pmf_ratio.py)
|      | [singular value decomposition (svd)](cornac/models/svd), [paper](https://people.engr.tamu.edu/huangrh/spring16/papers_course/matrix_factorization.pdf) | n/a | [svd_exp.py](examples/svd_example.py)
|      | [social recommendation using pmf (sorec)](cornac/models/sorec), [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.304.2464&rep=rep1&type=pdf) | n/a | [sorec_filmtrust.py](examples/sorec_filmtrust.py)
|      | [user k-nearest-neighbors (userknn)](cornac/models/knn), [paper](https://arxiv.org/pdf/1301.7363.pdf) | n/a | [knn_movielens.py](examples/knn_movielens.py)
|      | [weighted matrix factorization (wmf)](cornac/models/wmf), [paper](http://yifanhu.net/pub/cf.pdf) | [requirements.txt](cornac/models/wmf/requirements.txt) | [wmf_exp.py](examples/wmf_example.py)


## support

your contributions at any level of the library are welcome. if you intend to contribute, please:
  - fork the cornac repository to your own account.
  - make changes and create pull requests.

you can also post bug reports and feature requests in [github issues](https://github.com/preferredai/cornac/issues).

## citation

if you use cornac in a scientific publication, we would appreciate citations to the following papers:

- [cornac: a comparative framework for multimodal recommender systems](http://jmlr.org/papers/v21/19-805.html), salah *et al.*, journal of machine learning research, 21(95):1‚Äì5, 2020.

  ```
  @article{salah2020cornac,
    title={cornac: a comparative framework for multimodal recommender systems},
    author={salah, aghiles and truong, quoc-tuan and lauw, hady w},
    journal={journal of machine learning research},
    volume={21},
    number={95},
    pages={1--5},
    year={2020}
  }
  ```

- [exploring cross-modality utilization in recommender systems](https://ieeexplore.ieee.org/abstract/document/9354572), truong *et al.*, ieee internet computing, 25(4):50‚Äì57, 2021.

  ```
  @article{truong2021exploring,
    title={exploring cross-modality utilization in recommender systems},
    author={truong, quoc-tuan and salah, aghiles and tran, thanh-binh and guo, jingyao and lauw, hady w},
    journal={ieee internet computing},
    year={2021},
    publisher={ieee}
  }
  ```

- [multi-modal recommender systems: hands-on exploration](https://dl.acm.org/doi/10.1145/3460231.3473324), truong *et al.*, acm conference on recommender systems, 2021.

  ```
  @inproceedings{truong2021multi,
    title={multi-modal recommender systems: hands-on exploration},
    author={truong, quoc-tuan and salah, aghiles and lauw, hady},
    booktitle={fifteenth acm conference on recommender systems},
    pages={834--837},
    year={2021}
  }
  ```

## license

[apache license 2.0](license)
"
"JAX","<div align=""center"">
<img src=""https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png"" alt=""logo""></img>
</div>

# jax: autograd and xla

![continuous integration](https://github.com/google/jax/actions/workflows/ci-build.yaml/badge.svg)
![pypi version](https://img.shields.io/pypi/v/jax)

[**quickstart**](#quickstart-colab-in-the-cloud)
| [**transformations**](#transformations)
| [**install guide**](#installation)
| [**neural net libraries**](#neural-network-libraries)
| [**change logs**](https://jax.readthedocs.io/en/latest/changelog.html)
| [**reference docs**](https://jax.readthedocs.io/en/latest/)


## what is jax?

jax is [autograd](https://github.com/hips/autograd) and [xla](https://www.tensorflow.org/xla),
brought together for high-performance machine learning research.

with its updated version of [autograd](https://github.com/hips/autograd),
jax can automatically differentiate native
python and numpy functions. it can differentiate through loops, branches,
recursion, and closures, and it can take derivatives of derivatives of
derivatives. it supports reverse-mode differentiation (a.k.a. backpropagation)
via [`grad`](#automatic-differentiation-with-grad) as well as forward-mode differentiation,
and the two can be composed arbitrarily to any order.

what‚Äôs new is that jax uses [xla](https://www.tensorflow.org/xla)
to compile and run your numpy programs on gpus and tpus. compilation happens
under the hood by default, with library calls getting just-in-time compiled and
executed. but jax also lets you just-in-time compile your own python functions
into xla-optimized kernels using a one-function api,
[`jit`](#compilation-with-jit). compilation and automatic differentiation can be
composed arbitrarily, so you can express sophisticated algorithms and get
maximal performance without leaving python. you can even program multiple gpus
or tpu cores at once using [`pmap`](#spmd-programming-with-pmap), and
differentiate through the whole thing.

dig a little deeper, and you'll see that jax is really an extensible system for
[composable function transformations](#transformations). both
[`grad`](#automatic-differentiation-with-grad) and [`jit`](#compilation-with-jit)
are instances of such transformations. others are
[`vmap`](#auto-vectorization-with-vmap) for automatic vectorization and
[`pmap`](#spmd-programming-with-pmap) for single-program multiple-data (spmd)
parallel programming of multiple accelerators, with more to come.

this is a research project, not an official google product. expect bugs and
[sharp edges](https://jax.readthedocs.io/en/latest/notebooks/common_gotchas_in_jax.html).
please help by trying it out, [reporting
bugs](https://github.com/google/jax/issues), and letting us know what you
think!

```python
import jax.numpy as jnp
from jax import grad, jit, vmap

def predict(params, inputs):
  for w, b in params:
    outputs = jnp.dot(inputs, w) + b
    inputs = jnp.tanh(outputs)  # inputs to the next layer
  return outputs                # no activation on last layer

def loss(params, inputs, targets):
  preds = predict(params, inputs)
  return jnp.sum((preds - targets)**2)

grad_loss = jit(grad(loss))  # compiled gradient evaluation function
perex_grads = jit(vmap(grad_loss, in_axes=(none, 0, 0)))  # fast per-example grads
```

### contents
* [quickstart: colab in the cloud](#quickstart-colab-in-the-cloud)
* [transformations](#transformations)
* [current gotchas](#current-gotchas)
* [installation](#installation)
* [neural net libraries](#neural-network-libraries)
* [citing jax](#citing-jax)
* [reference documentation](#reference-documentation)

## quickstart: colab in the cloud
jump right in using a notebook in your browser, connected to a google cloud gpu.
here are some starter notebooks:
- [the basics: numpy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)
- [training a simple neural network, with tensorflow dataset data loading](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb)

**jax now runs on cloud tpus.** to try out the preview, see the [cloud tpu
colabs](https://github.com/google/jax/tree/main/cloud_tpu_colabs).

for a deeper dive into jax:
- [the autodiff cookbook, part 1: easy and powerful automatic differentiation in jax](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)
- [common gotchas and sharp edges](https://jax.readthedocs.io/en/latest/notebooks/common_gotchas_in_jax.html)
- see the [full list of
notebooks](https://github.com/google/jax/tree/main/docs/notebooks).

you can also take a look at [the mini-libraries in
`jax.example_libraries`](https://github.com/google/jax/tree/main/jax/example_libraries/readme.md),
like [`stax` for building neural
networks](https://github.com/google/jax/tree/main/jax/example_libraries/readme.md#neural-net-building-with-stax)
and [`optimizers` for first-order stochastic
optimization](https://github.com/google/jax/tree/main/jax/example_libraries/readme.md#first-order-optimization),
or the [examples](https://github.com/google/jax/tree/main/examples).

## transformations

at its core, jax is an extensible system for transforming numerical functions.
here are four transformations of primary interest: `grad`, `jit`, `vmap`, and
`pmap`.

### automatic differentiation with `grad`

jax has roughly the same api as [autograd](https://github.com/hips/autograd).
the most popular function is
[`grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)
for reverse-mode gradients:

```python
from jax import grad
import jax.numpy as jnp

def tanh(x):  # define a function
  y = jnp.exp(-2.0 * x)
  return (1.0 - y) / (1.0 + y)

grad_tanh = grad(tanh)  # obtain its gradient function
print(grad_tanh(1.0))   # evaluate it at x = 1.0
# prints 0.4199743
```

you can differentiate to any order with `grad`.

```python
print(grad(grad(grad(tanh)))(1.0))
# prints 0.62162673
```

for more advanced autodiff, you can use
[`jax.vjp`](https://jax.readthedocs.io/en/latest/jax.html#jax.vjp) for
reverse-mode vector-jacobian products and
[`jax.jvp`](https://jax.readthedocs.io/en/latest/jax.html#jax.jvp) for
forward-mode jacobian-vector products. the two can be composed arbitrarily with
one another, and with other jax transformations. here's one way to compose those
to make a function that efficiently computes [full hessian
matrices](https://jax.readthedocs.io/en/latest/jax.html#jax.hessian):

```python
from jax import jit, jacfwd, jacrev

def hessian(fun):
  return jit(jacfwd(jacrev(fun)))
```

as with [autograd](https://github.com/hips/autograd), you're free to use
differentiation with python control structures:

```python
def abs_val(x):
  if x > 0:
    return x
  else:
    return -x

abs_val_grad = grad(abs_val)
print(abs_val_grad(1.0))   # prints 1.0
print(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)
```

see the [reference docs on automatic
differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)
and the [jax autodiff
cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)
for more.

### compilation with `jit`

you can use xla to compile your functions end-to-end with
[`jit`](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit),
used either as an `@jit` decorator or as a higher-order function.

```python
import jax.numpy as jnp
from jax import jit

def slow_f(x):
  # element-wise ops see a large benefit from fusion
  return x * x + x * 2.0

x = jnp.ones((5000, 5000))
fast_f = jit(slow_f)
%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on titan x
%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on gpu via jax)
```

you can mix `jit` and `grad` and any other jax transformation however you like.

using `jit` puts constraints on the kind of python control flow
the function can use; see
the [gotchas
notebook](https://jax.readthedocs.io/en/latest/notebooks/common_gotchas_in_jax.html#python-control-flow-+-jit)
for more.

### auto-vectorization with `vmap`

[`vmap`](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap) is
the vectorizing map.
it has the familiar semantics of mapping a function along array axes, but
instead of keeping the loop on the outside, it pushes the loop down into a
function‚Äôs primitive operations for better performance.

using `vmap` can save you from having to carry around batch dimensions in your
code. for example, consider this simple *unbatched* neural network prediction
function:

```python
def predict(params, input_vec):
  assert input_vec.ndim == 1
  activations = input_vec
  for w, b in params:
    outputs = jnp.dot(w, activations) + b  # `activations` on the right-hand side!
    activations = jnp.tanh(outputs)        # inputs to the next layer
  return outputs                           # no activation on last layer
```

we often instead write `jnp.dot(activations, w)` to allow for a batch dimension on the
left side of `activations`, but we‚Äôve written this particular prediction function to
apply only to single input vectors. if we wanted to apply this function to a
batch of inputs at once, semantically we could just write

```python
from functools import partial
predictions = jnp.stack(list(map(partial(predict, params), input_batch)))
```

but pushing one example through the network at a time would be slow! it‚Äôs better
to vectorize the computation, so that at every layer we‚Äôre doing matrix-matrix
multiplication rather than matrix-vector multiplication.

the `vmap` function does that transformation for us. that is, if we write

```python
from jax import vmap
predictions = vmap(partial(predict, params))(input_batch)
# or, alternatively
predictions = vmap(predict, in_axes=(none, 0))(params, input_batch)
```

then the `vmap` function will push the outer loop inside the function, and our
machine will end up executing matrix-matrix multiplications exactly as if we‚Äôd
done the batching by hand.

it‚Äôs easy enough to manually batch a simple neural network without `vmap`, but
in other cases manual vectorization can be impractical or impossible. take the
problem of efficiently computing per-example gradients: that is, for a fixed set
of parameters, we want to compute the gradient of our loss function evaluated
separately at each example in a batch. with `vmap`, it‚Äôs easy:

```python
per_example_gradients = vmap(partial(grad(loss), params))(inputs, targets)
```

of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other
jax transformation! we use `vmap` with both forward- and reverse-mode automatic
differentiation for fast jacobian and hessian matrix calculations in
`jax.jacfwd`, `jax.jacrev`, and `jax.hessian`.

### spmd programming with `pmap`

for parallel programming of multiple accelerators, like multiple gpus, use
[`pmap`](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap).
with `pmap` you write single-program multiple-data (spmd) programs, including
fast parallel collective communication operations. applying `pmap` will mean
that the function you write is compiled by xla (similarly to `jit`), then
replicated and executed in parallel across devices.

here's an example on an 8-gpu machine:

```python
from jax import random, pmap
import jax.numpy as jnp

# create 8 random 5000 x 6000 matrices, one per gpu
keys = random.split(random.prngkey(0), 8)
mats = pmap(lambda key: random.normal(key, (5000, 6000)))(keys)

# run a local matmul on each device in parallel (no data transfer)
result = pmap(lambda x: jnp.dot(x, x.t))(mats)  # result.shape is (8, 5000, 5000)

# compute the mean on each device in parallel and print the result
print(pmap(jnp.mean)(result))
# prints [1.1566595 1.1805978 ... 1.2321935 1.2015157]
```

in addition to expressing pure maps, you can use fast [collective communication
operations](https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators)
between devices:

```python
from functools import partial
from jax import lax

@partial(pmap, axis_name='i')
def normalize(x):
  return x / lax.psum(x, 'i')

print(normalize(jnp.arange(4.)))
# prints [0.         0.16666667 0.33333334 0.5       ]
```

you can even [nest `pmap` functions](https://colab.research.google.com/github/google/jax/blob/main/cloud_tpu_colabs/pmap_cookbook.ipynb#scrollto=mdrscr5monun) for more
sophisticated communication patterns.

it all composes, so you're free to differentiate through parallel computations:

```python
from jax import grad

@pmap
def f(x):
  y = jnp.sin(x)
  @pmap
  def g(z):
    return jnp.cos(z) * jnp.tan(y.sum()) * jnp.tanh(x).sum()
  return grad(lambda w: jnp.sum(g(w)))(x)

print(f(x))
# [[ 0.        , -0.7170853 ],
#  [-3.1085174 , -0.4824318 ],
#  [10.366636  , 13.135289  ],
#  [ 0.22163185, -0.52112055]]

print(grad(lambda x: jnp.sum(f(x)))(x))
# [[ -3.2369726,  -1.6356447],
#  [  4.7572474,  11.606951 ],
#  [-98.524414 ,  42.76499  ],
#  [ -1.6007166,  -1.2568436]]
```

when reverse-mode differentiating a `pmap` function (e.g. with `grad`), the
backward pass of the computation is parallelized just like the forward pass.

see the [spmd
cookbook](https://colab.research.google.com/github/google/jax/blob/main/cloud_tpu_colabs/pmap_cookbook.ipynb)
and the [spmd mnist classifier from scratch
example](https://github.com/google/jax/blob/main/examples/spmd_mnist_classifier_fromscratch.py)
for more.

## current gotchas

for a more thorough survey of current gotchas, with examples and explanations,
we highly recommend reading the [gotchas
notebook](https://jax.readthedocs.io/en/latest/notebooks/common_gotchas_in_jax.html).
some standouts:

1. jax transformations only work on [pure functions](https://en.wikipedia.org/wiki/pure_function), which don't have side-effects and respect [referential transparency](https://en.wikipedia.org/wiki/referential_transparency) (i.e. object identity testing with `is` isn't preserved). if you use a jax transformation on an impure python function, you might see an error like `exception: can't lift traced...`  or `exception: different traces at same level`.
1. [in-place mutating updates of
   arrays](https://jax.readthedocs.io/en/latest/notebooks/common_gotchas_in_jax.html#in-place-updates), like `x[i] += y`, aren't supported, but [there are functional alternatives](https://jax.readthedocs.io/en/latest/jax.ops.html). under a `jit`, those functional alternatives will reuse buffers in-place automatically.
1. [random numbers are
   different](https://jax.readthedocs.io/en/latest/notebooks/common_gotchas_in_jax.html#random-numbers), but for [good reasons](https://github.com/google/jax/blob/main/docs/jep/263-prng.md).
1. if you're looking for [convolution
   operators](https://jax.readthedocs.io/en/latest/notebooks/convolutions.html),
   they're in the `jax.lax` package.
1. jax enforces single-precision (32-bit, e.g. `float32`) values by default, and
   [to enable
   double-precision](https://jax.readthedocs.io/en/latest/notebooks/common_gotchas_in_jax.html#double-64bit-precision)
   (64-bit, e.g. `float64`) one needs to set the `jax_enable_x64` variable at
   startup (or set the environment variable `jax_enable_x64=true`).
   on tpu, jax uses 32-bit values by default for everything _except_ internal
   temporary variables in 'matmul-like' operations, such as `jax.numpy.dot` and `lax.conv`.
   those ops have a `precision` parameter which can be used to simulate
   true 32-bit, with a cost of possibly slower runtime.
1. some of numpy's dtype promotion semantics involving a mix of python scalars
   and numpy types aren't preserved, namely `np.add(1, np.array([2],
   np.float32)).dtype` is `float64` rather than `float32`.
1. some transformations, like `jit`, [constrain how you can use python control
   flow](https://jax.readthedocs.io/en/latest/notebooks/common_gotchas_in_jax.html#control-flow).
   you'll always get loud errors if something goes wrong. you might have to use
   [`jit`'s `static_argnums`
   parameter](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit),
   [structured control flow
   primitives](https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators)
   like
   [`lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan),
   or just use `jit` on smaller subfunctions.

## installation

jax is written in pure python, but it depends on xla, which needs to be
installed as the `jaxlib` package. use the following instructions to install a
binary package with `pip` or `conda`, or to [build jax from
source](https://jax.readthedocs.io/en/latest/developer.html#building-from-source).

we support installing or building `jaxlib` on linux (ubuntu 16.04 or later) and
macos (10.12 or later) platforms.

windows users can use jax on cpu and gpu via the [windows subsystem for
linux](https://docs.microsoft.com/en-us/windows/wsl/about). in addition, there
is some initial community-driven native windows support, but since it is still
somewhat immature, there are no official binary releases and it must be [built
from source for windows](https://jax.readthedocs.io/en/latest/developer.html#additional-notes-for-building-jaxlib-from-source-on-windows).
for an unofficial discussion of native windows builds, see also the [issue #5795
thread](https://github.com/google/jax/issues/5795).

### pip installation: cpu

to install a cpu-only version of jax, which might be useful for doing local
development on a laptop, you can run

```bash
pip install --upgrade pip
pip install --upgrade ""jax[cpu]""
```

on linux, it is often necessary to first update `pip` to a version that supports
`manylinux2014` wheels. also note that for linux, we currently release wheels for `x86_64` architectures only, other architectures require building from source. trying to pip install with other linux architectures may lead to `jaxlib` not being installed alongside `jax`, although `jax` may successfully install (but fail at runtime). 
**these `pip` installations do not work with windows, and may fail silently; see
[above](#installation).**

### pip installation: gpu (cuda)

if you want to install jax with both cpu and nvidia gpu support, you must first
install [cuda](https://developer.nvidia.com/cuda-downloads) and
[cudnn](https://developer.nvidia.com/cudnn),
if they have not already been installed. unlike some other popular deep
learning systems, jax does not bundle cuda or cudnn as part of the `pip`
package.

jax provides pre-built cuda-compatible wheels for **linux only**,
with cuda 11.4 or newer, and cudnn 8.2 or newer. note these existing wheels are currently for `x86_64` architectures only. other combinations of
operating system, cuda, and cudnn are possible, but require [building from
source](https://jax.readthedocs.io/en/latest/developer.html#building-from-source).

* cuda 11.4 or newer is *required*.
  * your cuda installation must be new enough to support your gpu. if you have
    an ada lovelace (e.g., rtx 4080) or hopper (e.g., h100) gpu,
    you must use cuda 11.8 or newer.
* the supported cudnn versions for the prebuilt wheels are:
  * cudnn 8.6 or newer. we recommend using the cudnn 8.6 wheel if your cudnn
    installation is new enough, since it supports additional functionality.
  * cudnn 8.2 or newer.
* you *must* use an nvidia driver version that is at least as new as your
  [cuda toolkit's corresponding driver version](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-major-component-versions__table-cuda-toolkit-driver-versions).
  for example, if you have cuda 11.4 update 4 installed, you must use nvidia
  driver 470.82.01 or newer if on linux. this is a strict requirement that
  exists because jax relies on jit-compiling code; older drivers may lead to
  failures.
  * if you need to use an newer cuda toolkit with an older driver, for example
    on a cluster where you cannot update the nvidia driver easily, you may be
    able to use the
    [cuda forward compatibility packages](https://docs.nvidia.com/deploy/cuda-compatibility/)
    that nvidia provides for this purpose.


next, run

```bash
pip install --upgrade pip
# installs the wheel compatible with cuda 11 and cudnn 8.6 or newer.
# note: wheels only available on linux.
pip install --upgrade ""jax[cuda]"" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

**these `pip` installations do not work with windows, and may fail silently; see
[above](#installation).**

the jaxlib version must correspond to the version of the existing cuda
installation you want to use. you can specify a particular cuda and cudnn
version for jaxlib explicitly:

```bash
pip install --upgrade pip

# installs the wheel compatible with cuda >= 11.8 and cudnn >= 8.6
pip install ""jax[cuda11_cudnn86]"" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

# installs the wheel compatible with cuda >= 11.4 and cudnn >= 8.2
pip install ""jax[cuda11_cudnn82]"" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

you can find your cuda version with the command:

```bash
nvcc --version
```

some gpu functionality expects the cuda installation to be at
`/usr/local/cuda-x.x`, where x.x should be replaced with the cuda version number
(e.g. `cuda-11.8`). if cuda is installed elsewhere on your system, you can either
create a symlink:

```bash
sudo ln -s /path/to/cuda /usr/local/cuda-x.x
```

please let us know on [the issue tracker](https://github.com/google/jax/issues)
if you run into any errors or problems with the prebuilt wheels.

### pip installation: google cloud tpu
jax also provides pre-built wheels for
[google cloud tpu](https://cloud.google.com/tpu/docs/users-guide-tpu-vm).
to install jax along with appropriate versions of `jaxlib` and `libtpu`, you can run
the following in your cloud tpu vm:
```bash
pip install --upgrade pip
pip install ""jax[tpu]>=0.2.16"" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
```

### pip installation: colab tpu
colab tpu runtimes come with jax pre-installed, but before importing jax you must run the following code to initialize the tpu:
```python
import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()
```
colab tpu runtimes use an older tpu architecture than cloud tpu vms, so installing `jax[tpu]` should be avoided on colab.
if for any reason you would like to update the jax & jaxlib libraries on a colab tpu runtime, follow the cpu instructions above (i.e. install `jax[cpu]`).

### conda installation

there is a community-supported conda build of `jax`. to install using `conda`,
simply run

```bash
conda install jax -c conda-forge
```

to install on a machine with an nvidia gpu, run
```bash
conda install jax cuda-nvcc -c conda-forge -c nvidia
```

note the `cudatoolkit` distributed by `conda-forge` is missing `ptxas`, which
jax requires. you must therefore either install the `cuda-nvcc` package from
the `nvidia` channel, or install cuda on your machine separately so that `ptxas`
is in your path. the channel order above is important (`conda-forge` before
`nvidia`). we are working on simplifying this.

if you would like to override which release of cuda is used by jax, or to
install the cuda build on a machine without gpus, follow the instructions in the
[tips & tricks](https://conda-forge.org/docs/user/tipsandtricks.html#installing-cuda-enabled-packages-like-tensorflow-and-pytorch)
section of the `conda-forge` website.

see the `conda-forge`
[jaxlib](https://github.com/conda-forge/jaxlib-feedstock#installing-jaxlib) and
[jax](https://github.com/conda-forge/jax-feedstock#installing-jax) repositories
for more details.

### building jax from source
see [building jax from
source](https://jax.readthedocs.io/en/latest/developer.html#building-from-source).

## neural network libraries

multiple google research groups develop and share libraries for training neural
networks in jax. if you want a fully featured library for neural network
training with examples and how-to guides, try
[flax](https://github.com/google/flax).

in addition, deepmind has open-sourced an [ecosystem of libraries around
jax](https://deepmind.com/blog/article/using-jax-to-accelerate-our-research)
including [haiku](https://github.com/deepmind/dm-haiku) for neural network
modules, [optax](https://github.com/deepmind/optax) for gradient processing and
optimization, [rlax](https://github.com/deepmind/rlax) for rl algorithms, and
[chex](https://github.com/deepmind/chex) for reliable code and testing. (watch
the neurips 2020 jax ecosystem at deepmind talk
[here](https://www.youtube.com/watch?v=idxjxiyzsim))

## citing jax

to cite this repository:

```
@software{jax2018github,
  author = {james bradbury and roy frostig and peter hawkins and matthew james johnson and chris leary and dougal maclaurin and george necula and adam paszke and jake vander{p}las and skye wanderman-{m}ilne and qiao zhang},
  title = {{jax}: composable transformations of {p}ython+{n}um{p}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}
```

in the above bibtex entry, names are in alphabetical order, the version number
is intended to be that from [jax/version.py](../main/jax/version.py), and
the year corresponds to the project's open-source release.

a nascent version of jax, supporting only automatic differentiation and
compilation to xla, was described in a [paper that appeared at sysml
2018](https://mlsys.org/conferences/2019/doc/2018/146.pdf). we're currently working on
covering jax's ideas and capabilities in a more comprehensive and up-to-date
paper.

## reference documentation

for details about the jax api, see the
[reference documentation](https://jax.readthedocs.io/).

for getting started as a jax developer, see the
[developer documentation](https://jax.readthedocs.io/en/latest/developer.html).
"
"scikit-multiflow","<img src=""https://raw.githubusercontent.com/scikit-multiflow/scikit-multiflow/master/docs/_static/images/skmultiflow-logo-wide.png"" height=""100""/>

[![build status](https://travis-ci.org/scikit-multiflow/scikit-multiflow.svg?branch=master)](https://travis-ci.org/scikit-multiflow/scikit-multiflow)
[![build status](https://dev.azure.com/scikit-multiflow/scikit-multiflow/_apis/build/status/scikit-multiflow.scikit-multiflow?branchname=master)](https://dev.azure.com/scikit-multiflow/scikit-multiflow/_build/latest?definitionid=1&branchname=master)
[![codecov](https://codecov.io/gh/scikit-multiflow/scikit-multiflow/branch/master/graph/badge.svg)](https://codecov.io/gh/scikit-multiflow/scikit-multiflow)
![python version](https://img.shields.io/badge/python-3.5%20%7c%203.6%20%7c%203.7%20%7c%203.8-blue.svg)
[![anaconda-server badge](https://anaconda.org/conda-forge/scikit-multiflow/badges/platforms.svg)](https://anaconda.org/conda-forge/scikit-multiflow)
[![pypi version](https://badge.fury.io/py/scikit-multiflow.svg)](https://badge.fury.io/py/scikit-multiflow)
[![anaconda-server badge](https://anaconda.org/conda-forge/scikit-multiflow/badges/version.svg)](https://anaconda.org/conda-forge/scikit-multiflow)
[![dockerhub](https://img.shields.io/badge/docker-available-blue.svg?logo=docker)](https://hub.docker.com/r/skmultiflow/scikit-multiflow)
[![license](https://img.shields.io/badge/license-bsd%203--clause-blue.svg)](https://opensource.org/licenses/bsd-3-clause)
[![gitter](https://badges.gitter.im/scikit-multiflow/community.svg)](https://gitter.im/scikit-multiflow/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

`scikit-multiflow` is a machine learning package for streaming data in python.

---

[creme](https://maxhalford.github.io/) and [scikit-multiflow](https://scikit-multiflow.github.io/) are merging into a new project called [river](https://github.com/online-ml/river/).

we feel that both projects share the same vision. we believe that pooling our resources instead of duplicating work will benefit both sides. we are also confident that this will benefit both communities. there will be more people working on the new project, which will allow us to distribute work more efficiently. we will thus be able to work on more features and improve the overall quality of the project.

both projects will stop active development. the code for both projects will remain publicly available, although development will only focus on minor maintenance during a transition period. the architecture of the new package is very similar to that of creme. it will focus on single-instance incremental models.

we encourage users to use river instead of creme. we understand that this transition will require an extra effort in the short term from current users. however, we believe that the result will be better for everyone in the long run.

you will still be able to install and use `creme` as well as `scikit-multiflow`. both projects will remain on pypi, conda-forge and github.

---

### quick links
* [webpage](https://scikit-multiflow.github.io/)
* [documentation](https://scikit-multiflow.readthedocs.io/en/stable/)
* [community](https://scikit-multiflow.github.io/community/)

# features

### incremental learning
stream learning models are created incrementally and are updated continuously. they are suitable
for big data applications where real-time response is vital.

### adaptive learning
changes in data distribution harm learning. adaptive methods are specifically designed to be
robust to concept drift changes in dynamic environments.

### resource-wise efficient
streaming techniques efficiently handle resources such as memory and processing time given the
unbounded nature of data streams. 

### easy to use
scikit-multiflow is designed for users with any experience level. experiments are easy to design,
setup, and run. existing methods are easy to modify and extend.

### stream learning tools
in its current state, scikit-multiflow contains data generators, multi-output/multi-target stream
learning methods, change detection methods, evaluation methods, and more.

### open source
distributed under the 
[bsd 3-clause](https://github.com/scikit-multiflow/scikit-multiflow/blob/master/license), 
`scikit-multiflow` is developed and maintained by an active, diverse and growing [community](/community).

# use cases
the following tasks are supported in `scikit-multiflow`:

### supervised learning
when working with labeled data. depending on the target type can be either classification
(discrete values) or regression (continuous values)

### single/multi output
single-output methods predict a single target-label (binary or multi-class) for classification or
a single target-value for regression. multi-output methods simultaneously predict multiple
variables given an input.

### concept drift detection
changes in data distribution can harm learning. drift detection methods are designed to rise an
alarm in the presence of drift and are used alongside learning methods to improve their robustness
against this phenomenon in evolving data streams.

### unsupervised learning
when working with unlabeled data. for example, anomaly detection where the goal is the
identification of rare events or samples which differ significantly from the majority of the data.

---

#### jupyter notebooks
in order to display plots from `scikit-multiflow` within a [jupyter notebook]() we need to define
the proper mathplotlib backend to use. this is done by including the following magic command at the
beginning of the notebook:

```python
%matplotlib notebook
```

[jupyterlab](http://jupyterlab.readthedocs.io/en/stable/) is the next-generation user interface
for jupyter, currently in beta, it can display interactive plots with some caveats. if you use
jupyterlab then the current solution is to use the 
[jupyter-matplotlib](https://github.com/matplotlib/jupyter-matplotlib) extension:

```python
%matplotlib widget
```

## citing `scikit-multiflow`

if `scikit-multiflow` has been useful for your research and you would like to cite it in a academic
publication, please use the following bibtex entry:

```bibtex
@article{skmultiflow,
  author  = {jacob montiel and jesse read and albert bifet and talel abdessalem},
  title   = {scikit-multiflow: a multi-output streaming framework },
  journal = {journal of machine learning research},
  year    = {2018},
  volume  = {19},
  number  = {72},
  pages   = {1-5},
  url     = {http://jmlr.org/papers/v19/18-251.html}
}
```
"
"bayeso","<p align=""center"">
<img src=""docs/_static/assets/logo_bayeso_capitalized.svg"" width=""400"" />
</p>

# bayeso: a bayesian optimization framework in python
[![build status](https://github.com/jungtaekkim/bayeso/actions/workflows/pytest.yml/badge.svg)](https://github.com/jungtaekkim/bayeso/actions/workflows/pytest.yml)
[![coverage status](https://coveralls.io/repos/github/jungtaekkim/bayeso/badge.svg?branch=main)](https://coveralls.io/github/jungtaekkim/bayeso?branch=main)
[![pypi - python version](https://img.shields.io/pypi/pyversions/bayeso)](https://pypi.org/project/bayeso/)
[![license: mit](https://img.shields.io/badge/license-mit-yellow.svg)](https://opensource.org/licenses/mit)
[![documentation status](https://readthedocs.org/projects/bayeso/badge/?version=main)](https://bayeso.readthedocs.io/en/main/?badge=main)

<p align=""center"">
<img src=""docs/_static/steps/ei.gif"" width=""600"" />
</p>

simple, but essential bayesian optimization package.

* [https://bayeso.org](https://bayeso.org)
* [online documentation](https://bayeso.readthedocs.io)

## installation
we recommend installing it with `virtualenv`.
you can choose one of three installation options.

* using pypi repository (for user installation)

to install the released version in pypi repository, command it.

```shell
$ pip install bayeso
```

* using source code (for developer installation)

to install `bayeso` from source code, command

```shell
$ pip install .
```
in the `bayeso` root.

* using source code (for editable development mode)

to use editable development mode, command

```shell
$ pip install -r requirements.txt
$ python setup.py develop
```
in the `bayeso` root.

* uninstallation

if you would like to uninstall `bayeso`, command it.

```shell
$ pip uninstall bayeso
```

## required packages
mandatory pacakges are inlcuded in `requirements.txt`.
the following `requirements` files include the package list, the purpose of which is described as follows.

* `requirements-optional.txt`: it is an optional package list, but it needs to be installed to execute some features of `bayeso`.
* `requirements-dev.txt`: it is for developing the `bayeso` package.
* `requirements-examples.txt`: it needs to be installed to execute the examples included in the `bayeso` repository.

## supported python version
we test our package in the following versions.

* python 3.6
* python 3.7
* python 3.8
* python 3.9
* python 3.10

## contributor
* [jungtaek kim](https://jungtaek.github.io)

## citation
```
@misc{kimj2017bayeso,
    author={kim, jungtaek and choi, seungjin},
    title={{bayeso}: a {bayesian} optimization framework in {python}},
    howpublished={\url{https://bayeso.org}},
    year={2017}
}
```

## contact
* [jungtaek kim](https://jungtaek.github.io)

## license
[mit license](license)
"
"Determined","# determined: deep learning training platform

<p align=""center""><img src=""determined-logo.png"" alt=""determined ai logo""></p>

determined is an open-source deep learning training platform that makes building
models fast and easy. determined enables you to:

* **train models faster** using state-of-the-art distributed training, without
  changing your model code
* **automatically find high-quality models** with advanced hyperparameter tuning
  from the creators of hyperband
* **get more from your gpus** with smart scheduling and cut cloud gpu costs by
  seamlessly using preemptible instances
* **track and reproduce your work** with experiment tracking that works
  out-of-the-box, covering code versions, metrics, checkpoints, and
  hyperparameters

determined integrates these features into an easy-to-use, high-performance deep
learning environment ‚Äî which means you can spend your time building models
instead of managing infrastructure.

to use determined, you can continue using popular dl frameworks such as
tensorflow and pytorch; you just need to update your model code to integrate
with the determined api.

## try out determined locally

follow [these instructions](https://docs.determined.ai/latest/how-to/installation/requirements.html#install-docker) to install and set up docker.

 ```bash

# start a determined cluster locally.
python3.7 -m venv ~/.virtualenvs/test
. ~/.virtualenvs/test/bin/activate
pip install determined
# to start a cluster with gpus, remove `no-gpu` flag.
det deploy local cluster-up --no-gpu
# access web ui at localhost:8080. by default, ""determined"" user accepts a blank password.

# navigate to a determined example.
git clone --recurse-submodules https://github.com/determined-ai/determined
cd determined/examples/computer_vision/cifar10_pytorch

# submit job to train a single model on a single node.
det experiment create const.yaml .
 ```

## detailed installation guide

see [our installation guide](https://docs.determined.ai/latest/how-to/install-main.html) for details on how to install determined, including on aws and gcp.

### try now on aws

[![try now](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/create/review?templateurl=https://determined-ai-public.s3-us-west-2.amazonaws.com/simple.yaml)

## next steps

for a brief introduction to using determined, check out our
[quick start guide](https://docs.determined.ai/latest/getting-started.html).

to use an existing deep learning model with determined, follow the
tutorial for your preferred deep learning framework:

* [pytorch mnist tutorial](https://docs.determined.ai/latest/tutorials/pytorch-mnist-tutorial.html)
* [tensorflow keras mnist tutorial](https://docs.determined.ai/latest/tutorials/tf-mnist-tutorial.html)

## documentation

the documentation for the latest version of determined can always be found
[here](https://docs.determined.ai).

## community

if you need help, want to file a bug report, or just want to keep up-to-date
with the latest news about determined, please join the determined community!

* [slack](https://determined-community.slack.com) is the best place to
  ask questions about determined and get support. [click here to join our slack](
  https://join.slack.com/t/determined-community/shared_invite/zt-cnj7802v-kcvbaurizqowmkmy7gp0ew).
* you can also join the [community mailing list](https://groups.google.com/a/determined.ai/forum/#!forum/community)
  to ask questions about the project and receive announcements.
* to report a bug, [file an issue](https://github.com/determined-ai/determined/issues) on github.
* to report a security issue, email [`security@determined.ai`](mailto:security@determined.ai).

## contributing

[contributor's guide](contributing.md)

## license

[apache v2](license)
"
"sktime","<a href=""https://sktime.org""><img src=""https://github.com/sktime/sktime/blob/main/docs/source/images/sktime-logo.jpg?raw=true)"" width=""175"" align=""right"" /></a>

# welcome to sktime

> a unified interface for machine learning with time series

:rocket: **version 0.16.0 out now!** [check out the release notes here](https://www.sktime.org/en/latest/changelog.html).

sktime is a library for time series analysis in python. it provides a unified interface for multiple time series learning tasks. currently, this includes time series classification, regression, clustering, annotation and forecasting. it comes with [time series algorithms](https://www.sktime.org/en/stable/estimator_overview.html) and [scikit-learn] compatible tools to build, tune and validate time series models.

[scikit-learn]: https://scikit-learn.org/stable/

| overview | |
|---|---|
| **ci/cd** | [![github-actions](https://img.shields.io/github/actions/workflow/status/sktime/sktime/wheels.yml?logo=github)](https://github.com/sktime/sktime/actions/workflows/wheels.yml) [![!codecov](https://img.shields.io/codecov/c/github/sktime/sktime?label=codecov&logo=codecov)](https://codecov.io/gh/sktime/sktime) [![readthedocs](https://img.shields.io/readthedocs/sktime?logo=readthedocs)](https://www.sktime.org/en/latest/?badge=latest) [![platform](https://img.shields.io/conda/pn/conda-forge/sktime)](https://github.com/sktime/sktime) |
| **code** |  [![!pypi](https://img.shields.io/pypi/v/sktime?color=orange)](https://pypi.org/project/sktime/) [![!conda](https://img.shields.io/conda/vn/conda-forge/sktime)](https://anaconda.org/conda-forge/sktime) [![!python-versions](https://img.shields.io/pypi/pyversions/sktime)](https://www.python.org/) [![!black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sktime/sktime/main?filepath=examples) |
| **downloads**| [![downloads](https://static.pepy.tech/personalized-badge/sktime?period=week&units=international_system&left_color=grey&right_color=blue&left_text=weekly%20(pypi))](https://pepy.tech/project/sktime) [![downloads](https://static.pepy.tech/personalized-badge/sktime?period=month&units=international_system&left_color=grey&right_color=blue&left_text=monthly%20(pypi))](https://pepy.tech/project/sktime) [![downloads](https://static.pepy.tech/personalized-badge/sktime?period=total&units=international_system&left_color=grey&right_color=blue&left_text=cumulative%20(pypi))](https://pepy.tech/project/sktime) |
| **community** | [![!slack](https://img.shields.io/static/v1?logo=slack&label=slack&message=chat&color=lightgreen)](https://join.slack.com/t/sktime-group/shared_invite/zt-1cghagwee-sqlj~ehwgygzwbqux937ig) [![!discord](https://img.shields.io/static/v1?logo=discord&label=discord&message=chat&color=lightgreen)](https://discord.com/invite/gqsab2k) [![!slack](https://img.shields.io/static/v1?logo=linkedin&label=linkedin&message=news&color=lightblue)](https://www.linkedin.com/company/sktime/) [![!twitter](https://img.shields.io/static/v1?logo=twitter&label=twitter&message=news&color=lightblue)](https://twitter.com/sktime_toolbox) [![!youtube](https://img.shields.io/static/v1?logo=youtube&label=youtube&message=tutorials&color=red)](https://www.youtube.com/playlist?list=plks3uggjlwhqnzu0leoelkvnjvvest2d0) |
| **citation** | [![!zenodo](https://zenodo.org/badge/doi/10.5281/zenodo.3749000.svg)](https://doi.org/10.5281/zenodo.3749000) |

## :books: documentation

| documentation              |                                                                |
| -------------------------- | -------------------------------------------------------------- |
| :star: **[tutorials]**        | new to sktime? here's everything you need to know!              |
| :clipboard: **[binder notebooks]** | example notebooks to play with in your browser.              |
| :woman_technologist: **[user guides]**      | how to use sktime and its features.                             |
| :scissors: **[extension templates]** | how to build your own estimator using sktime's api.            |
| :control_knobs: **[api reference]**      | the detailed reference for sktime's api.                        |
| :tv: **[video tutorial]**            | our video tutorial from 2021 pydata global.      |
| :hammer_and_wrench: **[changelog]**          | changes and version history.                                   |
| :deciduous_tree: **[roadmap]**          | sktime's software and community development plan.                                   |
| :pencil: **[related software]**          | a list of related software. |

[tutorials]: https://www.sktime.org/en/latest/tutorials.html
[binder notebooks]: https://mybinder.org/v2/gh/sktime/sktime/main?filepath=examples
[user guides]: https://www.sktime.org/en/latest/user_guide.html
[video tutorial]: https://github.com/sktime/sktime-tutorial-pydata-global-2021
[api reference]: https://www.sktime.org/en/latest/api_reference.html
[changelog]: https://www.sktime.org/en/latest/changelog.html
[roadmap]: https://www.sktime.org/en/latest/roadmap.html
[related software]: https://www.sktime.org/en/latest/related_software.html

## :speech_balloon: where to ask questions

questions and feedback are extremely welcome! please understand that we won't be able to provide individual support via email. we also believe that help is much more valuable if it's shared publicly, so that more people can benefit from it.

| type                            | platforms                               |
| ------------------------------- | --------------------------------------- |
| :bug: **bug reports**              | [github issue tracker]                  |
| :sparkles: **feature requests & ideas** | [github issue tracker]                       |
| :woman_technologist: **usage questions**          | [github discussions] ¬∑ [stack overflow] |
| :speech_balloon: **general discussion**        | [github discussions] |
| :factory: **contribution & development** | [slack], contributors channel ¬∑ [discord] |
| :globe_with_meridians: **community collaboration session** | [discord] - fridays 1pm utc, dev/meet-ups channel |

[github issue tracker]: https://github.com/sktime/sktime/issues
[github discussions]: https://github.com/sktime/sktime/discussions
[stack overflow]: https://stackoverflow.com/questions/tagged/sktime
[discord]: https://discord.com/invite/gqsab2k
[slack]: https://join.slack.com/t/sktime-group/shared_invite/zt-1cghagwee-sqlj~ehwgygzwbqux937ig

## :dizzy: features
our aim is to make the time series analysis ecosystem more interoperable and usable as a whole. sktime provides a __unified interface for distinct but related time series learning tasks__. it features [__dedicated time series algorithms__](https://www.sktime.org/en/stable/estimator_overview.html) and __tools for composite model building__ including pipelining, ensembling, tuning and reduction that enables users to apply an algorithm for one task to another.

sktime also provides **interfaces to related libraries**, for example [scikit-learn], [statsmodels], [tsfresh], [pyod] and [fbprophet], among others.

for **deep learning**, see our companion package: [sktime-dl](https://github.com/sktime/sktime-dl).

[statsmodels]: https://www.statsmodels.org/stable/index.html
[tsfresh]: https://tsfresh.readthedocs.io/en/latest/
[pyod]: https://pyod.readthedocs.io/en/latest/
[fbprophet]: https://facebook.github.io/prophet/

| module | status | links |
|---|---|---|
| **[forecasting]** | stable | [tutorial](https://www.sktime.org/en/latest/examples/01_forecasting.html) ¬∑ [api reference](https://www.sktime.org/en/latest/api_reference.html#sktime-forecasting-time-series-forecasting) ¬∑ [extension template](https://github.com/sktime/sktime/blob/main/extension_templates/forecasting.py)  |
| **[time series classification]** | stable | [tutorial](https://github.com/sktime/sktime/blob/main/examples/02_classification.ipynb) ¬∑ [api reference](https://www.sktime.org/en/latest/api_reference.html#sktime-classification-time-series-classification) ¬∑ [extension template](https://github.com/sktime/sktime/blob/main/extension_templates/classification.py) |
| **[time series regression]** | stable | [api reference](https://www.sktime.org/en/latest/api_reference.html#sktime-classification-time-series-regression) |
| **[transformations]** | stable | [api reference](https://www.sktime.org/en/latest/api_reference.html#sktime-transformations-time-series-transformers) ¬∑ [extension template](https://github.com/sktime/sktime/blob/main/extension_templates/transformer.py)  |
| **[time series clustering]** | maturing | [extension template](https://github.com/sktime/sktime/blob/main/extension_templates/clustering.py) |
| **[time series distances/kernels]** | experimental | [extension template](https://github.com/sktime/sktime/blob/main/extension_templates/dist_kern_panel.py) |
| **[annotation]** | experimental | [extension template](https://github.com/sktime/sktime/blob/main/extension_templates/annotation.py) |

[forecasting]: https://github.com/sktime/sktime/tree/main/sktime/forecasting
[time series classification]: https://github.com/sktime/sktime/tree/main/sktime/classification
[time series regression]: https://github.com/sktime/sktime/tree/main/sktime/regression
[time series clustering]: https://github.com/sktime/sktime/tree/main/sktime/clustering
[annotation]: https://github.com/sktime/sktime/tree/main/sktime/annotation
[time series distances/kernels]: https://github.com/sktime/sktime/tree/main/sktime/dists_kernels
[transformations]: https://github.com/sktime/sktime/tree/main/sktime/transformations


## :hourglass_flowing_sand: install sktime
for trouble shooting and detailed installation instructions, see the [documentation](https://www.sktime.org/en/latest/installation.html).

- **operating system**: macos x ¬∑ linux ¬∑ windows 8.1 or higher
- **python version**: python 3.7, 3.8, 3.9, and 3.10 (only 64 bit)
- **package managers**: [pip] ¬∑ [conda] (via `conda-forge`)

[pip]: https://pip.pypa.io/en/stable/
[conda]: https://docs.conda.io/en/latest/

### pip
using pip, sktime releases are available as source packages and binary wheels. you can see all available wheels [here](https://pypi.org/simple/sktime/).

```bash
pip install sktime
```

or, with maximum dependencies,

```bash
pip install sktime[all_extras]
```

### conda
you can also install sktime from `conda` via the `conda-forge` channel. for the feedstock including the build recipe and configuration, check out [this repository](https://github.com/conda-forge/sktime-feedstock).

```bash
conda install -c conda-forge sktime
```

or, with maximum dependencies,

```bash
conda install -c conda-forge sktime-all-extras
```

## :zap: quickstart

### forecasting

```python
from sktime.datasets import load_airline
from sktime.forecasting.base import forecastinghorizon
from sktime.forecasting.model_selection import temporal_train_test_split
from sktime.forecasting.theta import thetaforecaster
from sktime.performance_metrics.forecasting import mean_absolute_percentage_error

y = load_airline()
y_train, y_test = temporal_train_test_split(y)
fh = forecastinghorizon(y_test.index, is_relative=false)
forecaster = thetaforecaster(sp=12)  # monthly seasonal periodicity
forecaster.fit(y_train)
y_pred = forecaster.predict(fh)
mean_absolute_percentage_error(y_test, y_pred)
>>> 0.08661467738190656
```

### time series classification

```python
from sktime.classification.interval_based import timeseriesforestclassifier
from sktime.datasets import load_arrow_head
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

x, y = load_arrow_head()
x_train, x_test, y_train, y_test = train_test_split(x, y)
classifier = timeseriesforestclassifier()
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
accuracy_score(y_test, y_pred)
>>> 0.8679245283018868
```

## :wave: how to get involved

there are many ways to join the sktime community. we follow the [all-contributors](https://github.com/all-contributors/all-contributors) specification: all kinds of contributions are welcome - not just code.

| documentation              |                                                                |
| -------------------------- | --------------------------------------------------------------        |
| :gift_heart: **[contribute]**        | how to contribute to sktime.          |
| :school_satchel:  **[mentoring]** | new to open source? apply to our mentoring program! |
| :date: **[meetings]** | join our discussions, tutorials, workshops and sprints! |
| :woman_mechanic:  **[developer guides]**      | how to further develop sktime's code base.                             |
| :construction: **[enhancement proposals]** | design a new feature for sktime. |
| :medal_sports: **[contributors]** | a list of all contributors. |
| :raising_hand: **[roles]** | an overview of our core community roles. |
| :money_with_wings: **[donate]** | fund sktime maintenance and development. |
| :classical_building: **[governance]** | how and by whom decisions are made in sktime's community.   |

[contribute]: https://www.sktime.org/en/latest/get_involved/contributing.html
[donate]: https://opencollective.com/sktime
[extension templates]: https://github.com/sktime/sktime/tree/main/extension_templates
[developer guides]: https://www.sktime.org/en/latest/developer_guide.html
[contributors]: https://github.com/sktime/sktime/blob/main/contributors.md
[governance]: https://www.sktime.org/en/latest/governance.html
[mentoring]: https://github.com/sktime/mentoring
[meetings]: https://calendar.google.com/calendar/u/0/embed?src=sktime.toolbox@gmail.com&ctz=utc
[enhancement proposals]: https://github.com/sktime/enhancement-proposals
[roles]: https://www.sktime.org/en/latest/about/team.html

## :bulb: project vision

* **by the community, for the community** -- developed by a friendly and collaborative community.
* the **right tool for the right task** -- helping users to diagnose their learning problem and suitable scientific model types.
* **embedded in state-of-art ecosystems** and **provider of interoperable interfaces** -- interoperable with [scikit-learn], [statsmodels], [tsfresh], and other community favourites.
* **rich model composition and reduction functionality** -- build tuning and feature extraction pipelines, solve forecasting tasks with [scikit-learn] regressors.
* **clean, descriptive specification syntax** -- based on modern object-oriented design principles for data science.
* **fair model assessment and benchmarking** -- build your models, inspect your models, check your models, avoid pitfalls.
* **easily extensible** -- easy extension templates to add your own algorithms compatible with sktime's api.
"
"Gradio","<!-- do not edit this file directly. instead edit the `readme_template.md` or `guides/1)getting_started/1)quickstart.md` templates and then run `render_readme.py` script. -->

<div align=""center"">

  [<img src=""readme_files/gradio.svg"" alt=""gradio"" width=300>](https://gradio.app)<br>
  <em>build & share delightful machine learning apps easily</em>

  [<img src=""https://circleci.com/gh/gradio-app/gradio.svg?style=svg"" alt=""circleci"">](https://circleci.com/gh/gradio-app/gradio)
  [<img src=""https://codecov.io/gh/gradio-app/gradio/branch/master/graph/badge.svg"" alt=""codecov"">](https://app.codecov.io/gh/gradio-app/gradio)
  [![pypi](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)
  [![pypi downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)
  ![python version](https://img.shields.io/badge/python-3.7+-important)
  [![twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)

  [website](https://gradio.app)
  | [documentation](https://gradio.app/docs/)
  | [guides](https://gradio.app/guides/)
  | [getting started](https://gradio.app/getting_started/)
  | [examples](demo/)
</div>

# gradio: build machine learning web apps ‚Äî in python

gradio is an open-source python library that is used to build machine learning and data science demos and web applications.

with gradio, you can quickly create a beautiful user interface around your machine learning models or data science workflow and let people ""try it out"" by dragging-and-dropping in their own images,
pasting text, recording their own voice, and interacting with your demo, all through the browser.

![interface montage](readme_files/header-image.jpg)

gradio is useful for:

- **demoing** your machine learning models for clients/collaborators/users/students.

- **deploying** your models quickly with automatic shareable links and getting feedback on model performance.

- **debugging** your model interactively during development using built-in manipulation and interpretation tools.

## quickstart

**prerequisite**: gradio requires python 3.7 or higher, that's all!

### what does gradio do?

one of the *best ways to share* your machine learning model, api, or data science workflow with others is to create an **interactive app** that allows your users or colleagues to try out the demo in their browsers.

gradio allows you to **build demos and share them, all in python.** and usually in just a few lines of code! so let's get started.

### hello, world

to get gradio running with a simple ""hello, world"" example, follow these three steps:

1\. install gradio using pip:

```bash
pip install gradio
```

2\. run the code below as a python script or in a jupyter notebook (or [google colab](https://colab.research.google.com/drive/18odkjvyxhuttn0p5apwygfo_xwncghdz?usp=sharing)):

```python
import gradio as gr

def greet(name):
    return ""hello "" + name + ""!""

demo = gr.interface(fn=greet, inputs=""text"", outputs=""text"")
demo.launch()
```


3\. the demo below will appear automatically within the jupyter notebook, or pop in a browser on [http://localhost:7860](http://localhost:7860) if running from a script:

![`hello_world` demo](demo/hello_world/screenshot.gif)

### the `interface` class

you'll notice that in order to make the demo, we created a `gradio.interface`. this `interface` class can wrap any python function with a user interface. in the example above, we saw a simple text-based function, but the function could be anything from music generator to a tax calculator to the prediction function of a pretrained machine learning model.

the core `interface` class is initialized with three required parameters:

- `fn`: the function to wrap a ui around
- `inputs`: which component(s) to use for the input (e.g. `""text""`, `""image""` or `""audio""`)
- `outputs`: which component(s) to use for the output (e.g. `""text""`, `""image""` or `""label""`)

let's take a closer look at these components used to provide input and output.

### components attributes

we saw some simple `textbox` components in the previous examples, but what if you want to change how the ui components look or behave?

let's say you want to customize the input text field ‚Äî for example, you wanted it to be larger and have a text placeholder. if we use the actual class for `textbox` instead of using the string shortcut, you have access to much more customizability through component attributes.

```python
import gradio as gr

def greet(name):
    return ""hello "" + name + ""!""

demo = gr.interface(
    fn=greet,
    inputs=gr.textbox(lines=2, placeholder=""name here...""),
    outputs=""text"",
)
demo.launch()
```

![`hello_world_2` demo](demo/hello_world_2/screenshot.gif)

### multiple input and output components

suppose you had a more complex function, with multiple inputs and outputs. in the example below, we define a function that takes a string, boolean, and number, and returns a string and number. take a look how you pass a list of input and output components.

```python
import gradio as gr

def greet(name, is_morning, temperature):
    salutation = ""good morning"" if is_morning else ""good evening""
    greeting = f""{salutation} {name}. it is {temperature} degrees today""
    celsius = (temperature - 32) * 5 / 9
    return greeting, round(celsius, 2)

demo = gr.interface(
    fn=greet,
    inputs=[""text"", ""checkbox"", gr.slider(0, 100)],
    outputs=[""text"", ""number""],
)
demo.launch()
```

![`hello_world_3` demo](demo/hello_world_3/screenshot.gif)

you simply wrap the components in a list. each component in the `inputs` list corresponds to one of the parameters of the function, in order. each component in the `outputs` list corresponds to one of the values returned by the function, again in order.

### an image example

gradio supports many types of components, such as `image`, `dataframe`, `video`, or `label`. let's try an image-to-image function to get a feel for these!

```python
import numpy as np
import gradio as gr

def sepia(input_img):
    sepia_filter = np.array([
        [0.393, 0.769, 0.189], 
        [0.349, 0.686, 0.168], 
        [0.272, 0.534, 0.131]
    ])
    sepia_img = input_img.dot(sepia_filter.t)
    sepia_img /= sepia_img.max()
    return sepia_img

demo = gr.interface(sepia, gr.image(shape=(200, 200)), ""image"")
demo.launch()
```

![`sepia_filter` demo](demo/sepia_filter/screenshot.gif)

when using the `image` component as input, your function will receive a numpy array with the shape `(width, height, 3)`, where the last dimension represents the rgb values. we'll return an image as well in the form of a numpy array.

you can also set the datatype used by the component with the `type=` keyword argument. for example, if you wanted your function to take a file path to an image instead of a numpy array, the input `image` component could be written as:

```python
gr.image(type=""filepath"", shape=...)
```

also note that our input `image` component comes with an edit button üñâ, which allows for cropping and zooming into images. manipulating images in this way can help reveal biases or hidden flaws in a machine learning model!

you can read more about the many components and how to use them in the [gradio docs](https://gradio.app/docs).

### blocks: more flexibility and control

gradio offers two classes to build apps:

1\. **interface**, that provides a high-level abstraction for creating demos that we've been discussing so far.

2\. **blocks**, a low-level api for designing web apps with more flexible layouts and data flows. blocks allows you to do things like feature multiple data flows and demos, control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in python. if this customizability is what you need, try `blocks` instead!

### hello, blocks

let's take a look at a simple example. note how the api here differs from `interface`.

```python
import gradio as gr

def greet(name):
    return ""hello "" + name + ""!""

with gr.blocks() as demo:
    name = gr.textbox(label=""name"")
    output = gr.textbox(label=""output box"")
    greet_btn = gr.button(""greet"")
    greet_btn.click(fn=greet, inputs=name, outputs=output)

demo.launch()
```

![`hello_blocks` demo](demo/hello_blocks/screenshot.gif)

things to note:

- `blocks` are made with a `with` clause, and any component created inside this clause is automatically added to the app.
- components appear vertically in the app in the order they are created. (later we will cover customizing layouts!)
- a `button` was created, and then a `click` event-listener was added to this button. the api for this should look familiar! like an `interface`, the `click` method takes a python function, input components, and output components.

### more complexity

here's an app to give you a taste of what's possible with `blocks`:

```python
import numpy as np
import gradio as gr

def flip_text(x):
    return x[::-1]

def flip_image(x):
    return np.fliplr(x)

with gr.blocks() as demo:
    gr.markdown(""flip text or image files using this demo."")
    with gr.tabs():
        with gr.tabitem(""flip text""):
            text_input = gr.textbox()
            text_output = gr.textbox()
            text_button = gr.button(""flip"")
        with gr.tabitem(""flip image""):
            with gr.row():
                image_input = gr.image()
                image_output = gr.image()
            image_button = gr.button(""flip"")
    
    text_button.click(flip_text, inputs=text_input, outputs=text_output)
    image_button.click(flip_image, inputs=image_input, outputs=image_output)
    
demo.launch()
```

![`blocks_flipper` demo](demo/blocks_flipper/screenshot.gif)

a lot more going on here! we'll cover how to create complex `blocks` apps like this in the [building with blocks](https://github.com/gradio-app/gradio/tree/main/guides/3\)building_with_blocks) section for you.

congrats, you're now familiar with the basics of gradio! ü•≥ go to our [next guide](https://gradio.app/key_features) to learn more about the key features of gradio.


## open source stack

gradio is built with many wonderful open-source libraries, please support them as well!

[<img src=""readme_files/huggingface_mini.svg"" alt=""huggingface"" height=40>](https://huggingface.co)
[<img src=""readme_files/python.svg"" alt=""python"" height=40>](https://www.python.org)
[<img src=""readme_files/fastapi.svg"" alt=""fastapi"" height=40>](https://fastapi.tiangolo.com)
[<img src=""readme_files/encode.svg"" alt=""encode"" height=40>](https://www.encode.io)
[<img src=""readme_files/svelte.svg"" alt=""svelte"" height=40>](https://svelte.dev)
[<img src=""readme_files/vite.svg"" alt=""vite"" height=40>](https://vitejs.dev)
[<img src=""readme_files/pnpm.svg"" alt=""pnpm"" height=40>](https://pnpm.io)
[<img src=""readme_files/tailwind.svg"" alt=""tailwind"" height=40>](https://tailwindcss.com)

## license

gradio is licensed under the apache license 2.0 found in the [license](license) file in the root directory of this repository.

## citation

also check out the paper *[gradio: hassle-free sharing and testing of ml models in the wild](https://arxiv.org/abs/1906.02569), icml hill 2019*, and please cite it if you use gradio in your work.

```
@article{abid2019gradio,
  title = {gradio: hassle-free sharing and testing of ml models in the wild},
  author = {abid, abubakar and abdalla, ali and abid, ali and khan, dawood and alfozan, abdulrahman and zou, james},
  journal = {arxiv preprint arxiv:1906.02569},
  year = {2019},
}
```

## see also

* the [gradio discord bot](https://github.com/gradio-app/gradio-discord-bot), a discord bot that allows you to try any [hugging face space](https://huggingface.co/spaces) that is running a gradio demo as a discord bot.
"
"Hub","### we are sunsetting hub 1.0. 
### hub 2.0, an easier-to-use and highly-performant replacement is available in the [release/2.0](https://github.com/activeloopai/hub/tree/release/2.0) branch. please contact us through our [slack community](http://slack.activeloop.ai/) if you'd like to be granted early access to hub 2.0.

<img src=""https://static.scarf.sh/a.png?x-pxid=96869414-64a7-4e7b-82b2-a15aec2d19d5"" />
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/activeloopai/hub/master/docs/use_hub_in_a_cvpr_challenge_and_win_up_to_$1000_(read_more).png"" height=""35"" /></a>
</p>
<p align=""center"">
    <br>
    <img src=""https://raw.githubusercontent.com/activeloopai/hub/master/docs/logo/logo-explainer-bg.png"" width=""50%""/>
    </br>
<p align=""center"">
    <a href=""http://docs.activeloop.ai/"">
        <img alt=""docs"" src=""https://readthedocs.org/projects/hubdb/badge/?version=latest"">
    </a>
    <a href=""https://pypi.org/project/hub/""><img src=""https://badge.fury.io/py/hub.svg"" alt=""pypi version"" height=""18""></a>
    <a href=""https://pepy.tech/project/hub""><img src=""https://static.pepy.tech/personalized-badge/hub?period=month&units=international_system&left_color=grey&right_color=orange&left_text=downloads"" alt=""pypi version"" height=""18""></a>
    <a href=""https://app.circleci.com/pipelines/github/activeloopai/hub"">
    <img alt=""circleci"" src=""https://img.shields.io/circleci/build/github/activeloopai/hub?logo=circleci""> </a>
     <a href=""https://github.com/activeloopai/hub/issues"">
    <img alt=""github issues"" src=""https://img.shields.io/github/issues/activeloopai/hub""> </a>
    <a href=""https://codecov.io/gh/activeloopai/hub/branch/master""><img src=""https://codecov.io/gh/activeloopai/hub/branch/master/graph/badge.svg"" alt=""codecov"" height=""18""></a>
    <a href=""https://twitter.com/intent/tweet?text=the%20fastest%20way%20to%20access%20and%20manage%20pytorch%20and%20tensorflow%20datasets%20is%20open-source&url=https://activeloop.ai/&via=activeloopai&hashtags=opensource,pytorch,tensorflow,data,datascience,datapipelines,activeloop,dockerhubfordatasets""> 
        <img alt=""tweet"" src=""https://img.shields.io/twitter/url/http/shields.io.svg?style=social""> </a>  
   </br> 
    <a href=""https://join.slack.com/t/hubdb/shared_invite/zt-ivhsj8sz-gwv9c5flbdvw8vn~sxrkqq"">
  <img src=""https://user-images.githubusercontent.com/13848158/97266254-9532b000-1841-11eb-8b06-ed73e99c2e5f.png"" height=""35"" /> </a>
    <a href=""https://docsv1.activeloop.ai/en/latest/"">
  <img src=""https://i.ibb.co/ybtccjc/output-onlinepngtools.png"" height=""35"" /></a>

---

</a>
</p>

<h3 align=""center""> introducing data 2.0, powered by hub. </br>the fastest way to store, access & manage datasets with version-control for pytorch/tensorflow.  works locally or on any cloud. scalable data pipelines.</h3>

---

[ english | [fran√ßais](./readme_translations/readme_fr.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](./readme_translations/readme_cn.md) | [t√ºrk√ße](./readme_translations/readme_tr.md) | [ÌïúÍ∏Ä](./readme_translations/readme_kr.md) | [bahasa indonesia](./readme_translations/readme_id.md)] | [—Ä—É—Å—Å–∫–∏–π](./readme_translations/readme_ru.md)]

<i> note: the translations of this document may not be up-to-date. for the latest version, please check the readme in english. </i>

### what is hub for?

software 2.0 needs data 2.0, and hub delivers it. most of the time data scientists/ml researchers work on data management and preprocessing instead of training models. with hub, we are fixing this. we store your (even petabyte-scale) datasets as single numpy-like array on the cloud, so you can seamlessly access and work with it from any machine. hub makes any data type (images, text files, audio, or video) stored in cloud usable as fast as if it were stored on premise. with same dataset view, your team can always be in sync.

hub is being used by waymo, red cross, world resources institute, omdena, and others.

### features

- store and retrieve large datasets with version-control
- collaborate as in google docs: multiple data scientists working on the same data in sync with no interruptions
- access from multiple machines simultaneously
- deploy anywhere - locally, on google cloud, s3, azure as well as activeloop (by default - and for free!)
- integrate with your ml tools like numpy, dask, ray, [pytorch](https://docs.activeloop.ai/en/latest/integrations/pytorch.html), or [tensorflow](https://docs.activeloop.ai/en/latest/integrations/tensorflow.html)
- create arrays as big as you want. you can store images as big as 100k by 100k!
- keep shape of each sample dynamic. this way you can store small and big arrays as 1 array.
- [visualize](http://app.activeloop.ai/?utm_source=github&utm_medium=repo&utm_campaign=readme) any slice of the data in a matter of seconds without redundant manipulations

 <p align=""center"">
    <br>
    <img src=""https://raw.githubusercontent.com/activeloopai/hub/master/docs/visualizer%20gif.gif"" width=""75%""/>
    </br>
visualization of a dataset uploaded to hub via app.activeloop.ai (free tool).

</p>

## getting started

work with public or your own data, locally or on any cloud.

### access public data. fast.

to load a public dataset, one needs to write dozens of lines of code and spend hours accessing and understanding the api as well as downloading the data. with hub, you only need 2 lines of code, and you **can get started working on your dataset in under 3 minutes**.

```sh
pip3 install hub
```

to be able to download datasets stores on hub platform, you would need to login:
you can register a free account at [activeloop](https://app.activeloop.ai/register/?utm_source=github&utm_medium=repo&utm_campaign=readme) and authenticate locally:

```sh
hub register
hub login

# alternatively, add username and password as arguments (use on platforms like kaggle)
hub login -u username -p password
```

access public datasets in hub by following a straight-forward convention which merely requires a few lines of simple code. run this excerpt to get the first thousand images in the [mnist database](https://app.activeloop.ai/dataset/activeloop/mnist/?utm_source=github&utm_medium=repo&utm_campaign=readme) in the numpy array format:

```python
from hub import dataset

mnist = dataset(""activeloop/mnist"")  # loading the mnist data lazily
# saving time with *compute* to retrieve just the necessary data
mnist[""image""][0:1000].compute()
```

you can find all the other popular datasets on [app.activeloop.ai](https://app.activeloop.ai/datasets/popular/?utm_source=github&utm_medium=repo&utm_campaign=readme).

### train a model

load the data and train your model **directly**. hub is integrated with pytorch and tensorflow and performs conversions between formats in an understandable fashion. take a look at the example with pytorch below:

```python
from hub import dataset
import torch

mnist = dataset(""activeloop/mnist"")
# converting mnist to pytorch format
mnist = mnist.to_pytorch(lambda x: (x[""image""], x[""label""]))

train_loader = torch.utils.data.dataloader(mnist, batch_size=1, num_workers=0)

for image, label in train_loader:
    # define your training loop here
```

### create a local dataset

if you want to work on your own data locally, you can start by creating a dataset:

```python
from hub import dataset, schema
import numpy as np

ds = dataset(
    ""./data/dataset_name"",  # file path to the dataset
    shape = (4,),  # follows numpy shape convention
    mode = ""w+"",  # reading & writing mode
    schema = {  # named blobs of data that may specify types
    # tensor is a generic structure that can contain any type of data
        ""image"": schema.tensor((512, 512), dtype=""float""),
        ""label"": schema.tensor((512, 512), dtype=""float""),
    }
)

# filling the data containers with data (here - zeroes to initialize)
ds[""image""][:] = np.zeros((4, 512, 512))
ds[""label""][:] = np.zeros((4, 512, 512))
ds.flush()  # executing the creation of the dataset
```

you can also specify `s3://bucket/path`, `gcs://bucket/path` or azure path. [here](https://docs.activeloop.ai/en/latest/simple.html#data-storage) you can find more information on cloud storage.
also, if you need a publicly available dataset that you cannot find in the hub, you may [file a request](https://github.com/activeloopai/hub/issues/new?assignees=&labels=i%3a+enhancement%2c+i%3a+needs+triage&template=feature_request.md&title=[feature]+new+dataset+required%3a+%2adataset_name%2a). we will enable it for everyone as soon as we can!

### upload your dataset and access it from <ins>anywhere</ins> in 3 simple steps

1. register a free account at [activeloop](https://app.activeloop.ai/register/?utm_source=github&utm_medium=repo&utm_campaign=readme) and authenticate locally:

   ```sh
   activeloop register
   activeloop login

   # alternatively, add username and password as arguments (use on platforms like kaggle)
   activeloop login -u username -p password
   ```

2. then create a dataset, specifying its name and upload it to your account. for instance:

   ```python
   from hub import dataset, schema
   import numpy as np

   ds = dataset(
       ""username/dataset_name"",
       shape = (4,),
       mode = ""w+"",
       schema = {
           ""image"": schema.tensor((512, 512), dtype=""float""),
           ""label"": schema.tensor((512, 512), dtype=""float""),
       }
   )

   ds[""image""][:] = np.zeros((4, 512, 512))
   ds[""label""][:] = np.zeros((4, 512, 512))
   ds.flush()
   ```

3. access it from anywhere else in the world, on any device having a command line:

   ```python
   from hub import dataset

   ds = dataset(""username/dataset_name"")
   ```

## documentation

for more advanced data pipelines like uploading large datasets or applying many transformations, please refer to our [documentation](https://docsv1.activeloop.ai/en/latest/).

## tutorial notebooks

the [examples](https://github.com/activeloopai/hub/tree/master/examples) directory has a series of examples and the [notebooks](https://github.com/activeloopai/hub/tree/master/examples/notebooks) has some notebooks with use cases. some of the notebooks are listed of below.

| notebook                                                                                                                                        | description                                       |                                                                                                                                                                                                                                    |
| :---------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| [uploading images](https://github.com/activeloopai/hub/blob/master/examples/tutorial/tutorial%201a%20-%20uploading%20images.ipynb)              | overview on how to upload and store images on hub |                 [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/activeloopai/hub/blob/master/examples/tutorial/tutorial%201a%20-%20uploading%20images.ipynb) |
| [uploading dataframes](https://github.com/activeloopai/hub/blob/master/examples/tutorial/tutorial%201b%20-%20uploading%20dataframes.ipynb)      | overview on how to upload dataframes on hub       |             [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/activeloopai/hub/blob/master/examples/tutorial/tutorial%201b%20-%20uploading%20dataframes.ipynb) |
| [uploading audio](https://github.com/activeloopai/hub/blob/master/examples/tutorial/tutorial%201c%20-%20uploading%20audio.ipynb)                | explains how to handle audio data in hub          |                  [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/activeloopai/hub/blob/master/examples/tutorial/tutorial%201c%20-%20uploading%20audio.ipynb) |
| [retrieving remote data](https://github.com/activeloopai/hub/blob/master/examples/tutorial/tutorial%202%20-%20retrieving%20remote%20data.ipynb) | explains how to retrieve data                     | [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/activeloopai/hub/blob/master/examples/tutorial/tutorial/tutorial%202%20-%20retrieving%20remote%20data.ipynb) |
| [transforming data](https://github.com/activeloopai/hub/blob/master/examples/tutorial/tutorial%203%20-%20transforming%20data.ipynb)             | briefs on how data transformation with hub        |                 [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/activeloopai/hub/blob/master/examples/tutorial/tutorial%203%20-%20transforming%20data.ipynb) |
| [dynamic tensors](https://github.com/activeloopai/hub/blob/master/examples/tutorial/tutorial%204%20-%20what%20are%20dynamic%20tensors.ipynb)    | handling data with variable shape and sizes       |      [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/activeloopai/hub/blob/master/examples/tutorial/tutorial%204%20-%20what%20are%20dynamic%20tensors.ipynb) |
| [nlp using hub](https://github.com/activeloopai/hub/blob/master/examples/notebooks/nlp_using_hub.ipynb)                                         | fine tuning bert for cola                         |                                         [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/activeloopai/hub/blob/master/examples/notebooks/nlp_using_hub.ipynb) |
| [getting started with text on hub](https://github.com/activeloopai/hub/blob/master/examples/notebooks/getting_started_with_text_on_hub.ipynb)   | overview on using text datasets in hub            |                      [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/activeloopai/hub/blob/master/examples/notebooks/getting_started_with_text_on_hub.ipynb) |

## use cases

- **satellite and drone imagery**: [smarter farming with scalable aerial pipelines](https://activeloop.ai/usecase/intelinair?utm_source=github&utm_medium=repo&utm_campaign=readme), [mapping economic well-being in india](https://towardsdatascience.com/faster-machine-learning-using-hub-by-activeloop-4ffb3420c005), [fighting desert locust in kenya with red cross](https://omdena.com/projects/ai-desert-locust/)
- **medical images**: volumetric images such as mri or xray
- **self-driving cars**: [radar, 3d lidar, point cloud, semantic segmentation, video objects](https://medium.com/snarkhub/extending-snark-hub-capabilities-to-handle-waymo-open-dataset-4dc7b7d8ab35)
- **retail**: self-checkout datasets
- **media**: images, video, audio storage

## why hub specifically?

there are quite a few dataset management libraries which offer functionality that might seem similar to hub. in fact, quite a few users migrate data from pytorch or tensorflow datasets to hub. here are a few startling differences you will encounter after switching to hub:

- the data is provided in chunks, which you may stream from a remote location, instead of downloading all of it at once
- as only the necessary portion of the dataset is evaluated, you are able to work on the data immediately
- you are able to store the data that would not fit in your memory in its entirety
- you may version control and collaborate with multiple users on your datasets across different machines
- you are equipped with tools that enhance your understanding of the data in a manner of seconds, such as our visualization tool
- you can easily prepare your data for multiple training libraries at ones (e.g. you can use the same dataset for training with pytorch and tensorflow)

## community

join our [**slack community**](https://join.slack.com/t/hubdb/shared_invite/zt-ivhsj8sz-gwv9c5flbdvw8vn~sxrkqq) to get help from activeloop team and other users, as well as stay up-to-date on dataset management/preprocessing best practices.

we'd love your feedback by completing our 3-minute [**survey**](https://forms.gle/rli4w33dow6csmcm9).

<img alt=""tweet"" src=""https://img.shields.io/twitter/follow/activeloopai?label=stay%20in%20the%20loop&style=social""> on twitter.

as always, thanks to our amazing contributors!

<a href=""https://github.com/activeloopai/hub/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=activeloopai/hub"" />
</a>

made with [contributors-img](https://contrib.rocks).

please read [contributing.md](contributing.md) to know how to get started with making contributions to hub.

## examples

activeloop's hub format lets you achieve faster inference at a lower cost. we have 30+ popular datasets already on our platform. these include:

- coco
- cifar-10
- pascal voc
- cars196
- kitti
- eurosat
- caltech-ucsd birds 200
- food101

check these and many more popular datasets on our [visualizer web app](https://app.activeloop.ai/datasets/popular/?utm_source=github&utm_medium=repo&utm_campaign=readme) and load them directly for model training!

## readme badge

using hub? add a readme badge to let everyone know:

[![hub](https://img.shields.io/badge/powered%20by-hub%20-ff5a1f.svg)](https://github.com/activeloopai/hub)

```
[![hub](https://img.shields.io/badge/powered%20by-hub%20-ff5a1f.svg)](https://github.com/activeloopai/hub)
```

## usage tracking

by default, we collect anonymous usage data using bugout (here's the [code](https://github.com/activeloopai/hub/blob/853456a314b4fb5623c936c825601097b0685119/hub/__init__.py#l24) that does it). it only logs hub library's own actions and parameters, and no user/ model data is collected.

this helps the activeloop team to understand how the tool is used and how to deliver maximum value to the community by building features that matter to you. you can easily opt-out of usage tracking during login.

## disclaimers

similarly to other dataset management packages, `hub` is a utility library that downloads and prepares public datasets. we do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have license to use the dataset. it is your responsibility to determine whether you have permission to use the dataset under the dataset's license.

if you're a dataset owner and wish to update any part of it (description, citation, etc.), or do not want your dataset to be included in this library, please get in touch through a [github issue](https://github.com/activeloopai/hub/issues/new). thanks for your contribution to the ml community!

## acknowledgement

this technology was inspired from our experience at princeton university and would like to thank william silversmith @seunglab with his awesome [cloud-volume](https://github.com/seung-lab/cloud-volume) tool. we are heavy users of [zarr](https://zarr.readthedocs.io/en/stable/) and would like to thank their community for building such a great fundamental block.
"
"FEDOT",".. |eng| image:: https://img.shields.io/badge/lang-en-red.svg
   :target: /readme_en.rst

.. |rus| image:: https://img.shields.io/badge/lang-ru-yellow.svg
   :target: /readme.rst

.. image:: docs/fedot_logo.png
   :alt: logo of fedot framework

.. start-badges
.. list-table::
   :stub-columns: 1

   * - package
     - | |pypi| |py_7| |py_8| |py_9|
   * - tests
     - | |build| |coverage|
   * - docs
     - |docs|
   * - license
     - | |license|
   * - stats
     - | |downloads_stats|
   * - support
     - | |tg|
   * - languages
     - | |eng| |rus|
.. end-badges

**fedot** - —ç—Ç–æ open-source —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∏–∑ –æ–±–ª–∞—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (automl). —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–¥ –ª–∏—Ü–µ–Ω–∑–∏–µ–π 3-clause bsd.

fedot –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –¥–∏–∑–∞–π–Ω –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. —è–¥—Ä–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–º –ø–æ–¥—Ö–æ–¥–µ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é (–±–∏–Ω–∞—Ä–Ω—É—é –∏ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤—É—é), —Ä–µ–≥—Ä–µ—Å—Å–∏—é, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∏ –∑–∞–¥–∞—á–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.

.. image:: docs/pipeline_small.png
   :alt: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –ø–æ–º–æ—â—å—é fedot

–∫–ª—é—á–µ–≤–æ–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å—é —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ —è–≤–ª—è–µ—Ç—Å—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω—ã–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤. –æ–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –≤–∏–¥–µ –≥—Ä–∞—Ñ–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –±–ª–æ–∫–∞–º–∏ –º–æ–¥–µ–ª–∏.

–ø—Ä–æ–µ–∫—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –≥—Ä—É–ø–ø–æ–π natural systems simulation lab, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é `–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –∏—Ç–º–æ <https://actcognitive.org/>`__.

–±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ fedot –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–º –≤–∏–¥–µ–æ:


.. image:: https://res.cloudinary.com/marcomontalbano/image/upload/v1606396758/video_to_markdown/images/youtube--rjbuv6i6de4-c05b58ac6eb4c4700831b2b3070cd403.jpg
   :target: http://www.youtube.com/watch?v=rjbuv6i6de4
   :alt: introducing fedot

–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
=====================

–æ—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞:

- **–≥–∏–±–∫–æ—Å—Ç—å.** fedot –æ—á–µ–Ω—å –≥–∏–±–∫–∏–π: –µ–≥–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –∑–∞–¥–∞—á, —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–µ–π;
- **–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ –º–æ.** fedot –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –º–æ (scikit-learn, can boost, xgboost –∏ —Ç.–¥.) –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏;
- **—Ä–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å –¥–ª—è –Ω–æ–≤—ã—Ö –≤–∏–¥–æ–≤ –∑–∞–¥–∞—á.** –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ –Ω–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –≤–∏–¥–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–¥–∞—á, –æ–¥–Ω–∞–∫–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –∑–∞–¥–∞—á –∏–ª–∏ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, nlp, —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ç.–¥.) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏;
- **–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π.** —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –µ–≥–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ ode –∏–ª–∏ pde;
- **–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.** –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ fedot;
- **–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å.** –º–æ–∂–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∏–≤—à–∏–µ—Å—è –ø–∞–ø–ª–∞–π–Ω—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ json –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞.

–ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏, fedot:

- –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –∏ —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–º, —Ç.–∫. —É –Ω–µ–≥–æ –Ω–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –¥–ª—è –≤–∏–¥–æ–≤ –∑–∞–¥–∞—á –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è;
- –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–µ–π –∏ —Ç–µ–º —Å–∞–º—ã–º –¥–æ—Å—Ç–∏–≥–∞—Ç—å –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤;
- –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ç—Ä–æ–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω—ã —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ç–µ–∫—Å—Ç—ã, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–∞–±–ª–∏—Ü—ã –∏ —Ç.–¥.).

—É—Å—Ç–∞–Ω–æ–≤–∫–∞
=========

—Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å fedot - —ç—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ``pip``:

.. code-block::

  $ pip install fedot

—É—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –¥–ª—è dnn:

.. code-block::

  $ pip install fedot[extra]

–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
================

fedot –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π api, –∫–æ—Ç–æ—Ä—ã–π —É–¥–æ–±–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å. api –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.
—á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å api, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ –¥–µ–π—Å—Ç–≤–∏—è:

1. –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –∫–ª–∞—Å—Å ``fedot``

.. code-block:: python

 from fedot.api.main import fedot

2. –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –æ–±—ä–µ–∫—Ç fedot –∏ –∑–∞–¥–∞–π—Ç–µ —Ç–∏–ø –∑–∞–¥–∞—á–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –æ–±—ä–µ–∫—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å fit/predict:

- ``fedot.fit()`` –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª—É—á–∏–≤—à–∏–π—Å—è —Å–æ—Å—Ç–∞–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω;
- ``fedot.predict()`` –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —É–∂–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω;
- ``fedot.get_metrics()`` –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.

–≤ –∫–∞—á–µ—Å—Ç–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –º–∞—Å—Å–∏–≤—ã numpy, –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã pandas –∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É. –≤ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–º –Ω–∏–∂–µ –ø—Ä–∏–º–µ—Ä–µ ``x_train``, ``y_train`` –∏ ``x_test`` —è–≤–ª—è—é—Ç—Å—è ``numpy.ndarray()``:

.. code-block:: python

    model = fedot(problem='classification', timeout=5, preset='best_quality', n_jobs=-1)
    model.fit(features=x_train, target=y_train)
    prediction = model.predict(features=x_test)
    metrics = model.get_metrics(target=y_test)

–±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± api –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ `–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ <https://fedot.readthedocs.io/en/latest/api/api.html>`__, –∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –ø–æ–∫–∞–∑–∞–Ω—ã `–≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ <https://github.com/aimclub/fedot/tree/master/examples/advanced>`__.

–ø—Ä–∏–º–µ—Ä—ã
=======

jupyter –Ω–æ—É—Ç–±—É–∫–∏ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ `fedot-examples <https://github.com/itmo-nss-team/fedot-examples>`__. —Ç–∞–º –≤—ã –º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞:

* `intro to automl <https://github.com/itmo-nss-team/fedot-examples/blob/main/notebooks/latest/1_intro_to_automl.ipynb>`__
* `intro to fedot functionality <https://github.com/itmo-nss-team/fedot-examples/blob/main/notebooks/latest/2_intro_to_fedot.ipynb>`__
* `intro to time series forecasting with fedot <https://github.com/itmo-nss-team/fedot-examples/blob/main/notebooks/latest/3_intro_ts_forecasting.ipynb>`__
* `advanced time series forecasting <https://github.com/itmo-nss-team/fedot-examples/blob/main/notebooks/latest/4_auto_ts_forecasting.ipynb>`__
* `gap-filling in time series and out-of-sample forecasting <https://github.com/itmo-nss-team/fedot-examples/blob/main/notebooks/latest/5_ts_specific_cases.ipynb>`__
* `hybrid modelling with custom models <https://github.com/itmo-nss-team/fedot-examples/blob/main/notebooks/latest/6_hybrid_modelling.ipynb>`__

–≤–µ—Ä—Å–∏–∏ –Ω–æ—É—Ç–±—É–∫–æ–≤ –≤—ã–ø—É—Å–∫–∞—é—Ç—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –≤–µ—Ä—Å–∏—è–º–∏ —Ä–µ–ª–∏–∑–æ–≤ (–≤–µ—Ä—Å–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - ""latest"").

—Ç–∞–∫–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã –ø—Ä–∏–º–µ—Ä—ã –Ω–∞ –≤–Ω–µ—à–Ω–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö:

* `kaggle: baseline for microsoft stock - time series analysis task <https://www.kaggle.com/dreamlone/microsoft-stocks-price-prediction-automl>`__

—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã:

- –∑–∞–¥–∞—á–∞ —Å –∫—Ä–µ–¥–∏—Ç–Ω—ã–º —Å–∫–æ—Ä–∏–Ω–≥–æ–º `binary classification task <https://github.com/aimclub/fedot/blob/master/cases/credit_scoring/credit_scoring_problem.py>`__
- –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ `random process regression <https://github.com/aimclub/fedot/blob/master/cases/metocean_forecasting_problem.py>`__
- –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–ø–∞–º–∞ `natural language preprocessing <https://github.com/aimclub/fedot/blob/master/cases/spam_detection.py>`__
- –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–æ—Ä—Ç–∞ –≤–∏–Ω–∞ `multi-modal data <https://github.com/aimclub/fedot/blob/master/examples/advanced/multimodal_text_num_example.py>`__


—Ç–∞–∫–∂–µ –¥–æ—Å—Ç—É–ø–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ `–≤–∏–¥–µ–æ —É—Ä–æ–∫–æ–≤ <https://www.youtube.com/playlist?list=pllbchj5ytafujaxpzf7fbeaanmqpdyhnc>`__ (–Ω–∞ —Ä—É—Å—Å–∫–æ–º).

–ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –æ fedot
==================

–º—ã –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞:

–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º:

- how automl helps to create composite ai? - `towardsdatascience.com <https://towardsdatascience.com/how-automl-helps-to-create-composite-ai-f09e05287563>`__
- automl for time series: definitely a good idea - `towardsdatascience.com <https://towardsdatascience.com/automl-for-time-series-definitely-a-good-idea-c51d39b2b3f>`__
- automl for time series: advanced approaches with fedot framework - `towardsdatascience.com <https://towardsdatascience.com/automl-for-time-series-advanced-approaches-with-fedot-framework-4f9d8ea3382c>`__
- winning a flood-forecasting hackathon with hydrology and automl - `towardsdatascience.com <https://towardsdatascience.com/winning-a-flood-forecasting-hackathon-with-hydrology-and-automl-156a8a7a4ede>`__
- clean automl for ‚Äúdirty‚Äù data - `towardsdatascience.com <https://towardsdatascience.com/clean-automl-for-dirty-data-how-and-why-to-automate-preprocessing-of-tables-in-machine-learning-d79ac87780d3>`__
- fedot as a factory of human-competitive results - `youtube.com <https://www.youtube.com/watch?v=9rhqcsrolb8&ab_channel=nss-lab>`__
- hyperparameters tuning for machine learning model ensembles - `towardsdatascience.com <https://towardsdatascience.com/hyperparameters-tuning-for-machine-learning-model-ensembles-8051782b538b>`__

–Ω–∞ —Ä—É—Å—Å–∫–æ–º:

- –∫–∞–∫ automl –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–≥–æ –∏–∏ ‚Äî –≥–æ–≤–æ—Ä–∏–º –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–µ fedot - `habr.com <https://habr.com/ru/company/spbifmo/blog/558450>`__
- –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é automl - `habr.com <https://habr.com/ru/post/559796/>`__
- –∫–∞–∫ –º—ã ‚Äú–ø–æ–≤–µ—Ä–Ω—É–ª–∏ —Ä–µ–∫–∏ –≤—Å–ø—è—Ç—å‚Äù –Ω–∞ emergency datahack 2021, –æ–±—ä–µ–¥–∏–Ω–∏–≤ –≥–∏–¥—Ä–æ–ª–æ–≥–∏—é –∏ automl - `habr.com <https://habr.com/ru/post/577886/>`__
- —á–∏—Å—Ç—ã–π automl –¥–ª—è ‚Äú–≥—Ä—è–∑–Ω—ã—Ö‚Äù –¥–∞–Ω–Ω—ã—Ö: –∫–∞–∫ –∏ –∑–∞—á–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–∞–±–ª–∏—Ü –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ - `ods blog <https://habr.com/ru/company/ods/blog/657525/>`__
- —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è fedot (–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è highload++ 2022) - `presentation <https://docs.yandex.ru/docs/view?url=ya-disk-public%3a%2f%2fi27lscu3s3iihdzixt9o5eieaml6thy6qlu3x1oyh%2ffial%2blcnp4o4ytsyd2grznw5adq4kmzexe%2bwnjbq78ug%3d%3d%3a%2f%d0%94%d0%b5%d0%bd%d1%8c%201%2f4.%d0%a1%d0%b8%d0%bd%d0%bd%d0%b0%d0%ba%d1%81%2f9.open%20source-%d1%82%d1%80%d0%b8%d0%b1%d1%83%d0%bd%d0%b0_hl_fedot.pptx&name=9.open%20source-%d1%82%d1%80%d0%b8%d0%b1%d1%83%d0%bd%d0%b0_hl_fedot.pptx>`__
- –ø—Ä–æ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∞–Ω—Å–∞–º–±–ª–µ–π –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - `habr.com <https://habr.com/ru/post/672486/>`__

–Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º:

- ÁîüÊàêÂºèËá™Âä®Êú∫Âô®Â≠¶‰π†Á≥ªÁªü (–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ ""open innovations 2.0"") - `youtube.com <https://www.youtube.com/watch?v=peet0ebcscy>`__


—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
=================

–ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è fedot –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ `–≤–µ—Ç–∫–µ master <https://github.com/aimclub/fedot/tree/master>`__.

—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:

* –≤ –ø–∞–∫–µ—Ç–µ `core <https://github.com/aimclub/fedot/tree/master/fedot/core>`__  –Ω–∞—Ö–æ–¥—è—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∞—Å—Å—ã –∏ —Å–∫—Ä–∏–ø—Ç—ã. —ç—Ç–æ *—è–¥—Ä–æ* —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ fedot.
* –≤ –ø–∞–∫–µ—Ç–µ `examples <https://github.com/aimclub/fedot/tree/master/examples>`__ —Å–æ–±—Ä–∞–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, —Å –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–Ω–æ –Ω–∞—á–∞—Ç—å –∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å fedot.
* –≤—Å–µ —Ç–µ—Å—Ç—ã (unit –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ) –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ `test <https://github.com/aimclub/fedot/tree/master/test>`__.
* –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–∞–ø–∫–µ `docs <https://github.com/aimclub/fedot/tree/master/docs>`__.

—Ç–µ–∫—É—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è/—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–ª–∞–Ω—ã –Ω–∞ –±—É–¥—É—â–µ–µ
==================================================

–≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –º—ã —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–¥ –Ω–æ–≤—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∏ –ø—ã—Ç–∞–µ–º—Å—è —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —É–¥–æ–±—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è fedot.
–æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–∫—É—â–∏–µ –∑–∞–¥–∞—á–∏ –∏ –ø–ª–∞–Ω—ã:

* —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∏ –≥–æ—Ç–æ–≤—ã–µ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —à–∞–±–ª–æ–Ω—ã –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –∏ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö;
* –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å gpu —á–µ—Ä–µ–∑ rapids framework;
* –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–æ–π;
* –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ml flow –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤;
* —É–ª—É—á—à–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ api.


–∫—Ä–æ–º–µ —Ç–æ–≥–æ, –º—ã —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–¥ —Ä—è–¥–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–æ–º –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é automl –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.

–Ω–∞—à–∞ –Ω–∞—É—á–Ω–æ-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∫–æ–º–∞–Ω–¥–∞ –æ—Ç–∫—Ä—ã—Ç–∞ –¥–ª—è —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ —Å –¥—Ä—É–≥–∏–º–∏ –Ω–∞—É—á–Ω—ã–º–∏ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–∞–º–∏, –∞ —Ç–∞–∫–∂–µ —Å –ø–∞—Ä—Ç–Ω–µ—Ä–∞–º–∏ –∏–∑ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏.

–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
============

–æ–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ `fedot.docs <https://itmo-nss-team.github.io/fedot.miscellaneous>`__.

–ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ fedot api –¥–æ—Å—Ç—É–ø–Ω–æ –≤ —Ä–∞–∑–¥–µ–ª–µ `read the docs <https://fedot.readthedocs.io/en/latest/>`__.

–∫–∞–∫ —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å
===============

- –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ `—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ <https://github.com/aimclub/fedot/blob/master/docs/source/contribution.rst>`__.

–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏
=============

–º—ã –±–ª–∞–≥–æ–¥–∞—Ä–Ω—ã –∫–æ–Ω—Ç—Ä–∏–±—å—é—Ç–µ—Ä–∞–º –∑–∞ –∏—Ö –≤–∞–∂–Ω—ã–π –≤–∫–ª–∞–¥, –∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–π –∏ —Å–µ–º–∏–Ω–∞—Ä–æ–≤ - –∑–∞ –∏—Ö —Ü–µ–Ω–Ω—ã–µ —Å–æ–≤–µ—Ç—ã –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã
======================
- –ø—Ä–æ—Ç–æ—Ç–∏–ø web-gui –¥–ª—è fedot –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ `fedot.web <https://github.com/aimclub/fedot.web>`__.


–∫–æ–Ω—Ç–∞–∫—Ç—ã
========
- `telegram-–∫–∞–Ω–∞–ª <https://t.me/fedot_helpdesk>`_  –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ fedot
- –∫–æ–º–∞–Ω–¥–∞ `natural system simulation <https://itmo-nss-team.github.io/>`_
- `–∞–Ω–Ω–∞ –∫–∞–ª—é–∂–Ω–∞—è <https://scholar.google.com/citations?user=bjiilqcaaaaj&hl=ru>`_, —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å (anna.kalyuzhnaya@itmo.ru)
- `–Ω–æ–≤–æ—Å—Ç–∏ <https://t.me/nss_group>`_
- `youtube –∫–∞–Ω–∞–ª <https://www.youtube.com/channel/uc4k9qwaeupt_p3r4fedp5ja>`_

—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –≤–µ–¥—ë—Ç—Å—è –ø—Ä–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–µ
================================

- `–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –∏—Ç–º–æ <https://actcognitive.org/>`_

—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
===========

@article{nikitin2021automated,
  title = {automated evolutionary approach for the design of composite machine learning pipelines},
  author = {nikolay o. nikitin and pavel vychuzhanin and mikhail sarafanov and iana s. polonskaia and ilia revin and irina v. barabanova and gleb maximov and anna v. kalyuzhnaya and alexander boukhanovsky},
  journal = {future generation computer systems},
  year = {2021},
  issn = {0167-739x},
  doi = {https://doi.org/10.1016/j.future.2021.08.022}}

@inproceedings{polonskaia2021multi,
  title={multi-objective evolutionary design of composite data-driven models},
  author={polonskaia, iana s. and nikitin, nikolay o. and revin, ilia and vychuzhanin, pavel and kalyuzhnaya, anna v.},
  booktitle={2021 ieee congress on evolutionary computation (cec)},
  year={2021},
  pages={926-933},
  doi={10.1109/cec45853.2021.9504773}}


–¥—Ä—É–≥–∏–µ —Å—Ç–∞—Ç—å–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞ `researchgate <https://www.researchgate.net/project/evolutionary-multi-modal-automl-with-fedot-framework>`_.

.. |docs| image:: https://readthedocs.org/projects/ebonite/badge/?style=flat
   :target: https://fedot.readthedocs.io/en/latest/
   :alt: documentation status

.. |build| image:: https://github.com/aimclub/fedot/workflows/build/badge.svg?branch=master
   :alt: build status
   :target: https://github.com/aimclub/fedot/actions

.. |coverage| image:: https://codecov.io/gh/nccr-itmo/fedot/branch/master/graph/badge.svg
   :alt: coverage status
   :target: https://codecov.io/gh/nccr-itmo/fedot

.. |pypi| image:: https://badge.fury.io/py/fedot.svg
   :alt: supported python versions
   :target: https://badge.fury.io/py/fedot

.. |py_7| image:: https://img.shields.io/badge/python_3.7-passing-success
   :alt: supported python versions
   :target: https://img.shields.io/badge/python_3.7-passing-success

.. |py_8| image:: https://img.shields.io/badge/python_3.8-passing-success
   :alt: supported python versions
   :target: https://img.shields.io/badge/python_3.8-passing-success

.. |py_9| image:: https://img.shields.io/badge/python_3.9-passing-success
   :alt: supported python versions
   :target: https://img.shields.io/badge/python_3.9-passing-success

.. |license| image:: https://img.shields.io/github/license/nccr-itmo/fedot
   :alt: supported python versions
   :target: https://github.com/aimclub/fedot/blob/master/license.md

.. |downloads_stats| image:: https://static.pepy.tech/personalized-badge/fedot?period=total&units=international_system&left_color=grey&right_color=brightgreen&left_text=downloads
   :target: https://pepy.tech/project/fedot

.. |tg| image:: https://img.shields.io/badge/telegram-group-blue.svg
          :target: https://t.me/fedot_helpdesk
          :alt: telegram chat
"
"Sklearn-genetic-opt",".. -*- mode: rst -*-

|tests|_ |codecov|_ |pythonversion|_ |pypi|_ |docs|_

.. |tests| image:: https://github.com/rodrigo-arenas/sklearn-genetic-opt/actions/workflows/ci-tests.yml/badge.svg?branch=master
.. _tests: https://github.com/rodrigo-arenas/sklearn-genetic-opt/actions/workflows/ci-tests.yml

.. |codecov| image:: https://codecov.io/gh/rodrigo-arenas/sklearn-genetic-opt/branch/master/graphs/badge.svg?branch=master&service=github
.. _codecov: https://codecov.io/github/rodrigo-arenas/sklearn-genetic-opt?branch=master

.. |pythonversion| image:: https://img.shields.io/badge/python-3.7%20%7c%203.8%20%7c%203.9-blue
.. _pythonversion : https://www.python.org/downloads/
.. |pypi| image:: https://badge.fury.io/py/sklearn-genetic-opt.svg
.. _pypi: https://badge.fury.io/py/sklearn-genetic-opt

.. |docs| image:: https://readthedocs.org/projects/sklearn-genetic-opt/badge/?version=latest
.. _docs: https://sklearn-genetic-opt.readthedocs.io/en/latest/?badge=latest

.. |contributors| image:: https://contributors-img.web.app/image?repo=rodrigo-arenas/sklearn-genetic-opt
.. _contributors: https://github.com/rodrigo-arenas/sklearn-genetic-opt/graphs/contributors


.. image:: https://github.com/rodrigo-arenas/sklearn-genetic-opt/blob/master/docs/logo.png?raw=true

sklearn-genetic-opt
###################

scikit-learn models hyperparameters tuning and feature selection, using evolutionary algorithms.

this is meant to be an alternative to popular methods inside scikit-learn such as grid search and randomized grid search
for hyperparameteres tuning, and from rfe, select from model for feature selection.

sklearn-genetic-opt uses evolutionary algorithms from the deap package to choose the set of hyperparameters that
optimizes (max or min) the cross-validation scores, it can be used for both regression and classification problems.

documentation is available `here <https://sklearn-genetic-opt.readthedocs.io/>`_

main features:
##############

* **gasearchcv**: main class of the package for hyperparameters tuning, holds the evolutionary cross-validation optimization routine.
* **gafeatureselectioncv**: main class of the package for feature selection.
* **algorithms**: set of different evolutionary algorithms to use as an optimization procedure.
* **callbacks**: custom evaluation strategies to generate early stopping rules,
  logging (into tensorboard, .pkl files, etc) or your custom logic.
* **schedulers**: adaptive methods to control learning parameters.
* **plots**: generate pre-defined plots to understand the optimization process.
* **mlflow**: build-in integration with mlflow to log all the hyperparameters, cv-scores and the fitted models.

demos on features:
##################

visualize the progress of your training:

.. image:: https://github.com/rodrigo-arenas/sklearn-genetic-opt/blob/master/docs/images/progress_bar.gif?raw=true

real-time metrics visualization and comparison across runs:

.. image:: https://github.com/rodrigo-arenas/sklearn-genetic-opt/blob/master/docs/images/tensorboard_log.png?raw=true

sampled distribution of hyperparameters:

.. image:: https://github.com/rodrigo-arenas/sklearn-genetic-opt/blob/master/docs/images/density.png?raw=true

artifacts logging:

.. image:: https://github.com/rodrigo-arenas/sklearn-genetic-opt/blob/master/docs/images/mlflow_artifacts_4.png?raw=true


usage:
######

install sklearn-genetic-opt

it's advised to install sklearn-genetic using a virtual env, inside the env use::

   pip install sklearn-genetic-opt

if you want to get all the features, including plotting and mlflow logging capabilities,
install all the extra packages::

    pip install sklearn-genetic-opt[all]

the only optional dependency that the last command does not install, it's tensorflow,
it is usually advised to look further which distribution works better for you.


example: hyperparameters tuning
###############################

.. code-block:: python

   from sklearn_genetic import gasearchcv
   from sklearn_genetic.space import continuous, categorical, integer
   from sklearn.ensemble import randomforestclassifier
   from sklearn.model_selection import train_test_split, stratifiedkfold
   from sklearn.datasets import load_digits
   from sklearn.metrics import accuracy_score

   data = load_digits()
   n_samples = len(data.images)
   x = data.images.reshape((n_samples, -1))
   y = data['target']
   x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

   clf = randomforestclassifier()

   param_grid = {'min_weight_fraction_leaf': continuous(0.01, 0.5, distribution='log-uniform'),
                 'bootstrap': categorical([true, false]),
                 'max_depth': integer(2, 30),
                 'max_leaf_nodes': integer(2, 35),
                 'n_estimators': integer(100, 300)}

   cv = stratifiedkfold(n_splits=3, shuffle=true)

   evolved_estimator = gasearchcv(estimator=clf,
                                  cv=cv,
                                  scoring='accuracy',
                                  population_size=10,
                                  generations=35,
                                  param_grid=param_grid,
                                  n_jobs=-1,
                                  verbose=true,
                                  keep_top_k=4)

   # train and optimize the estimator
   evolved_estimator.fit(x_train, y_train)
   # best parameters found
   print(evolved_estimator.best_params_)
   # use the model fitted with the best parameters
   y_predict_ga = evolved_estimator.predict(x_test)
   print(accuracy_score(y_test, y_predict_ga))

   # saved metadata for further analysis
   print(""stats achieved in each generation: "", evolved_estimator.history)
   print(""best k solutions: "", evolved_estimator.hof)


example: feature selection
##########################

.. code:: python3

    import matplotlib.pyplot as plt
    from sklearn_genetic import gafeatureselectioncv
    from sklearn.model_selection import train_test_split, stratifiedkfold
    from sklearn.svm import svc
    from sklearn.datasets import load_iris
    from sklearn.metrics import accuracy_score
    import numpy as np

    data = load_iris()
    x, y = data[""data""], data[""target""]

    # add random non-important features
    noise = np.random.uniform(0, 10, size=(x.shape[0], 5))
    x = np.hstack((x, noise))

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=0)

    clf = svc(gamma='auto')

    evolved_estimator = gafeatureselectioncv(
        estimator=clf,
        scoring=""accuracy"",
        population_size=30,
        generations=20,
        n_jobs=-1)

    # train and select the features
    evolved_estimator.fit(x_train, y_train)

    # features selected by the algorithm
    features = evolved_estimator.best_features_
    print(features)

    # predict only with the subset of selected features
    y_predict_ga = evolved_estimator.predict(x_test[:, features])
    print(accuracy_score(y_test, y_predict_ga))


changelog
#########

see the `changelog <https://sklearn-genetic-opt.readthedocs.io/en/latest/release_notes.html>`__
for notes on the changes of sklearn-genetic-opt

important links
###############

- official source code repo: https://github.com/rodrigo-arenas/sklearn-genetic-opt/
- download releases: https://pypi.org/project/sklearn-genetic-opt/
- issue tracker: https://github.com/rodrigo-arenas/sklearn-genetic-opt/issues
- stable documentation: https://sklearn-genetic-opt.readthedocs.io/en/stable/

source code
###########

you can check the latest development version with the command::

   git clone https://github.com/rodrigo-arenas/sklearn-genetic-opt.git

install the development dependencies::
  
  pip install -r dev-requirements.txt
  
check the latest in-development documentation: https://sklearn-genetic-opt.readthedocs.io/en/latest/

contributing
############

contributions are more than welcome!
there are several opportunities on the ongoing project, so please get in touch if you would like to help out.
make sure to check the current issues and also
the `contribution guide <https://github.com/rodrigo-arenas/sklearn-genetic-opt/blob/master/contributing.md>`_.

big thanks to the people who are helping with this project!

|contributors|_

testing
#######

after installation, you can launch the test suite from outside the source directory::

   pytest sklearn_genetic

"
"Optuna","<div align=""center""><img src=""https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png"" width=""800""/></div>

# optuna: a hyperparameter optimization framework

[![python](https://img.shields.io/badge/python-3.7%20%7c%203.8%20%7c%203.9%20%7c%203.10%20%7c%203.11-blue)](https://www.python.org)
[![pypi](https://img.shields.io/pypi/v/optuna.svg)](https://pypi.python.org/pypi/optuna)
[![conda](https://img.shields.io/conda/vn/conda-forge/optuna.svg)](https://anaconda.org/conda-forge/optuna)
[![github license](https://img.shields.io/badge/license-mit-blue.svg)](https://github.com/optuna/optuna)
[![read the docs](https://readthedocs.org/projects/optuna/badge/?version=stable)](https://optuna.readthedocs.io/en/stable/)
[![codecov](https://codecov.io/gh/optuna/optuna/branch/master/graph/badge.svg)](https://codecov.io/gh/optuna/optuna/branch/master)

[**website**](https://optuna.org/)
| [**docs**](https://optuna.readthedocs.io/en/stable/)
| [**install guide**](https://optuna.readthedocs.io/en/stable/installation.html)
| [**tutorial**](https://optuna.readthedocs.io/en/stable/tutorial/index.html)
| [**examples**](https://github.com/optuna/optuna-examples)

*optuna* is an automatic hyperparameter optimization software framework, particularly designed
for machine learning. it features an imperative, *define-by-run* style user api. thanks to our
*define-by-run* api, the code written with optuna enjoys high modularity, and the user of
optuna can dynamically construct the search spaces for the hyperparameters.

## key features

optuna has modern functionalities as follows:

- [lightweight, versatile, and platform agnostic architecture](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/001_first.html)
  - handle a wide variety of tasks with a simple installation that has few requirements.
- [pythonic search spaces](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html)
  - define search spaces using familiar python syntax including conditionals and loops.
- [efficient optimization algorithms](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html)
  - adopt state-of-the-art algorithms for sampling hyperparameters and efficiently pruning unpromising trials.
- [easy parallelization](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html)
  - scale studies to tens or hundreds or workers with little or no changes to the code.
- [quick visualization](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html)
  - inspect optimization histories from a variety of plotting functions.


## basic concepts

we use the terms *study* and *trial* as follows:

- study: optimization based on an objective function
- trial: a single execution of the objective function

please refer to sample code below. the goal of a *study* is to find out the optimal set of
hyperparameter values (e.g., `regressor` and `svr_c`) through multiple *trials* (e.g.,
`n_trials=100`). optuna is a framework designed for the automation and the acceleration of the
optimization *studies*.

[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/optuna/optuna-examples/blob/main/quickstart.ipynb)

```python
import ...

# define an objective function to be minimized.
def objective(trial):

    # invoke suggest methods of a trial object to generate hyperparameters.
    regressor_name = trial.suggest_categorical('regressor', ['svr', 'randomforest'])
    if regressor_name == 'svr':
        svr_c = trial.suggest_float('svr_c', 1e-10, 1e10, log=true)
        regressor_obj = sklearn.svm.svr(c=svr_c)
    else:
        rf_max_depth = trial.suggest_int('rf_max_depth', 2, 32)
        regressor_obj = sklearn.ensemble.randomforestregressor(max_depth=rf_max_depth)

    x, y = sklearn.datasets.fetch_california_housing(return_x_y=true)
    x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(x, y, random_state=0)

    regressor_obj.fit(x_train, y_train)
    y_pred = regressor_obj.predict(x_val)

    error = sklearn.metrics.mean_squared_error(y_val, y_pred)

    return error  # an objective value linked with the trial object.

study = optuna.create_study()  # create a new study.
study.optimize(objective, n_trials=100)  # invoke optimization of the objective function.
```

## examples

examples can be found in [optuna/optuna-examples](https://github.com/optuna/optuna-examples).

## integrations

[integrations modules](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html#integration-modules-for-pruning), which allow pruning, or early stopping, of unpromising trials are available for the following libraries:

* [allennlp](https://github.com/optuna/optuna-examples/tree/main/allennlp)
* [catalyst](https://github.com/optuna/optuna-examples/tree/main/pytorch/catalyst_simple.py)
* [catboost](https://github.com/optuna/optuna-examples/tree/main/catboost/catboost_pruning.py)
* [chainer](https://github.com/optuna/optuna-examples/tree/main/chainer/chainer_integration.py)
* fastai ([v1](https://github.com/optuna/optuna-examples/tree/main/fastai/fastaiv1_simple.py), [v2](https://github.com/optuna/optuna-examples/tree/main/fastai/fastaiv2_simple.py))
* [keras](https://github.com/optuna/optuna-examples/tree/main/keras/keras_integration.py)
* [lightgbm](https://github.com/optuna/optuna-examples/tree/main/lightgbm/lightgbm_integration.py)
* [mxnet](https://github.com/optuna/optuna-examples/tree/main/mxnet/mxnet_integration.py)
* [pytorch](https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_simple.py)
* [pytorch ignite](https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_ignite_simple.py)
* [pytorch lightning](https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_lightning_simple.py)
* [tensorflow](https://github.com/optuna/optuna-examples/tree/main/tensorflow/tensorflow_estimator_integration.py)
* [tf.keras](https://github.com/optuna/optuna-examples/tree/main/tfkeras/tfkeras_integration.py)
* [xgboost](https://github.com/optuna/optuna-examples/tree/main/xgboost/xgboost_integration.py)


## web dashboard

[optuna dashboard](https://github.com/optuna/optuna-dashboard) is a real-time web dashboard for optuna.
you can check the optimization history, hyperparameter importances, etc. in graphs and tables.
you don't need to create a python script to call [optuna's visualization](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html) functions.
feature requests and bug reports welcome!

![optuna-dashboard](https://user-images.githubusercontent.com/5564044/204975098-95c2cb8c-0fb5-4388-abc4-da32f56cb4e5.gif)

install `optuna-dashboard` via pip:

```
$ pip install optuna-dashboard
$ optuna-dashboard sqlite:///db.sqlite3
...
listening on http://localhost:8080/
hit ctrl-c to quit.
```

## installation

optuna is available at [the python package index](https://pypi.org/project/optuna/) and on [anaconda cloud](https://anaconda.org/conda-forge/optuna).

```bash
# pypi
$ pip install optuna
```

```bash
# anaconda cloud
$ conda install -c conda-forge optuna
```

optuna supports python 3.7 or newer.

also, we also provide optuna docker images on [dockerhub](https://hub.docker.com/r/optuna/optuna).

## communication

- [github discussions] for questions.
- [github issues] for bug reports and feature requests.

[github discussions]: https://github.com/optuna/optuna/discussions
[github issues]: https://github.com/optuna/optuna/issues


## contribution

any contributions to optuna are more than welcome!

if you are new to optuna, please check the [good first issues](https://github.com/optuna/optuna/labels/good%20first%20issue). they are relatively simple, well-defined and are often good starting points for you to get familiar with the contribution workflow and other developers.

if you already have contributed to optuna, we recommend the other [contribution-welcome issues](https://github.com/optuna/optuna/labels/contribution-welcome).

for general guidelines how to contribute to the project, take a look at [contributing.md](./contributing.md).


## reference

takuya akiba, shotaro sano, toshihiko yanase, takeru ohta, and masanori koyama. 2019.
optuna: a next-generation hyperparameter optimization framework. in kdd ([arxiv](https://arxiv.org/abs/1907.10902)).
"
"Deepchecks","<!--
   ~ ----------------------------------------------------------------------------
   ~ copyright (c) 2021-2023 deepchecks (https://www.deepchecks.com)
   ~
   ~ this file is part of deepchecks.
   ~ deepchecks is distributed under the terms of the gnu affero general
   ~ public license (version 3 or later).
   ~ you should have received a copy of the gnu affero general public license
   ~ along with deepchecks.  if not, see <http://www.gnu.org/licenses/>.
   ~ ----------------------------------------------------------------------------
   ~
-->


<p align=""center"">
   &emsp;
   <a href=""https://www.deepchecks.com/slack"">join&nbsp;slack</a>
   &emsp; | &emsp; 
   <a href=""https://docs.deepchecks.com/?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=top_links"">documentation</a>
   &emsp; | &emsp; 
   <a href=""https://deepchecks.com/blog/?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=top_links"">blog</a>
   &emsp; | &emsp;  
   <a href=""https://twitter.com/deepchecks"">twitter</a>
   &emsp;
</p>
   

<p align=""center"">
   <a href=""https://deepchecks.com/?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=logo"">
      <img src=""docs/source/_static/images/general/deepchecks-logo-with-white-wide-back.png"">
   </a>
</p>


[![github
stars](https://img.shields.io/github/stars/deepchecks/deepchecks.svg?style=social&label=star&maxage=2592000)](https://github.com/deepchecks/deepchecks/stargazers/)
![build](https://github.com/deepchecks/deepchecks/actions/workflows/build.yml/badge.svg)
![pkgversion](https://img.shields.io/pypi/v/deepchecks)
![pyversions](https://img.shields.io/pypi/pyversions/deepchecks)
[![maintainability](https://api.codeclimate.com/v1/badges/970b11794144139975fa/maintainability)](https://codeclimate.com/github/deepchecks/deepchecks/maintainability)
[![coverage
status](https://coveralls.io/repos/github/deepchecks/deepchecks/badge.svg?branch=main)](https://coveralls.io/github/deepchecks/deepchecks?branch=main)
<!-- all-contributors-badge:start - do not remove or modify this section -->
[![all contributors](https://img.shields.io/badge/all_contributors-38-orange.svg?style=flat-round)](#https://github.com/deepchecks/deepchecks/blob/main/contributing.rst)
<!-- all-contributors-badge:end --> 


<h1 align=""center"">
   tests for continuous validation of ml models & data
</h1>


<p align=""center"">
   <a href=""https://docs.deepchecks.com/?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=checks_and_conditions_img"">
   <img src=""docs/source/_static/images/general/checks-and-conditions.png"">
   </a>
</p>


## üßê what is deepchecks?

deepchecks is a python package for comprehensively validating your
machine learning models and data with minimal effort. this includes
checks related to various types of issues, such as model performance,
data integrity, distribution mismatches, and more.


## üñºÔ∏è computer vision & üî¢ tabular support

**this readme refers to the tabular version** of deepchecks.

check out the [deepchecks for computer vision & images subpackage](deepchecks/vision) for more details about deepchecks for cv, currently in *beta release*.


## üíª installation


### using pip

```bash
pip install deepchecks -u --user
```


> note: computer vision install
>
> to install deepchecks together with the **computer vision submodule** that 
> is currently in *beta release*, replace 
> ``deepchecks`` with ``""deepchecks[vision]""`` as follows:   
> ```bash
> pip install ""deepchecks[vision]"" -u --user
> ```
>  
   
### using conda

```bash
conda install -c conda-forge deepchecks
```



## ‚è© try it out!

### üèÉ‚Äç‚ôÄÔ∏è see it in action

head over to one of our following quickstart tutorials, and have deepchecks running on your environment in less than 5 min:

- [train-test validation quickstart (loans data)](
   https://docs.deepchecks.com/stable/user-guide/tabular/auto_quickstarts/plot_quick_train_test_validation.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=try_it_out)
- [data integrity quickstart (avocado sales data)](
   https://docs.deepchecks.com/stable/user-guide/tabular/auto_quickstarts/plot_quick_data_integrity.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=try_it_out)
- [model evaluation quickstart (wine quality data)](
   https://docs.deepchecks.com/en/stable/user-guide/tabular/auto_quickstarts/plot_quickstart_in_5_minutes.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=try_it_out)

> **recommended - download the code and run it locally** on the built-in dataset and (optional) model, or **replace them with your own**.


### üöÄ see our checks demo

play with some of the existing checks in our [interactive checks demo](
   https://checks-demo.deepchecks.com/?check=no+check+selected&utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=try_it_out), 
and see how they work on various datasets with custom corruptions injected.


## üìä usage examples

### running a suite

a [suite](#suite) runs a collection of [checks](#check) with optional
[conditions](#condition) added to them.

example for running a suite on given
[datasets](https://docs.deepchecks.com/stable/user-guide/tabular/dataset_object.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=running_a_suite)
and with a [supported
model](https://docs.deepchecks.com/stable/user-guide/supported_models.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=running_a_suite):

```python
from deepchecks.tabular.suites import model_evaluation
suite = model_evaluation()
result = suite.run(train_dataset=train_dataset, test_dataset=test_dataset, model=model)
result.save_as_html() # replace this with result.show() or result.show_in_window() to see results inline or in window
```

which will result in a report that looks like this:


<p align=""center"">
   <img src=""docs/source/_static/images/general/model_evaluation_suite.gif"" width=""800"">
</p>


note:

- results can be [displayed](https://docs.deepchecks.com/stable/user-guide/general/showing_results.html) in various manners, or [exported](https://docs.deepchecks.com/stable/user-guide/general/export_save_results.html) to an html report, saved as json, or integrated with other tools (e.g. wandb).
- other suites that run only on the data (``data_integrity``, ``train_test_validation``) don't require a model as part of the input.

see the [full code tutorials
here](
   https://docs.deepchecks.com/stable/user-guide/tabular/auto_quickstarts/index.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=try_it_out).


in the following section you can see an example of how the output of a single check without a condition may look.

### running a check

to run a specific single check, all you need to do is import it and then
to run it with the required (check-dependent) input parameters. more
details about the existing checks and the parameters they can receive
can be found in our [api
reference](https://docs.deepchecks.com/stable/api/index.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=running_a_check).

```python
from deepchecks.tabular.checks import traintestfeaturedrift
import pandas as pd

train_df = pd.read_csv('train_data.csv')
test_df = pd.read_csv('test_data.csv')
# initialize and run desired check
traintestfeaturedrift().run(train_df, test_df)
```

will produce output of the type:

>   <h4>train test drift</h4>
>  <p>the drift score is a measure for the difference between two distributions,
>   in this check - the test and train distributions. <br>
>   the check shows the drift score and distributions for the features,
>   sorted by feature importance and showing only the top 5 features, according to feature importance.
>   if available, the plot titles also show the feature importance (fi) rank.</p>
>   <p align=""left"">
>      <img src=""docs/source/_static/images/general/train-test-drift-output.png"">
>   </p>


## üôãüèº  when should you use deepchecks?

while you‚Äôre in the research phase, and want to validate your data, find potential methodological problems, 
and/or validate your model and evaluate it.


<p align=""center"">
   <img src=""/docs/source/_static/images/general/pipeline_when_to_validate.svg"">
</p>


see more about typical usage scenarios and the built-in suites in the [docs](
   https://docs.deepchecks.com/stable/getting-started/welcome.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=what_do_you_need_in_order_to_start_validating#when-should-you-use-deepchecks).


## üóùÔ∏è key concepts

### check

each check enables you to inspect a specific aspect of your data and
models. they are the basic building block of the deepchecks package,
covering all kinds of common issues, such as:

- weak segments performance
- train test feature drift
- date train test leakage overlap
- conflicting labels 

and [many more
checks](https://docs.deepchecks.com/stable/checks_gallery/tabular.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=key_concepts__check).


each check can have two types of
results:

1. a visual result meant for display (e.g. a figure or a table).
2. a return value that can be used for validating the expected check
   results (validations are typically done by adding a ""condition"" to
   the check, as explained below).

### condition

a condition is a function that can be added to a check, which returns a
pass ‚úì, fail ‚úñ or warning ! result, intended for validating the check's
return value. an example for adding a condition would be:

```python
from deepchecks.tabular.checks import boostingoverfit
boostingoverfit().add_condition_test_score_percent_decline_not_greater_than(threshold=0.05)
```

which will return a check failure when running it if there is a difference of
more than 5% between the best score achieved on the test set during the boosting
iterations and the score achieved in the last iteration (the model's ""original"" score
on the test set).

### suite

an ordered collection of checks, that can have conditions added to them.
the suite enables displaying a concluding report for all of the checks
that ran.

see the list of [predefined existing suites](deepchecks/tabular/suites)
for tabular data to learn more about the suites you can work with
directly and also to see a code example demonstrating how to build your
own custom suite.

the existing suites include default conditions added for most of the checks.
you can edit the preconfigured suites or build a suite of your own with a collection
of checks and optional conditions.


<p align=""center"">
   <img src=""/docs/source/_static/images/general/diagram.svg"">
</p>


## ü§î what do you need in order to start validating?

### environment

- the deepchecks package installed
- jupyterlab or jupyter notebook or any python ide


### data / model 

depending on your phase and what you wish to validate, you'll need a
subset of the following:

-  raw data (before pre-processing such as ohe, string processing,
   etc.), with optional labels
-  the model's training data with labels
-  test data (which the model isn't exposed to) with labels
-  a [supported
    model](https://docs.deepchecks.com/stable/user-guide/supported_models.html?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=running_a_suite) (e.g. scikit-learn models, xgboost, any model implementing the ``predict`` method in the required format)


### supported data types

the package currently supports tabular data and is in *beta release* for
the [computer vision subpackage](deepchecks/vision).


## üìñ documentation

-   [https://docs.deepchecks.com/](https://docs.deepchecks.com/?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=documentation) -   html documentation (latest release)
-   [https://docs.deepchecks.com/dev](https://docs.deepchecks.com/dev/?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=documentation) -   html documentation (dev version - git main branch)


## üë≠ community

-   join our [slack community](https://www.deepchecks.com/slack) to
    connect with the maintainers and follow users and interesting
    discussions
-   post a [github
    issue](https://github.com/deepchecks/deepchecks/issues) to suggest
    improvements, open an issue, or share feedback.



## ‚ú® contributors

thanks goes to these wonderful people ([emoji
key](https://allcontributors.org/docs/en/emoji-key)):


<!-- all-contributors-list:start - do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/itaygabbay""><img src=""https://avatars.githubusercontent.com/u/20860465?v=4?s=100"" width=""100px;"" alt=""itay gabbay""/><br /><sub><b>itay gabbay</b></sub></a><br /><a href=""#code-itaygabbay"" title=""code"">üíª</a> <a href=""#doc-itaygabbay"" title=""documentation"">üìñ</a> <a href=""#ideas-itaygabbay"" title=""ideas, planning, & feedback"">ü§î</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/matanper""><img src=""https://avatars.githubusercontent.com/u/9868530?v=4?s=100"" width=""100px;"" alt=""matanper""/><br /><sub><b>matanper</b></sub></a><br /><a href=""#doc-matanper"" title=""documentation"">üìñ</a> <a href=""#ideas-matanper"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""#code-matanper"" title=""code"">üíª</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jkl98isr""><img src=""https://avatars.githubusercontent.com/u/26321553?v=4?s=100"" width=""100px;"" alt=""jkl98isr""/><br /><sub><b>jkl98isr</b></sub></a><br /><a href=""#ideas-jkl98isr"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""#code-jkl98isr"" title=""code"">üíª</a> <a href=""#doc-jkl98isr"" title=""documentation"">üìñ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/yromanyshyn""><img src=""https://avatars.githubusercontent.com/u/71635444?v=4?s=100"" width=""100px;"" alt=""yurii romanyshyn""/><br /><sub><b>yurii romanyshyn</b></sub></a><br /><a href=""#ideas-yromanyshyn"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""#code-yromanyshyn"" title=""code"">üíª</a> <a href=""#doc-yromanyshyn"" title=""documentation"">üìñ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/noamzbr""><img src=""https://avatars.githubusercontent.com/u/17730502?v=4?s=100"" width=""100px;"" alt=""noam bressler""/><br /><sub><b>noam bressler</b></sub></a><br /><a href=""#code-noamzbr"" title=""code"">üíª</a> <a href=""#doc-noamzbr"" title=""documentation"">üìñ</a> <a href=""#ideas-noamzbr"" title=""ideas, planning, & feedback"">ü§î</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/nirhutnik""><img src=""https://avatars.githubusercontent.com/u/92314933?v=4?s=100"" width=""100px;"" alt=""nir hutnik""/><br /><sub><b>nir hutnik</b></sub></a><br /><a href=""#code-nirhutnik"" title=""code"">üíª</a> <a href=""#doc-nirhutnik"" title=""documentation"">üìñ</a> <a href=""#ideas-nirhutnik"" title=""ideas, planning, & feedback"">ü§î</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/nadav-barak""><img src=""https://avatars.githubusercontent.com/u/67195469?v=4?s=100"" width=""100px;"" alt=""nadav-barak""/><br /><sub><b>nadav-barak</b></sub></a><br /><a href=""#code-nadav-barak"" title=""code"">üíª</a> <a href=""#doc-nadav-barak"" title=""documentation"">üìñ</a> <a href=""#ideas-nadav-barak"" title=""ideas, planning, & feedback"">ü§î</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/thesoly""><img src=""https://avatars.githubusercontent.com/u/99395146?v=4?s=100"" width=""100px;"" alt=""sol""/><br /><sub><b>sol</b></sub></a><br /><a href=""#code-thesoly"" title=""code"">üíª</a> <a href=""#doc-thesoly"" title=""documentation"">üìñ</a> <a href=""#ideas-thesoly"" title=""ideas, planning, & feedback"">ü§î</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://www.linkedin.com/in/dan-arlowski""><img src=""https://avatars.githubusercontent.com/u/59116108?v=4?s=100"" width=""100px;"" alt=""danarlowski""/><br /><sub><b>danarlowski</b></sub></a><br /><a href=""#code-danarlowski"" title=""code"">üíª</a> <a href=""#infra-danarlowski"" title=""infrastructure (hosting, build-tools, etc)"">üöá</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/benisraeldan""><img src=""https://avatars.githubusercontent.com/u/42312361?v=4?s=100"" width=""100px;"" alt=""dbi""/><br /><sub><b>dbi</b></sub></a><br /><a href=""#code-benisraeldan"" title=""code"">üíª</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/orlyshmorly""><img src=""https://avatars.githubusercontent.com/u/110338263?v=4?s=100"" width=""100px;"" alt=""orlyshmorly""/><br /><sub><b>orlyshmorly</b></sub></a><br /><a href=""#design-orlyshmorly"" title=""design"">üé®</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/shir22""><img src=""https://avatars.githubusercontent.com/u/33841818?v=4?s=100"" width=""100px;"" alt=""shir22""/><br /><sub><b>shir22</b></sub></a><br /><a href=""#ideas-shir22"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""#doc-shir22"" title=""documentation"">üìñ</a> <a href=""#talk-shir22"" title=""talks"">üì¢</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/yaronzo1""><img src=""https://avatars.githubusercontent.com/u/107114284?v=4?s=100"" width=""100px;"" alt=""yaronzo1""/><br /><sub><b>yaronzo1</b></sub></a><br /><a href=""#ideas-yaronzo1"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""#content-yaronzo1"" title=""content"">üñã</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/ptannor""><img src=""https://avatars.githubusercontent.com/u/34207422?v=4?s=100"" width=""100px;"" alt=""ptannor""/><br /><sub><b>ptannor</b></sub></a><br /><a href=""#ideas-ptannor"" title=""ideas, planning, & feedback"">ü§î</a> <a href=""#content-ptannor"" title=""content"">üñã</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/avitzd""><img src=""https://avatars.githubusercontent.com/u/84308273?v=4?s=100"" width=""100px;"" alt=""avitzd""/><br /><sub><b>avitzd</b></sub></a><br /><a href=""#eventorganizing-avitzd"" title=""event organizing"">üìã</a> <a href=""#video-avitzd"" title=""videos"">üìπ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/danbasson""><img src=""https://avatars.githubusercontent.com/u/46203939?v=4?s=100"" width=""100px;"" alt=""danbasson""/><br /><sub><b>danbasson</b></sub></a><br /><a href=""#doc-danbasson"" title=""documentation"">üìñ</a> <a href=""#bug-danbasson"" title=""bug reports"">üêõ</a> <a href=""#example-danbasson"" title=""examples"">üí°</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/kishore-s-15""><img src=""https://avatars.githubusercontent.com/u/56688194?v=4?s=100"" width=""100px;"" alt=""s.kishore""/><br /><sub><b>s.kishore</b></sub></a><br /><a href=""#code-kishore-s-15"" title=""code"">üíª</a> <a href=""#doc-kishore-s-15"" title=""documentation"">üìñ</a> <a href=""#bug-kishore-s-15"" title=""bug reports"">üêõ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://www.shaypalachy.com/""><img src=""https://avatars.githubusercontent.com/u/917954?v=4?s=100"" width=""100px;"" alt=""shay palachy-affek""/><br /><sub><b>shay palachy-affek</b></sub></a><br /><a href=""#data-shaypal5"" title=""data"">üî£</a> <a href=""#example-shaypal5"" title=""examples"">üí°</a> <a href=""#usertesting-shaypal5"" title=""user testing"">üìì</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/cemalgurpinar""><img src=""https://avatars.githubusercontent.com/u/36713268?v=4?s=100"" width=""100px;"" alt=""cemal gurpinar""/><br /><sub><b>cemal gurpinar</b></sub></a><br /><a href=""#doc-cemalgurpinar"" title=""documentation"">üìñ</a> <a href=""#bug-cemalgurpinar"" title=""bug reports"">üêõ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/daavoo""><img src=""https://avatars.githubusercontent.com/u/12677733?v=4?s=100"" width=""100px;"" alt=""david de la iglesia castro""/><br /><sub><b>david de la iglesia castro</b></sub></a><br /><a href=""#code-daavoo"" title=""code"">üíª</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/tak""><img src=""https://avatars.githubusercontent.com/u/142250?v=4?s=100"" width=""100px;"" alt=""levi bard""/><br /><sub><b>levi bard</b></sub></a><br /><a href=""#doc-tak"" title=""documentation"">üìñ</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/julienschuermans""><img src=""https://avatars.githubusercontent.com/u/14927054?v=4?s=100"" width=""100px;"" alt=""julien schuermans""/><br /><sub><b>julien schuermans</b></sub></a><br /><a href=""#bug-julienschuermans"" title=""bug reports"">üêõ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://www.nirbenzvi.com""><img src=""https://avatars.githubusercontent.com/u/4930255?v=4?s=100"" width=""100px;"" alt=""nir ben-zvi""/><br /><sub><b>nir ben-zvi</b></sub></a><br /><a href=""#code-nirbenz"" title=""code"">üíª</a> <a href=""#ideas-nirbenz"" title=""ideas, planning, & feedback"">ü§î</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://ashtavakra.org""><img src=""https://avatars.githubusercontent.com/u/322451?v=4?s=100"" width=""100px;"" alt=""shiv shankar dayal""/><br /><sub><b>shiv shankar dayal</b></sub></a><br /><a href=""#infra-shivshankardayal"" title=""infrastructure (hosting, build-tools, etc)"">üöá</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/ronitay""><img src=""https://avatars.githubusercontent.com/u/33497483?v=4?s=100"" width=""100px;"" alt=""ronitay""/><br /><sub><b>ronitay</b></sub></a><br /><a href=""#bug-ronitay"" title=""bug reports"">üêõ</a> <a href=""#code-ronitay"" title=""code"">üíª</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://jeroen.vangoey.be""><img src=""https://avatars.githubusercontent.com/u/59344?v=4?s=100"" width=""100px;"" alt=""jeroen van goey""/><br /><sub><b>jeroen van goey</b></sub></a><br /><a href=""#bug-biogeek"" title=""bug reports"">üêõ</a> <a href=""#doc-biogeek"" title=""documentation"">üìñ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://about.me/ido.weiss""><img src=""https://avatars.githubusercontent.com/u/10072365?v=4?s=100"" width=""100px;"" alt=""idow09""/><br /><sub><b>idow09</b></sub></a><br /><a href=""#bug-idow09"" title=""bug reports"">üêõ</a> <a href=""#example-idow09"" title=""examples"">üí°</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://bandism.net/""><img src=""https://avatars.githubusercontent.com/u/22633385?v=4?s=100"" width=""100px;"" alt=""ikko ashimine""/><br /><sub><b>ikko ashimine</b></sub></a><br /><a href=""#doc-eltociear"" title=""documentation"">üìñ</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jhwohlgemuth""><img src=""https://avatars.githubusercontent.com/u/6383605?v=4?s=100"" width=""100px;"" alt=""jason wohlgemuth""/><br /><sub><b>jason wohlgemuth</b></sub></a><br /><a href=""#doc-jhwohlgemuth"" title=""documentation"">üìñ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://lokin.dev""><img src=""https://avatars.githubusercontent.com/u/34796341?v=4?s=100"" width=""100px;"" alt=""lokin sethia""/><br /><sub><b>lokin sethia</b></sub></a><br /><a href=""#code-alphabetagamer"" title=""code"">üíª</a> <a href=""#bug-alphabetagamer"" title=""bug reports"">üêõ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://www.ingomarquart.de""><img src=""https://avatars.githubusercontent.com/u/102803372?v=4?s=100"" width=""100px;"" alt=""ingo marquart""/><br /><sub><b>ingo marquart</b></sub></a><br /><a href=""#code-ingostatworx"" title=""code"">üíª</a> <a href=""#bug-ingostatworx"" title=""bug reports"">üêõ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/osw282""><img src=""https://avatars.githubusercontent.com/u/25309418?v=4?s=100"" width=""100px;"" alt=""oscar""/><br /><sub><b>oscar</b></sub></a><br /><a href=""#code-osw282"" title=""code"">üíª</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/rcwoolston""><img src=""https://avatars.githubusercontent.com/u/5957841?v=4?s=100"" width=""100px;"" alt=""richard w""/><br /><sub><b>richard w</b></sub></a><br /><a href=""#code-rcwoolston"" title=""code"">üíª</a> <a href=""#doc-rcwoolston"" title=""documentation"">üìñ</a> <a href=""#ideas-rcwoolston"" title=""ideas, planning, & feedback"">ü§î</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/bgalvao""><img src=""https://avatars.githubusercontent.com/u/17158288?v=4?s=100"" width=""100px;"" alt=""bernardo""/><br /><sub><b>bernardo</b></sub></a><br /><a href=""#code-bgalvao"" title=""code"">üíª</a> <a href=""#doc-bgalvao"" title=""documentation"">üìñ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://olivierbinette.github.io/""><img src=""https://avatars.githubusercontent.com/u/784901?v=4?s=100"" width=""100px;"" alt=""olivier binette""/><br /><sub><b>olivier binette</b></sub></a><br /><a href=""#code-olivierbinette"" title=""code"">üíª</a> <a href=""#doc-olivierbinette"" title=""documentation"">üìñ</a> <a href=""#ideas-olivierbinette"" title=""ideas, planning, & feedback"">ü§î</a></td>
    </tr>
    <tr>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/chendingyan""><img src=""https://avatars.githubusercontent.com/u/16874978?v=4?s=100"" width=""100px;"" alt=""ÈôàÈºéÂΩ¶""/><br /><sub><b>ÈôàÈºéÂΩ¶</b></sub></a><br /><a href=""#bug-chendingyan"" title=""bug reports"">üêõ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.k-lab.tk/""><img src=""https://avatars.githubusercontent.com/u/16821717?v=4?s=100"" width=""100px;"" alt=""andres vargas""/><br /><sub><b>andres vargas</b></sub></a><br /><a href=""#doc-vargasa"" title=""documentation"">üìñ</a></td>
      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/michaelmarien""><img src=""https://avatars.githubusercontent.com/u/13829139?v=4?s=100"" width=""100px;"" alt=""michael marien""/><br /><sub><b>michael marien</b></sub></a><br /><a href=""#doc-michaelmarien"" title=""documentation"">üìñ</a> <a href=""#bug-michaelmarien"" title=""bug reports"">üêõ</a></td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- all-contributors-list:end -->

this project follows the [all-contributors](https://allcontributors.org)
specification. contributions of any kind are welcome!
"
"Mars",".. image:: https://raw.githubusercontent.com/mars-project/mars/master/docs/source/images/mars-logo-title.png

|pypi version| |docs| |build| |coverage| |quality| |license|

mars is a tensor-based unified framework for large-scale data computation
which scales numpy, pandas, scikit-learn and many other libraries.

`documentation`_, `‰∏≠ÊñáÊñáÊ°£`_

installation
------------

mars is easy to install by

.. code-block:: bash

    pip install pymars


installation for developers
```````````````````````````

when you want to contribute code to mars, you can follow the instructions below to install mars
for development:

.. code-block:: bash

    git clone https://github.com/mars-project/mars.git
    cd mars
    pip install -e "".[dev]""

more details about installing mars can be found at
`installation <https://docs.pymars.org/en/latest/installation/index.html>`_ section in
mars document.


architecture overview
---------------------

.. image:: https://raw.githubusercontent.com/mars-project/mars/master/docs/source/images/architecture.png


getting started
---------------

starting a new runtime locally via:

.. code-block:: python

    >>> import mars
    >>> mars.new_session()

or connecting to a mars cluster which is already initialized.

.. code-block:: python

    >>> import mars
    >>> mars.new_session('http://<web_ip>:<ui_port>')


mars tensor
-----------

mars tensor provides a familiar interface like numpy.

+-----------------------------------------------+-----------------------------------------------+
| **numpy**                                     | **mars tensor**                               |
+-----------------------------------------------+-----------------------------------------------+
|.. code-block:: python                         |.. code-block:: python                         |
|                                               |                                               |
|    import numpy as np                         |    import mars.tensor as mt                   |
|    n = 200_000_000                            |    n = 200_000_000                            |
|    a = np.random.uniform(-1, 1, size=(n, 2))  |    a = mt.random.uniform(-1, 1, size=(n, 2))  |
|    print((np.linalg.norm(a, axis=1) < 1)      |    print(((mt.linalg.norm(a, axis=1) < 1)     |
|          .sum() * 4 / n)                      |            .sum() * 4 / n).execute())         |
|                                               |                                               |
+-----------------------------------------------+-----------------------------------------------+
|.. code-block::                                |.. code-block::                                |
|                                               |                                               |
|    3.14174502                                 |     3.14161908                                |
|    cpu times: user 11.6 s, sys: 8.22 s,       |     cpu times: user 966 ms, sys: 544 ms,      |
|               total: 19.9 s                   |                total: 1.51 s                  |
|    wall time: 22.5 s                          |     wall time: 3.77 s                         |
|                                               |                                               |
+-----------------------------------------------+-----------------------------------------------+

mars can leverage multiple cores, even on a laptop, and could be even faster for a distributed setting.


mars dataframe
--------------

mars dataframe provides a familiar interface like pandas.

+-----------------------------------------+-----------------------------------------+
| **pandas**                              | **mars dataframe**                      |
+-----------------------------------------+-----------------------------------------+
|.. code-block:: python                   |.. code-block:: python                   |
|                                         |                                         |
|    import numpy as np                   |    import mars.tensor as mt             |
|    import pandas as pd                  |    import mars.dataframe as md          |
|    df = pd.dataframe(                   |    df = md.dataframe(                   |
|        np.random.rand(100000000, 4),    |        mt.random.rand(100000000, 4),    |
|        columns=list('abcd'))            |        columns=list('abcd'))            |
|    print(df.sum())                      |    print(df.sum().execute())            |
|                                         |                                         |
+-----------------------------------------+-----------------------------------------+
|.. code-block::                          |.. code-block::                          |
|                                         |                                         |
|    cpu times: user 10.9 s, sys: 2.69 s, |    cpu times: user 1.21 s, sys: 212 ms, |
|               total: 13.6 s             |               total: 1.42 s             |
|    wall time: 11 s                      |    wall time: 2.75 s                    |
+-----------------------------------------+-----------------------------------------+


mars learn
----------

mars learn provides a familiar interface like scikit-learn.

+---------------------------------------------+----------------------------------------------------+
| **scikit-learn**                            | **mars learn**                                     |
+---------------------------------------------+----------------------------------------------------+
|.. code-block:: python                       |.. code-block:: python                              |
|                                             |                                                    |
|    from sklearn.datasets import make_blobs  |    from mars.learn.datasets import make_blobs      |
|    from sklearn.decomposition import pca    |    from mars.learn.decomposition import pca        |
|    x, y = make_blobs(                       |    x, y = make_blobs(                              |
|        n_samples=100000000, n_features=3,   |        n_samples=100000000, n_features=3,          |
|        centers=[[3, 3, 3], [0, 0, 0],       |        centers=[[3, 3, 3], [0, 0, 0],              |
|                 [1, 1, 1], [2, 2, 2]],      |                  [1, 1, 1], [2, 2, 2]],            |
|        cluster_std=[0.2, 0.1, 0.2, 0.2],    |        cluster_std=[0.2, 0.1, 0.2, 0.2],           |
|        random_state=9)                      |        random_state=9)                             |
|    pca = pca(n_components=3)                |    pca = pca(n_components=3)                       |
|    pca.fit(x)                               |    pca.fit(x)                                      |
|    print(pca.explained_variance_ratio_)     |    print(pca.explained_variance_ratio_)            |
|    print(pca.explained_variance_)           |    print(pca.explained_variance_)                  |
|                                             |                                                    |
+---------------------------------------------+----------------------------------------------------+

mars learn also integrates with many libraries:

- `tensorflow <https://docs.pymars.org/en/latest/user_guide/learn/tensorflow.html>`_
- `pytorch <https://docs.pymars.org/en/latest/user_guide/learn/pytorch.html>`_
- `xgboost <https://docs.pymars.org/en/latest/user_guide/learn/xgboost.html>`_
- `lightgbm <https://docs.pymars.org/en/latest/user_guide/learn/lightgbm.html>`_
- `joblib <https://docs.pymars.org/en/latest/user_guide/learn/joblib.html>`_
- `statsmodels <https://docs.pymars.org/en/latest/user_guide/learn/statsmodels.html>`_

mars remote
-----------

mars remote allows users to execute functions in parallel.

+-------------------------------------------+--------------------------------------------+
| **vanilla function calls**                | **mars remote**                            |
+-------------------------------------------+--------------------------------------------+
|.. code-block:: python                     |.. code-block:: python                      |
|                                           |                                            |
|    import numpy as np                     |    import numpy as np                      |
|                                           |    import mars.remote as mr                |
|                                           |                                            |
|    def calc_chunk(n, i):                  |    def calc_chunk(n, i):                   |
|        rs = np.random.randomstate(i)      |        rs = np.random.randomstate(i)       |
|        a = rs.uniform(-1, 1, size=(n, 2)) |        a = rs.uniform(-1, 1, size=(n, 2))  |
|        d = np.linalg.norm(a, axis=1)      |        d = np.linalg.norm(a, axis=1)       |
|        return (d < 1).sum()               |        return (d < 1).sum()                |
|                                           |                                            |
|    def calc_pi(fs, n):                    |    def calc_pi(fs, n):                     |
|        return sum(fs) * 4 / n             |        return sum(fs) * 4 / n              |
|                                           |                                            |
|    n = 200_000_000                        |    n = 200_000_000                         |
|    n = 10_000_000                         |    n = 10_000_000                          |
|                                           |                                            |
|    fs = [calc_chunk(n, i)                 |    fs = [mr.spawn(calc_chunk, args=(n, i)) |
|          for i in range(n // n)]          |          for i in range(n // n)]           |
|    pi = calc_pi(fs, n)                    |    pi = mr.spawn(calc_pi, args=(fs, n))    |
|    print(pi)                              |    print(pi.execute().fetch())             |
|                                           |                                            |
+-------------------------------------------+--------------------------------------------+
|.. code-block::                            |.. code-block::                             |
|                                           |                                            |
|    3.1416312                              |    3.1416312                               |
|    cpu times: user 32.2 s, sys: 4.86 s,   |    cpu times: user 616 ms, sys: 307 ms,    |
|               total: 37.1 s               |               total: 923 ms                |
|    wall time: 12.4 s                      |    wall time: 3.99 s                       |
|                                           |                                            |
+-------------------------------------------+--------------------------------------------+

dask on mars
------------

refer to `dask on mars`_ for more information.

eager mode
```````````

mars supports eager mode which makes it friendly for developing and easy to debug.

users can enable the eager mode by options, set options at the beginning of the program or console session.

.. code-block:: python

    >>> from mars.config import options
    >>> options.eager_mode = true

or use a context.

.. code-block:: python

    >>> from mars.config import option_context
    >>> with option_context() as options:
    >>>     options.eager_mode = true
    >>>     # the eager mode is on only for the with statement
    >>>     ...

if eager mode is on, tensor, dataframe etc will be executed immediately
by default session once it is created.

.. code-block:: python

    >>> import mars.tensor as mt
    >>> import mars.dataframe as md
    >>> from mars.config import options
    >>> options.eager_mode = true
    >>> t = mt.arange(6).reshape((2, 3))
    >>> t
    array([[0, 1, 2],
           [3, 4, 5]])
    >>> df = md.dataframe(t)
    >>> df.sum()
    0    3
    1    5
    2    7
    dtype: int64


mars on ray
------------
mars also has deep integration with ray and can run on `ray <https://docs.ray.io/en/latest/>`_ efficiently and
interact with the large ecosystem of machine learning and distributed systems built on top of the core ray.

starting a new mars on ray runtime locally via:

.. code-block:: python

    import mars
    mars.new_session(backend='ray')
    # perform computation

interact with ray dataset:

.. code-block:: python

    import mars.tensor as mt
    import mars.dataframe as md
    df = md.dataframe(
        mt.random.rand(1000_0000, 4),
        columns=list('abcd'))
    # convert mars dataframe to ray dataset
    ds = md.to_ray_dataset(df)
    print(ds.schema(), ds.count())
    ds.filter(lambda row: row[""a""] > 0.5).show(5)
    # convert ray dataset to mars dataframe
    df2 = md.read_ray_dataset(ds)
    print(df2.head(5).execute())

refer to `mars on ray`_ for more information.


easy to scale in and scale out
------------------------------

mars can scale in to a single machine, and scale out to a cluster with thousands of machines.
it's fairly simple to migrate from a single machine to a cluster to
process more data or gain a better performance.


bare metal deployment
`````````````````````

mars is easy to scale out to a cluster by starting different components of
mars distributed runtime on different machines in the cluster.

a node can be selected as supervisor which integrated a web service,
leaving other nodes as workers.  the supervisor can be started with the following command:

.. code-block:: bash

    mars-supervisor -h <host_name> -p <supervisor_port> -w <web_port>

workers can be started with the following command:

.. code-block:: bash

    mars-worker -h <host_name> -p <worker_port> -s <supervisor_endpoint>

after all mars processes are started, users can run

.. code-block:: python

    >>> sess = new_session('http://<web_ip>:<ui_port>')
    >>> # perform computation


kubernetes deployment
`````````````````````

refer to `run on kubernetes`_ for more information.


yarn deployment
```````````````

refer to `run on yarn`_ for more information.


getting involved
----------------

- read `development guide <https://docs.pymars.org/en/latest/development/index.html>`_.
- join our slack workgroup: `slack <https://join.slack.com/t/mars-computing/shared_invite/zt-17pw2cfua-nrb2h4vrg77pr9t4g3nqoq>`_.
- join the mailing list: send an email to `mars-dev@googlegroups.com`_.
- please report bugs by submitting a `github issue`_.
- submit contributions using `pull requests`_.

thank you in advance for your contributions!


.. |build| image:: https://github.com/mars-project/mars/workflows/mars%20ci%20core/badge.svg
   :target: https://github.com/mars-project/mars/actions
.. |coverage| image:: https://codecov.io/gh/mars-project/mars/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/mars-project/mars
.. |quality| image:: https://img.shields.io/codacy/grade/6a80bb4659ed410eb33795f580c8615e.svg
   :target: https://app.codacy.com/project/mars-project/mars/dashboard
.. |pypi version| image:: https://img.shields.io/pypi/v/pymars.svg
   :target: https://pypi.python.org/pypi/pymars
.. |docs| image:: https://img.shields.io/badge/docs-latest-brightgreen.svg
   :target: `documentation`_
.. |license| image:: https://img.shields.io/pypi/l/pymars.svg
   :target: https://github.com/mars-project/mars/blob/master/license
.. _`mars-dev@googlegroups.com`: https://groups.google.com/forum/#!forum/mars-dev
.. _`github issue`: https://github.com/mars-project/mars/issues
.. _`pull requests`: https://github.com/mars-project/mars/pulls
.. _`documentation`: https://docs.pymars.org
.. _`‰∏≠ÊñáÊñáÊ°£`: https://docs.pymars.org/zh_cn/latest/
.. _`mars on ray`: https://docs.pymars.org/en/latest/installation/ray.html
.. _`run on kubernetes`: https://docs.pymars.org/en/latest/installation/kubernetes.html
.. _`run on yarn`: https://docs.pymars.org/en/latest/installation/yarn.html
.. _`dask on mars`: https://docs.pymars.org/en/latest/user_guide/contrib/dask.html
"
"NetworkX","networkx
========

[networkx survey 2023!!](https://forms.gle/nugcbxyjx5onbagc8) üéâ fill out the survey to tell us about your ideas, complaints, praises of networkx!


.. image:: https://github.com/networkx/networkx/workflows/test/badge.svg?branch=main
  :target: https://github.com/networkx/networkx/actions?query=workflow%3a%22test%22

.. image:: https://codecov.io/gh/networkx/networkx/branch/main/graph/badge.svg
   :target: https://app.codecov.io/gh/networkx/networkx/branch/main
   
.. image:: https://img.shields.io/github/labels/networkx/networkx/good%20first%20issue?color=green&label=contribute%20&style=flat-square
   :target: https://github.com/networkx/networkx/issues?q=is%3aopen+is%3aissue+label%3a%22good+first+issue%22
   

networkx is a python package for the creation, manipulation,
and study of the structure, dynamics, and functions
of complex networks.

- **website (including documentation):** https://networkx.org
- **mailing list:** https://groups.google.com/forum/#!forum/networkx-discuss
- **source:** https://github.com/networkx/networkx
- **bug reports:** https://github.com/networkx/networkx/issues
- **report a security vulnerability:** https://tidelift.com/security
- **tutorial:** https://networkx.org/documentation/latest/tutorial.html
- **github discussions:** https://github.com/networkx/networkx/discussions

simple example
--------------

find the shortest path between two nodes in an undirected graph:

.. code:: pycon

    >>> import networkx as nx
    >>> g = nx.graph()
    >>> g.add_edge(""a"", ""b"", weight=4)
    >>> g.add_edge(""b"", ""d"", weight=2)
    >>> g.add_edge(""a"", ""c"", weight=3)
    >>> g.add_edge(""c"", ""d"", weight=4)
    >>> nx.shortest_path(g, ""a"", ""d"", weight=""weight"")
    ['a', 'b', 'd']

install
-------

install the latest version of networkx::

    $ pip install networkx

install with all optional dependencies::

    $ pip install networkx[all]

for additional details, please see `install.rst`.

bugs
----

please report any bugs that you find `here <https://github.com/networkx/networkx/issues>`_.
or, even better, fork the repository on `github <https://github.com/networkx/networkx>`_
and create a pull request (pr). we welcome all changes, big or small, and we
will help you make the pr if you are new to `git` (just ask on the issue and/or
see `contributing.rst`).

license
-------

released under the 3-clause bsd license (see `license.txt`)::

   copyright (c) 2004-2023 networkx developers
   aric hagberg <hagberg@lanl.gov>
   dan schult <dschult@colgate.edu>
   pieter swart <swart@lanl.gov>
"
"igraph","
[![build and test with tox](https://github.com/igraph/python-igraph/actions/workflows/build.yml/badge.svg)](https://github.com/igraph/python-igraph/actions/workflows/build.yml)
[![pypi pyversions](https://img.shields.io/pypi/pyversions/igraph)](https://pypi.python.org/pypi/igraph)
[![pypi wheels](https://img.shields.io/pypi/wheel/igraph.svg)](https://pypi.python.org/pypi/igraph)
[![documentation status](https://readthedocs.org/projects/igraph/badge/?version=latest)](https://igraph.readthedocs.io/)

python interface for the igraph library
---------------------------------------

igraph is a library for creating and manipulating graphs.
it is intended to be as powerful (ie. fast) as possible to enable the
analysis of large graphs.

this repository contains the source code to the python interface of
igraph.

since version 0.10.2, the documentation is hosted on
[readthedocs](https://igraph.readthedocs.io). earlier versions are documented
on [our old website](https://igraph.org/python/versions/0.10.1/).

igraph is a collaborative work of many people from all around the world ‚Äî
see the [list of contributors here](./contributors.md).

## installation from pypi

we aim to provide wheels on pypi for most of the stock python versions;
typically at least the three most recent minor releases from python 3.x.
therefore, running the following command should work without having to compile
anything during installation:

```
pip install igraph
```

see details in [installing python modules](https://docs.python.org/3/installing/).

### installation from source with pip on debian / ubuntu and derivatives

if you need to compile igraph from source for some reason, you need to
install some dependencies first:

```
sudo apt install build-essential python-dev libxml2 libxml2-dev zlib1g-dev
```

and then run

```
pip install igraph
```

this should compile the c core of igraph as well as the python extension
automatically.

### installation from source on windows

it is now also possible to compile `igraph` from source under windows for
python 3.7 and later. make sure that you have microsoft visual studio 2015 or
later installed, and of course python 3.7 or later. first extract the source to
a suitable directory. if you launch the developer command prompt and navigate to
the directory where you extracted the source code, you should be able to build
and install igraph using `python setup.py install`

you may need to set the architecture that you are building on explicitly by setting the environment variable

```
set igraph_cmake_extra_args=-a [arch]
```

where `[arch]` is either `win32` for 32-bit builds or `x64` for 64-bit builds.
also, when building in msys2, you need to set the `setuptools_use_distutils`
environment variable to `stdlib`; this is because msys2 uses a patched version
of `distutils` that conflicts with `setuptools >= 60.0`.

#### enabling graphml

by default, graphml is disabled, because `libxml2` is not available on windows in
the standard installation. you can install `libxml2` on windows using
[`vcpkg`](https://github.com/microsoft/vcpkg). after installation of `vcpkg` you
can install `libxml2` as follows

```
vcpkg.exe install libxml2:x64-windows-static-md
```

for 64-bit version (for 32-bit versions you can use the `x86-windows-static-md`
triplet). you need to integrate `vcpkg` in the build environment using

```
vcpkg.exe integrate install
```

this mentions that

> cmake projects should use: `-dcmake_toolchain_file=[vcpkg build script]`

which we will do next. in order to build `igraph` correctly, you also
need to set some other environment variables before building `igraph`:

```
set igraph_cmake_extra_args=-dvcpkg_target_triplet=x64-windows-static-md -dcmake_toolchain_file=[vcpkg build script]
set igraph_extra_library_path=[vcpkg directory]/installed/x64-windows-static-md/lib/
set igraph_static_extension=true
set igraph_extra_libraries=libxml2,lzma,zlib,iconv,charset
set igraph_extra_dynamic_libraries: wsock32,ws2_32
```

you can now build and install `igraph` again by simply running `python
setup.py build`. please make sure to use a clean source tree, if you built
previously without graphml, it will not update the build.

### linking to an existing igraph installation

the source code of the python package includes the source code of the matching
igraph version that the python interface should compile against. however, if
you want to link the python interface to a custom installation of the c core
that has already been compiled and installed on your system, you can ask
`setup.py` to use the pre-compiled version. this option requires that your
custom installation of igraph is discoverable with `pkg-config`. first, check
whether `pkg-config` can tell you the required compiler and linker flags for
igraph:

```bash
pkg-config --cflags --libs igraph
```

if `pkg-config` responds with a set of compiler and linker flags and not an
error message, you are probably okay. you can then proceed with the
installation using pip after setting the environment variable named
`igraph_use_pkg_config` to `1` to indicate that you want to use an
igraph instance discoverable with `pkg-config`:

```bash
igraph_use_pkg_config=1 pip install igraph
```

alternatively, if you have already downloaded and extracted the source code
of igraph, you can run `setup.py` directly:

```bash
igraph_use_pkg_config=1 python setup.py build
igraph_use_pkg_config=1 python setup.py install
```

(note that you need the `igraph_use_pkg_config=1` environment variable
for both invocations, otherwise the call to `setup.py install` would still
build the vendored c core instead of linking to an existing installation).

this option is primarily intended for package maintainers in linux
distributions so they can ensure that the packaged python interface links to
the packaged igraph library instead of bringing its own copy.

it is also useful on macos if you want to link to the igraph library installed
from homebrew.

due to the lack of support of `pkg-config` on windows, it is currently not
possible to build against an external library on windows.

**warning:** the python interface is guaranteed to work only with the same
version of the c core that is vendored inside the `vendor/source/igraph`
folder. while we try hard not to break api or abi in the c core of igraph
between minor versions in the 0.x branch and we will keep on doing so for major
versions once 1.0 is released, there are certain functions in the c api that
are marked as _experimental_ (see the documentation of the c core for details),
and we reserve the right to break the apis of those functions, even if they are
already exposed in a higher-level interface. this is because the easiest way to
test these functions in real-life research scenarios is to expose them in one
of the higher level interfaces. therefore, if you unbundle the vendored source
code of igraph and link to an external version instead, we can make no
guarantees about stability unless you link to the exact same version as the
one we have vendored in this source tree.

if you are curious about which version of the python interface is compatible
with which version of the c core, you can look up the corresponding tag in
github and check which revision of the c core the repository points to in
the `vendor/source/igraph` submodule.

## compiling the development version

if you want to install the development version, the easiest way to do so is to
install it using

```bash
pip install git+https://github.com/igraph/python-igraph
```

this automatically fetches the development version from the repository, builds
the package and installs it. by default, this will install the python interface
from the `main` branch, which is used as the basis for the development of the
current release series. unstable and breaking changes are being made in the
`develop` branch. you can install this similarly by doing

```bash
pip install git+https://github.com/igraph/python-igraph@develop
```

in addition to `git`, the installation of the development version requires some
additional dependencies, read further below for details.

for more information about installing directly from `git` using `pip` see 
https://pip.pypa.io/en/stable/topics/vcs-support/#git.


alternatively, you can clone this repository locally. this repository contains a
matching version of the c core of `igraph` as a git submodule. in order to
install the development version from source, you need to instruct git to check
out the submodules first:

```bash
git submodule update --init
```

compiling the development version additionally requires `flex` and `bison`. you
can install those on ubuntu using

```bash
sudo apt install bison flex
```

on macos you can install these from homebrew or macports. on windows you can
install `winflexbison3` from chocolatey.

then, running the setup script should work if you have a c compiler and the
necessary build dependencies (see also the previous section):

```bash
python setup.py build
```

you can install it using

```bash
python setup.py install
```

### running unit tests

unit tests can be executed from within the repository directory with `tox` or
with the built-in `unittest` module:

```bash
python -m unittest
```

## contributing

contributions to `igraph` are welcome!

if you want to add a feature, fix a bug, or suggest an improvement, open an
issue on this repository and we'll try to answer. if you have a piece of code
that you would like to see included in the main tree, open a pr on this repo.

to start developing `igraph`, follow the steps above about installing the development version. make sure that you do so by cloning the repository locally so that you are able to make changes.

for easier development, you can install `igraph` in development mode so your changes in the python source
code are picked up automatically by python:

```bash
python setup.py develop
```

changes that you make to the python code do not need any extra action. however,
if you adjust the source code of the c extension, you need to rebuild it by running
`python setup.py develop` again. compilation of the c core of `igraph` is
cached in ``vendor/build`` and ``vendor/install`` so subsequent builds are much
faster than the first one as the c core does not need to be recompiled.

## notes

### supported python versions

we aim to keep up with the development cycle of python and support all official
python versions that have not reached their end of life yet. currently this
means that we support python 3.7 to 3.11, inclusive. please refer to [this
page](https://devguide.python.org/versions/) for the status of python
branches and let us know if you encounter problems with `igraph` on any
of the non-eol python versions.

continuous integration tests are regularly executed on all non-eol python
branches.

### pypy

this version of igraph is compatible with [pypy](http://pypy.org/) and
is regularly tested on [pypy](http://pypy.org/) with ``tox``. however, the
pypy version falls behind the cpython version in terms of performance; for
instance, running all the tests takes ~5 seconds on my machine with cpython and
~15 seconds with pypy. this can probably be attributed to the need for
emulating cpython reference counting, and does not seem to be alleviated by the
jit.

there are also some subtle differences between the cpython and pypy versions:

- docstrings defined in the c source code are not visible from pypy.

- ``graphbase`` is hashable and iterable in pypy but not in cpython. since
  ``graphbase`` is internal anyway, this is likely to stay this way.

"
"Pandas","<div align=""center"">
  <img src=""https://pandas.pydata.org/static/img/pandas.svg""><br>
</div>

-----------------

# pandas: powerful python data analysis toolkit
[![pypi latest release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/)
[![conda latest release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/anaconda/pandas/)
[![doi](https://zenodo.org/badge/doi/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134)
[![package status](https://img.shields.io/pypi/status/pandas.svg)](https://pypi.org/project/pandas/)
[![license](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/main/license)
[![coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas)
[![downloads](https://static.pepy.tech/personalized-badge/pandas?period=month&units=international_system&left_color=black&right_color=orange&left_text=pypi%20downloads%20per%20month)](https://pepy.tech/project/pandas)
[![slack](https://img.shields.io/badge/join_slack-information-brightgreen.svg?logo=slack)](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack)
[![powered by numfocus](https://img.shields.io/badge/powered%20by-numfocus-orange.svg?style=flat&colora=e1523d&colorb=007d8a)](https://numfocus.org)
[![code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelcolor=ef8336)](https://pycqa.github.io/isort/)

## what is it?

**pandas** is a python package that provides fast, flexible, and expressive data
structures designed to make working with ""relational"" or ""labeled"" data both
easy and intuitive. it aims to be the fundamental high-level building block for
doing practical, **real world** data analysis in python. additionally, it has
the broader goal of becoming **the most powerful and flexible open source data
analysis / manipulation tool available in any language**. it is already well on
its way towards this goal.

## main features
here are just a few of the things that pandas does well:

  - easy handling of [**missing data**][missing-data] (represented as
    `nan`, `na`, or `nat`) in floating point as well as non-floating point data
  - size mutability: columns can be [**inserted and
    deleted**][insertion-deletion] from dataframe and higher dimensional
    objects
  - automatic and explicit [**data alignment**][alignment]: objects can
    be explicitly aligned to a set of labels, or the user can simply
    ignore the labels and let `series`, `dataframe`, etc. automatically
    align the data for you in computations
  - powerful, flexible [**group by**][groupby] functionality to perform
    split-apply-combine operations on data sets, for both aggregating
    and transforming data
  - make it [**easy to convert**][conversion] ragged,
    differently-indexed data in other python and numpy data structures
    into dataframe objects
  - intelligent label-based [**slicing**][slicing], [**fancy
    indexing**][fancy-indexing], and [**subsetting**][subsetting] of
    large data sets
  - intuitive [**merging**][merging] and [**joining**][joining] data
    sets
  - flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of
    data sets
  - [**hierarchical**][mi] labeling of axes (possible to have multiple
    labels per tick)
  - robust io tools for loading data from [**flat files**][flat-files]
    (csv and delimited), [**excel files**][excel], [**databases**][db],
    and saving/loading data from the ultrafast [**hdf5 format**][hdfstore]
  - [**time series**][timeseries]-specific functionality: date range
    generation and frequency conversion, moving window statistics,
    date shifting and lagging


   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html
   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion
   [alignment]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures
   [groupby]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine
   [conversion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe
   [slicing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges
   [fancy-indexing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced
   [subsetting]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing
   [merging]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging
   [joining]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-on-index
   [reshape]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [pivot-table]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [mi]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#hierarchical-indexing-multiindex
   [flat-files]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#csv-text-files
   [excel]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excel-files
   [db]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries
   [hdfstore]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#hdf5-pytables
   [timeseries]: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-series-date-functionality

## where to get it
the source code is currently hosted on github at:
https://github.com/pandas-dev/pandas

binary installers for the latest released version are available at the [python
package index (pypi)](https://pypi.org/project/pandas) and on [conda](https://docs.conda.io/en/latest/).

```sh
# conda
conda install pandas
```

```sh
# or pypi
pip install pandas
```

## dependencies
- [numpy - adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays](https://www.numpy.org)
- [python-dateutil - provides powerful extensions to the standard datetime module](https://dateutil.readthedocs.io/en/stable/index.html)
- [pytz - brings the olson tz database into python which allows accurate and cross platform timezone calculations](https://github.com/stub42/pytz)

see the [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies) for minimum supported versions of required, recommended and optional dependencies.

## installation from sources
to install pandas from source you need [cython](https://cython.org/) in addition to the normal
dependencies above. cython can be installed from pypi:

```sh
pip install cython
```

in the `pandas` directory (same one where you found this file after
cloning the git repo), execute:

```sh
python setup.py install
```

or for installing in [development mode](https://pip.pypa.io/en/latest/cli/pip_install/#install-editable):


```sh
python -m pip install -e . --no-build-isolation --no-use-pep517
```

or alternatively

```sh
python setup.py develop
```

see the full instructions for [installing from source](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-from-source).

## license
[bsd 3](license)

## documentation
the official documentation is hosted on pydata.org: https://pandas.pydata.org/pandas-docs/stable

## background
work on ``pandas`` started at [aqr](https://www.aqr.com/) (a quantitative hedge fund) in 2008 and
has been under active development since then.

## getting help

for usage questions, the best place to go to is [stackoverflow](https://stackoverflow.com/questions/tagged/pandas).
further, general questions and discussions can also take place on the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata).

## discussion and development
most development discussions take place on github in this repo. further, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [slack channel](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) is available for quick development related questions.

## contributing to pandas [![open source helpers](https://www.codetriage.com/pandas-dev/pandas/badges/users.svg)](https://www.codetriage.com/pandas-dev/pandas)

all contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.

a detailed overview on how to contribute can be found in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**.

if you are simply looking to start working with the pandas codebase, navigate to the [github ""issues"" tab](https://github.com/pandas-dev/pandas/issues) and start looking through interesting issues. there are a number of issues listed under [docs](https://github.com/pandas-dev/pandas/issues?labels=docs&sort=updated&state=open) and [good first issue](https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&sort=updated&state=open) where you could start out.

you can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. if you would like to start triaging issues, one easy way to get started is to [subscribe to pandas on codetriage](https://www.codetriage.com/pandas-dev/pandas).

or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‚Äòthis can be improved‚Äô...you can do something about it!

feel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata) or on [slack](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack).

as contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. more information can be found at: [contributor code of conduct](https://github.com/pandas-dev/.github/blob/master/code_of_conduct.md)
"
"Open Mining","open mining
===========

.. image:: https://circleci.com/gh/mining/mining/tree/master.svg?style=svg
    :target: https://circleci.com/gh/mining/mining/tree/master
    :alt: build status - circle ci

.. image:: https://coveralls.io/repos/github/mining/mining/badge.svg?branch=master
    :target: https://coveralls.io/github/mining/mining?branch=master

.. image:: https://landscape.io/github/mining/mining/master/landscape.svg?style=flat
   :target: https://landscape.io/github/mining/mining/master
   :alt: code health


.. image:: https://raw.githubusercontent.com/mining/frontend/master/assets/image/openmining.io.png
    :alt: openmining

business intelligence (bi) application server written in python


requirements
------------

* python 2.7 (backend)
* lua 5.2 or luajit 5.1 (oml backend)
* mongodb (admin)
* redis (queue and datawarehouse)
* bower (install frontend libs, nodejs depends)


install dependencies
-------

.. code:: bash
    
    $ sudo apt-get install mongodb-10gen redis-server nodejs nodejs-dev npm
    $ npm install bower


if you use mac osx you can install all dependencies using `homebrew <http://brew.sh/>`_.


install open mining
-------

**clone the repository**

.. code:: bash

    $ git clone git@github.com:mining/mining.git

**install python and bower dependencies using make command**

.. code:: bash

    $ make build

**faq**

**if mongodb or redis-server problems**

install mongodb and redis-server, make sure it running


**supported databases**

for example, to connect to a postgresql database make sure you install a driver like **psycopg2**. openmining supports all databases that the underlying orm sqlalchemy supports.

see the `sqlalchemy documentation <http://docs.sqlalchemy.org/en/rel_0_9/core/engines.html>`_ for more info about drivers and connection strings.


run
---

.. code:: bash

    python manage.py runserver
    python manage.py celery
    python manage.py scheduler


running demo
------------

make sure runserver still running when run 'build_demo' command.

.. code:: bash

    python manage.py runserver
    python manage.py build_demo


and now you can login with: username 'admin' and password 'admin'.

screenshots
-----------

**dashboard openmining**

.. image:: https://raw.github.com/mining/mining/master/docs/docs/img/dashboard-openmining_new.png
    :alt: dashboard openmining

**dashboard charts openmining**

.. image:: https://raw.github.com/mining/mining/master/docs/docs/img/charts-openmining_new.png
    :alt: dashboard charts openmining

**dashboard charts openmining**

.. image:: https://raw.github.com/mining/mining/master/docs/docs/img/charts2-openmining_new.png
    :alt: dashboard charts openmining

**dashboard widgets openmining**

.. image:: https://raw.github.com/mining/mining/master/docs/docs/img/widgets-openmining_new.png
    :alt: dashboard widgets openmining


**late scheduler and running cubes openmining**

.. image:: https://raw.github.com/mining/mining/master/docs/docs/img/late-scheduler-openmining_new.png
    :alt: late scheduler and running cubes openmining


contribute
----------

join us on irc at **#openmining** on freenode (`web access <http://webchat.freenode.net/?channels=openmining>`_).


credits
-------

authors: `avelino <https://github.com/avelino/>`_ and `up! ess√™ncia <http://www.upessencia.com.br/>`_

many thanks to all the contributors!


license
-------

licensed under the mit license (see the (`license file <https://github.com/mining/mining/blob/master/license>`_).
"
"PyMC",".. image:: https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/pymc_banner.svg
    :height: 100px
    :alt: pymc logo
    :align: center

|build status| |coverage| |numfocus_badge| |binder| |dockerhub| |doizenodo|

pymc (formerly pymc3) is a python package for bayesian statistical modeling
focusing on advanced markov chain monte carlo (mcmc) and variational inference (vi)
algorithms. its flexibility and extensibility make it applicable to a
large suite of problems.

check out the `pymc overview <https://docs.pymc.io/en/latest/learn/core_notebooks/pymc_overview.html>`__,  or
one of `the many examples <https://www.pymc.io/projects/examples/en/latest/gallery.html>`__!
for questions on pymc, head on over to our `pymc discourse <https://discourse.pymc.io/>`__ forum.

features
========

-  intuitive model specification syntax, for example, ``x ~ n(0,1)``
   translates to ``x = normal('x',0,1)``
-  **powerful sampling algorithms**, such as the `no u-turn
   sampler <http://www.jmlr.org/papers/v15/hoffman14a.html>`__, allow complex models
   with thousands of parameters with little specialized knowledge of
   fitting algorithms.
-  **variational inference**: `advi <http://www.jmlr.org/papers/v18/16-107.html>`__
   for fast approximate posterior estimation as well as mini-batch advi
   for large data sets.
-  relies on `pytensor <https://pytensor.readthedocs.io/en/latest/>`__ which provides:
    *  computation optimization and dynamic c or jax compilation
    *  numpy broadcasting and advanced indexing
    *  linear algebra operators
    *  simple extensibility
-  transparent support for missing value imputation

getting started
===============

if you already know about bayesian statistics:
----------------------------------------------

-  `api quickstart guide <https://www.pymc.io/projects/examples/en/latest/howto/api_quickstart.html>`__
-  the `pymc tutorial <https://docs.pymc.io/en/latest/learn/core_notebooks/pymc_overview.html>`__
-  `pymc examples <https://www.pymc.io/projects/examples/en/latest/gallery.html>`__ and the `api reference <https://docs.pymc.io/en/stable/api.html>`__

learn bayesian statistics with a book together with pymc
--------------------------------------------------------

-  `probabilistic programming and bayesian methods for hackers <https://github.com/camdavidsonpilon/probabilistic-programming-and-bayesian-methods-for-hackers>`__: fantastic book with many applied code examples.
-  `pymc port of the book ""doing bayesian data analysis"" by john kruschke <https://github.com/aloctavodia/doing_bayesian_data_analysis>`__ as well as the `second edition <https://github.com/jwarmenhoven/dbda-python>`__: principled introduction to bayesian data analysis.
-  `pymc port of the book ""statistical rethinking a bayesian course with examples in r and stan"" by richard mcelreath <https://github.com/pymc-devs/resources/tree/master/rethinking>`__
-  `pymc port of the book ""bayesian cognitive modeling"" by michael lee and ej wagenmakers <https://github.com/pymc-devs/resources/tree/master/bcm>`__: focused on using bayesian statistics in cognitive modeling.
-  `bayesian analysis with python  <https://www.packtpub.com/big-data-and-business-intelligence/bayesian-analysis-python-second-edition>`__ (second edition) by osvaldo martin: great introductory book. (`code <https://github.com/aloctavodia/bap>`__ and errata).

audio & video
-------------

- here is a `youtube playlist <https://www.youtube.com/playlist?list=pl1ma_1dbbe82ovw8fz_6ts1ooeyoaiovy>`__ gathering several talks on pymc.
- you can also find all the talks given at **pymcon 2020** `here <https://discourse.pymc.io/c/pymcon/2020talks/15>`__.
- the `""learning bayesian statistics"" podcast <https://www.learnbayesstats.com/>`__ helps you discover and stay up-to-date with the vast bayesian community. bonus: it's hosted by alex andorra, one of the pymc core devs!

installation
============

to install pymc on your system, follow the instructions on the `installation guide <https://www.pymc.io/projects/docs/en/latest/installation.html>`__.

citing pymc
===========
please choose from the following:

- |doipaper| *probabilistic programming in python using pymc3*, salvatier j., wiecki t.v., fonnesbeck c. (2016)
- |doizenodo| a doi for all versions.
- dois for specific versions are shown on zenodo and under `releases <https://github.com/pymc-devs/pymc/releases>`_

.. |doipaper| image:: https://img.shields.io/badge/doi-10.7717%2fpeerj--cs.55-blue
     :target: https://doi.org/10.7717/peerj-cs.55
.. |doizenodo| image:: https://zenodo.org/badge/doi/10.5281/zenodo.4603970.svg
   :target: https://doi.org/10.5281/zenodo.4603970

contact
=======

we are using `discourse.pymc.io <https://discourse.pymc.io/>`__ as our main communication channel.

to ask a question regarding modeling or usage of pymc we encourage posting to our discourse forum under the `‚Äúquestions‚Äù category <https://discourse.pymc.io/c/questions>`__. you can also suggest feature in the `‚Äúdevelopment‚Äù category <https://discourse.pymc.io/c/development>`__.

you can also follow us on these social media platforms for updates and other announcements:

- `linkedin @pymc <https://www.linkedin.com/company/pymc/>`__
- `youtube @pymcdevelopers <https://www.youtube.com/c/pymcdevelopers>`__
- `twitter @pymc_devs <https://twitter.com/pymc_devs>`__
- `mastodon @pymc@bayes.club <https://bayes.club/@pymc>`__

to report an issue with pymc please use the `issue tracker <https://github.com/pymc-devs/pymc/issues>`__.

finally, if you need to get in touch for non-technical information about the project, `send us an e-mail <info@pymc-devs.org>`__.

license
=======

`apache license, version
2.0 <https://github.com/pymc-devs/pymc/blob/main/license>`__


software using pymc
===================

general purpose
---------------

- `bambi <https://github.com/bambinos/bambi>`__: bayesian model-building interface (bambi) in python.
- `calibr8 <https://calibr8.readthedocs.io>`__: a toolbox for constructing detailed observation models to be used as likelihoods in pymc.
- `gumbi <https://github.com/johngoertz/gumbi>`__: a high-level interface for building gp models.
- `sunode <https://github.com/aseyboldt/sunode>`__: fast ode solver, much faster than the one that comes with pymc.
- `pymc-learn <https://github.com/pymc-learn/pymc-learn>`__: custom pymc models built on top of pymc3_models/scikit-learn api

domain specific
---------------

- `exoplanet <https://github.com/dfm/exoplanet>`__: a toolkit for modeling of transit and/or radial velocity observations of exoplanets and other astronomical time series.
- `beat <https://github.com/hvasbath/beat>`__: bayesian earthquake analysis tool.
- `causalpy <https://github.com/pymc-labs/causalpy>`__: a package focussing on causal inference in quasi-experimental settings.

please contact us if your software is not listed here.

papers citing pymc
==================

see `google scholar <https://scholar.google.de/scholar?oi=bibs&hl=en&authuser=1&cites=6936955228135731011>`__ for a continuously updated list.

contributors
============

see the `github contributor
page <https://github.com/pymc-devs/pymc/graphs/contributors>`__. also read our `code of conduct <https://github.com/pymc-devs/pymc/blob/main/code_of_conduct.md>`__ guidelines for a better contributing experience.

support
=======

pymc is a non-profit project under numfocus umbrella. if you want to support pymc financially, you can donate `here <https://numfocus.salsalabs.org/donate-to-pymc3/index.html>`__.

professional consulting support
===============================

you can get professional consulting support from `pymc labs <https://www.pymc-labs.io>`__.

sponsors
========

|numfocus|

|pymclabs|

.. |binder| image:: https://mybinder.org/badge_logo.svg
   :target: https://mybinder.org/v2/gh/pymc-devs/pymc/main?filepath=%2fdocs%2fsource%2fnotebooks
.. |build status| image:: https://github.com/pymc-devs/pymc/workflows/pytest/badge.svg
   :target: https://github.com/pymc-devs/pymc/actions
.. |coverage| image:: https://codecov.io/gh/pymc-devs/pymc/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/pymc-devs/pymc
.. |dockerhub| image:: https://img.shields.io/docker/automated/pymc/pymc.svg
   :target: https://hub.docker.com/r/pymc/pymc
.. |numfocus| image:: https://www.numfocus.org/wp-content/uploads/2017/03/1457562110.png
   :target: http://www.numfocus.org/
.. |numfocus_badge| image:: https://img.shields.io/badge/powered%20by-numfocus-orange.svg?style=flat&colora=e1523d&colorb=007d8a
   :target: http://www.numfocus.org/
.. |pymclabs| image:: https://raw.githubusercontent.com/pymc-devs/pymc/main/docs/logos/sponsors/pymc-labs.png
   :target: https://pymc-labs.io
"
"zipline",".. image:: https://media.quantopian.com/logos/open_source/zipline-logo-03_.png
    :target: https://www.zipline.io
    :width: 212px
    :align: center
    :alt: zipline

=============

|gitter|
|pypi version status|
|pypi pyversion status|
|travis status|
|appveyor status|
|coverage status|

zipline is a pythonic algorithmic trading library. it is an event-driven
system for backtesting. zipline is currently used in production as the backtesting and live-trading
engine powering `quantopian <https://www.quantopian.com>`_ -- a free,
community-centered, hosted platform for building and executing trading
strategies. quantopian also offers a `fully managed service for professionals <https://factset.quantopian.com>`_
that includes zipline, alphalens, pyfolio, factset data, and more.

- `join our community! <https://groups.google.com/forum/#!forum/zipline>`_
- `documentation <https://www.zipline.io>`_
- want to contribute? see our `development guidelines <https://www.zipline.io/development-guidelines>`_

features
========

- **ease of use:** zipline tries to get out of your way so that you can
  focus on algorithm development. see below for a code example.
- **""batteries included"":** many common statistics like
  moving average and linear regression can be readily accessed from
  within a user-written algorithm.
- **pydata integration:** input of historical data and output of performance statistics are
  based on pandas dataframes to integrate nicely into the existing
  pydata ecosystem.
- **statistics and machine learning libraries:** you can use libraries like matplotlib, scipy,
  statsmodels, and sklearn to support development, analysis, and
  visualization of state-of-the-art trading systems.

installation
============

zipline currently supports python 2.7, 3.5, and 3.6, and may be installed via
either pip or conda.

**note:** installing zipline is slightly more involved than the average python
package. see the full `zipline install documentation`_ for detailed
instructions.

for a development installation (used to develop zipline itself), create and
activate a virtualenv, then run the ``etc/dev-install`` script.

quickstart
==========

see our `getting started tutorial <https://www.zipline.io/beginner-tutorial>`_.

the following code implements a simple dual moving average algorithm.

.. code:: python

    from zipline.api import order_target, record, symbol

    def initialize(context):
        context.i = 0
        context.asset = symbol('aapl')


    def handle_data(context, data):
        # skip first 300 days to get full windows
        context.i += 1
        if context.i < 300:
            return

        # compute averages
        # data.history() has to be called with the same params
        # from above and returns a pandas dataframe.
        short_mavg = data.history(context.asset, 'price', bar_count=100, frequency=""1d"").mean()
        long_mavg = data.history(context.asset, 'price', bar_count=300, frequency=""1d"").mean()

        # trading logic
        if short_mavg > long_mavg:
            # order_target orders as many shares as needed to
            # achieve the desired number of shares.
            order_target(context.asset, 100)
        elif short_mavg < long_mavg:
            order_target(context.asset, 0)

        # save values for later inspection
        record(aapl=data.current(context.asset, 'price'),
               short_mavg=short_mavg,
               long_mavg=long_mavg)


you can then run this algorithm using the zipline cli.
first, you must download some sample pricing and asset data:

.. code:: bash

    $ zipline ingest
    $ zipline run -f dual_moving_average.py --start 2014-1-1 --end 2018-1-1 -o dma.pickle --no-benchmark

this will download asset pricing data data sourced from quandl, and stream it through the algorithm over the specified time range.
then, the resulting performance dataframe is saved in ``dma.pickle``, which you can load and analyze from within python.

you can find other examples in the ``zipline/examples`` directory.

questions?
==========

if you find a bug, feel free to `open an issue <https://github.com/quantopian/zipline/issues/new>`_ and fill out the issue template.

contributing
============

all contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome. details on how to set up a development environment can be found in our `development guidelines <https://www.zipline.io/development-guidelines>`_.

if you are looking to start working with the zipline codebase, navigate to the github `issues` tab and start looking through interesting issues. sometimes there are issues labeled as `beginner friendly <https://github.com/quantopian/zipline/issues?q=is%3aissue+is%3aopen+label%3a%22beginner+friendly%22>`_ or `help wanted <https://github.com/quantopian/zipline/issues?q=is%3aissue+is%3aopen+label%3a%22help+wanted%22>`_.

feel free to ask questions on the `mailing list <https://groups.google.com/forum/#!forum/zipline>`_ or on `gitter <https://gitter.im/quantopian/zipline>`_.

.. note::

   please note that zipline is not a community-led project. zipline is
   maintained by the quantopian engineering team, and we are quite small and
   often busy.

   because of this, we want to warn you that we may not attend to your pull
   request, issue, or direct mention in months, or even years. we hope you
   understand, and we hope that this note might help reduce any frustration or
   wasted time.


.. |gitter| image:: https://badges.gitter.im/join%20chat.svg
   :target: https://gitter.im/quantopian/zipline?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
.. |pypi version status| image:: https://img.shields.io/pypi/v/zipline.svg
   :target: https://pypi.python.org/pypi/zipline
.. |pypi pyversion status| image:: https://img.shields.io/pypi/pyversions/zipline.svg
   :target: https://pypi.python.org/pypi/zipline
.. |travis status| image:: https://travis-ci.org/quantopian/zipline.svg?branch=master
   :target: https://travis-ci.org/quantopian/zipline
.. |appveyor status| image:: https://ci.appveyor.com/api/projects/status/3dg18e6227dvstw6/branch/master?svg=true
   :target: https://ci.appveyor.com/project/quantopian/zipline/branch/master
.. |coverage status| image:: https://coveralls.io/repos/quantopian/zipline/badge.svg
   :target: https://coveralls.io/r/quantopian/zipline

.. _`zipline install documentation` : https://www.zipline.io/install
"
"SymPy","# sympy

[![pypi version](https://img.shields.io/pypi/v/sympy.svg)](https://pypi.python.org/pypi/sympy)
[![join the chat at https://gitter.im/sympy/sympy](https://badges.gitter.im/join%20chat.svg)](https://gitter.im/sympy/sympy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![zenodo badge](https://zenodo.org/badge/18918/sympy/sympy.svg)](https://zenodo.org/badge/latestdoi/18918/sympy/sympy)
[![downloads](https://pepy.tech/badge/sympy/month)](https://pepy.tech/project/sympy)
[![github issues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/sympy/sympy/issues)
[![git tutorial](https://img.shields.io/badge/pr-welcome-%23ff8300.svg?)](https://git-scm.com/book/en/v2/github-contributing-to-a-project)
[![powered by numfocus](https://img.shields.io/badge/powered%20by-numfocus-orange.svg?style=flat&colora=e1523d&colorb=007d8a)](https://numfocus.org)
[![commits since last release](https://img.shields.io/github/commits-since/sympy/sympy/latest.svg?longcache=true&style=flat-square&logo=git&logocolor=fff)](https://github.com/sympy/sympy/releases)

[![sympy banner](https://github.com/sympy/sympy/raw/master/banner.svg)](https://sympy.org/)


see the [authors](authors) file for the list of authors.

and many more people helped on the sympy mailing list, reported bugs,
helped organize sympy's participation in the google summer of code, the
google highly open participation contest, google code-in, wrote and
blogged about sympy...

license: new bsd license (see the [license](license) file for details) covers all
files in the sympy repository unless stated otherwise.

our mailing list is at
<https://groups.google.com/forum/?fromgroups#!forum/sympy>.

we have a community chat at [gitter](https://gitter.im/sympy/sympy). feel
free to ask us anything there. we have a very welcoming and helpful
community.

## download

the recommended installation method is through anaconda,
<https://www.anaconda.com/download/>

you can also get the latest version of sympy from
<https://pypi.python.org/pypi/sympy/>

to get the git version do

    $ git clone https://github.com/sympy/sympy.git

for other options (tarballs, debs, etc.), see
<https://docs.sympy.org/dev/install.html>.

## documentation and usage

for in-depth instructions on installation and building the
documentation, see the [sympy documentation style guide](https://docs.sympy.org/dev/documentation-style-guide.html).

everything is at:

<https://docs.sympy.org/>

you can generate everything at the above site in your local copy of
sympy by:

    $ cd doc
    $ make html

then the docs will be in <span class=""title-ref"">\_build/html</span>. if
you don't want to read that, here is a short usage:

from this directory, start python and:

``` python
>>> from sympy import symbol, cos
>>> x = symbol('x')
>>> e = 1/cos(x)
>>> print(e.series(x, 0, 10))
1 + x**2/2 + 5*x**4/24 + 61*x**6/720 + 277*x**8/8064 + o(x**10)
```

sympy also comes with a console that is a simple wrapper around the
classic python console (or ipython when available) that loads the sympy
namespace and executes some common commands for you.

to start it, issue:

    $ bin/isympy

from this directory, if sympy is not installed or simply:

    $ isympy

if sympy is installed.

## installation

sympy has a hard dependency on the [mpmath](http://mpmath.org/) library
(version \>= 0.19). you should install it first, please refer to the
mpmath installation guide:

<https://github.com/fredrik-johansson/mpmath#1-download--installation>

to install sympy using pypi, run the following command:

    $ pip install sympy

to install sympy using anaconda, run the following command:

    $ conda install -c anaconda sympy

to install sympy from github source, first clone sympy using `git`:

    $ git clone https://github.com/sympy/sympy.git

then, in the `sympy` repository that you cloned, simply run:

    $ python setup.py install

see <https://docs.sympy.org/dev/install.html> for more information.

## contributing

we welcome contributions from anyone, even if you are new to open
source. please read our [introduction to contributing](https://github.com/sympy/sympy/wiki/introduction-to-contributing)
page and the [sympy documentation style guide](https://docs.sympy.org/dev/documentation-style-guide.html). if you
are new and looking for some way to contribute, a good place to start is
to look at the issues tagged [easy to fix](https://github.com/sympy/sympy/issues?q=is%3aopen+is%3aissue+label%3a%22easy+to+fix%22).

please note that all participants in this project are expected to follow
our code of conduct. by participating in this project you agree to abide
by its terms. see [code\_of\_conduct.md](code_of_conduct.md).

## tests

to execute all tests, run:

    $./setup.py test

in the current directory.

for the more fine-grained running of tests or doctests, use `bin/test`
or respectively `bin/doctest`. the master branch is automatically tested
by github actions.

to test pull requests, use
[sympy-bot](https://github.com/sympy/sympy-bot).

## regenerate experimental <span class=""title-ref"">latex</span> parser/lexer

the parser and lexer were generated with the [antlr4](http://antlr4.org)
toolchain in `sympy/parsing/latex/_antlr` and checked into the repo.
presently, most users should not need to regenerate these files, but
if you plan to work on this feature, you will need the `antlr4`
command-line tool (and you must ensure that it is in your `path`).
one way to get it is:

    $ conda install -c conda-forge antlr=4.11.1

alternatively, follow the instructions on the antlr website and download
the `antlr-4.11.1-complete.jar`. then export the `classpath` as instructed
and instead of creating `antlr4` as an alias, make it an executable file
with the following contents:
``` bash
#!/bin/bash
java -jar /usr/local/lib/antlr-4.11.1-complete.jar ""$@""
```

after making changes to `sympy/parsing/latex/latex.g4`, run:

    $ ./setup.py antlr

## clean

to clean everything (thus getting the same tree as in the repository):

    $ git clean -xdf

which will clear everything ignored by `.gitignore`, and:

    $ git clean -df

to clear all untracked files. you can revert the most recent changes in
git with:

    $ git reset --hard

warning: the above commands will all clear changes you may have made,
and you will lose them forever. be sure to check things with `git
status`, `git diff`, `git clean -xn`, and `git clean -n` before doing any
of those.

## bugs

our issue tracker is at <https://github.com/sympy/sympy/issues>. please
report any bugs that you find. or, even better, fork the repository on
github and create a pull request. we welcome all changes, big or small,
and we will help you make the pull request if you are new to git (just
ask on our mailing list or gitter channel). if you further have any queries, you can find answers
on stack overflow using the [sympy](https://stackoverflow.com/questions/tagged/sympy) tag.

## brief history

sympy was started by ond≈ôej ƒçert√≠k in 2005, he wrote some code during
the summer, then he wrote some more code during summer 2006. in february
2007, fabian pedregosa joined the project and helped fix many things,
contributed documentation, and made it alive again. 5 students (mateusz
paprocki, brian jorgensen, jason gedge, robert schwarz, and chris wu)
improved sympy incredibly during summer 2007 as part of the google
summer of code. pearu peterson joined the development during the summer
2007 and he has made sympy much more competitive by rewriting the core
from scratch, which has made it from 10x to 100x faster. jurjen n.e. bos
has contributed pretty-printing and other patches. fredrik johansson has
written mpmath and contributed a lot of patches.

sympy has participated in every google summer of code since 2007. you
can see <https://github.com/sympy/sympy/wiki#google-summer-of-code> for
full details. each year has improved sympy by bounds. most of sympy's
development has come from google summer of code students.

in 2011, ond≈ôej ƒçert√≠k stepped down as lead developer, with aaron
meurer, who also started as a google summer of code student, taking his
place. ond≈ôej ƒçert√≠k is still active in the community but is too busy
with work and family to play a lead development role.

since then, a lot more people have joined the development and some
people have also left. you can see the full list in doc/src/aboutus.rst,
or online at:

<https://docs.sympy.org/dev/aboutus.html#sympy-development-team>

the git history goes back to 2007 when development moved from svn to hg.
to see the history before that point, look at
<https://github.com/sympy/sympy-old>.

you can use git to see the biggest developers. the command:

    $ git shortlog -ns

will show each developer, sorted by commits to the project. the command:

    $ git shortlog -ns --since=""1 year""

will show the top developers from the last year.

## citation

to cite sympy in publications use

> meurer a, smith cp, paprocki m, ƒçert√≠k o, kirpichev sb, rocklin m,
> kumar a, ivanov s, moore jk, singh s, rathnayake t, vig s, granger be,
> muller rp, bonazzi f, gupta h, vats s, johansson f, pedregosa f, curry
> mj, terrel ar, rouƒçka ≈°, saboo a, fernando i, kulal s, cimrman r,
> scopatz a. (2017) sympy: symbolic computing in python. *peerj computer
> science* 3:e103 <https://doi.org/10.7717/peerj-cs.103>

a bibtex entry for latex users is

``` bibtex
@article{10.7717/peerj-cs.103,
 title = {sympy: symbolic computing in python},
 author = {meurer, aaron and smith, christopher p. and paprocki, mateusz and \v{c}ert\'{i}k, ond\v{r}ej and kirpichev, sergey b. and rocklin, matthew and kumar, amit and ivanov, sergiu and moore, jason k. and singh, sartaj and rathnayake, thilina and vig, sean and granger, brian e. and muller, richard p. and bonazzi, francesco and gupta, harsh and vats, shivam and johansson, fredrik and pedregosa, fabian and curry, matthew j. and terrel, andy r. and rou\v{c}ka, \v{s}t\v{e}p\'{a}n and saboo, ashutosh and fernando, isuru and kulal, sumith and cimrman, robert and scopatz, anthony},
 year = 2017,
 month = jan,
 keywords = {python, computer algebra system, symbolics},
 abstract = {
            sympy is an open-source computer algebra system written in pure python. it is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. these characteristics have led sympy to become a popular symbolic library for the scientific python ecosystem. this paper presents the architecture of sympy, a description of its features, and a discussion of select submodules. the supplementary material provides additional examples and further outlines details of the architecture and features of sympy.
         },
 volume = 3,
 pages = {e103},
 journal = {peerj computer science},
 issn = {2376-5992},
 url = {https://doi.org/10.7717/peerj-cs.103},
 doi = {10.7717/peerj-cs.103}
}
```

sympy is bsd licensed, so you are free to use it whatever you like, be
it academic, commercial, creating forks or derivatives, as long as you
copy the bsd statement if you redistribute it (see the license file for
details). that said, although not required by the sympy license, if it
is convenient for you, please cite sympy when using it in your work and
also consider contributing all your changes back, so that we can
incorporate it and all of us will benefit in the end.
"
"statsmodels",".. image:: docs/source/images/statsmodels-logo-v2-horizontal.svg
  :alt: statsmodels logo

|pypi version| |conda version| |license| |azure ci build status|
|codecov coverage| |coveralls coverage| |pypi downloads| |conda downloads|

about statsmodels
=================

statsmodels is a python package that provides a complement to scipy for
statistical computations including descriptive statistics and estimation
and inference for statistical models.


documentation
=============

the documentation for the latest release is at

https://www.statsmodels.org/stable/

the documentation for the development version is at

https://www.statsmodels.org/dev/

recent improvements are highlighted in the release notes

https://www.statsmodels.org/stable/release/

backups of documentation are available at https://statsmodels.github.io/stable/
and https://statsmodels.github.io/dev/.


main features
=============

* linear regression models:

  - ordinary least squares
  - generalized least squares
  - weighted least squares
  - least squares with autoregressive errors
  - quantile regression
  - recursive least squares

* mixed linear model with mixed effects and variance components
* glm: generalized linear models with support for all of the one-parameter
  exponential family distributions
* bayesian mixed glm for binomial and poisson
* gee: generalized estimating equations for one-way clustered or longitudinal data
* discrete models:

  - logit and probit
  - multinomial logit (mnlogit)
  - poisson and generalized poisson regression
  - negative binomial regression
  - zero-inflated count models

* rlm: robust linear models with support for several m-estimators.
* time series analysis: models for time series analysis

  - complete statespace modeling framework

    - seasonal arima and arimax models
    - varma and varmax models
    - dynamic factor models
    - unobserved component models

  - markov switching models (msar), also known as hidden markov models (hmm)
  - univariate time series analysis: ar, arima
  - vector autoregressive models, var and structural var
  - vector error correction model, vecm
  - exponential smoothing, holt-winters
  - hypothesis tests for time series: unit root, cointegration and others
  - descriptive statistics and process models for time series analysis

* survival analysis:

  - proportional hazards regression (cox models)
  - survivor function estimation (kaplan-meier)
  - cumulative incidence function estimation

* multivariate:

  - principal component analysis with missing data
  - factor analysis with rotation
  - manova
  - canonical correlation

* nonparametric statistics: univariate and multivariate kernel density estimators
* datasets: datasets used for examples and in testing
* statistics: a wide range of statistical tests

  - diagnostics and specification tests
  - goodness-of-fit and normality tests
  - functions for multiple testing
  - various additional statistical tests

* imputation with mice, regression on order statistic and gaussian imputation
* mediation analysis
* graphics includes plot functions for visual analysis of data and model results

* i/o

  - tools for reading stata .dta files, but pandas has a more recent version
  - table output to ascii, latex, and html

* miscellaneous models
* sandbox: statsmodels contains a sandbox folder with code in various stages of
  development and testing which is not considered ""production ready"".  this covers
  among others

  - generalized method of moments (gmm) estimators
  - kernel regression
  - various extensions to scipy.stats.distributions
  - panel data models
  - information theoretic measures

how to get it
=============
the main branch on github is the most up to date code

https://www.github.com/statsmodels/statsmodels

source download of release tags are available on github

https://github.com/statsmodels/statsmodels/tags

binaries and source distributions are available from pypi

https://pypi.org/project/statsmodels/

binaries can be installed in anaconda

conda install statsmodels


installing from sources
=======================

see install.txt for requirements or see the documentation

https://statsmodels.github.io/dev/install.html

contributing
============
contributions in any form are welcome, including:

* documentation improvements
* additional tests
* new features to existing models
* new models

https://www.statsmodels.org/stable/dev/test_notes

for instructions on installing statsmodels in *editable* mode.

license
=======

modified bsd (3-clause)

discussion and development
==========================

discussions take place on the mailing list

https://groups.google.com/group/pystatsmodels

and in the issue tracker. we are very interested in feedback
about usability and suggestions for improvements.

bug reports
===========

bug reports can be submitted to the issue tracker at

https://github.com/statsmodels/statsmodels/issues

.. |azure ci build status| image:: https://dev.azure.com/statsmodels/statsmodels-testing/_apis/build/status/statsmodels.statsmodels?branchname=main
   :target: https://dev.azure.com/statsmodels/statsmodels-testing/_build/latest?definitionid=1&branchname=main
.. |codecov coverage| image:: https://codecov.io/gh/statsmodels/statsmodels/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/statsmodels/statsmodels
.. |coveralls coverage| image:: https://coveralls.io/repos/github/statsmodels/statsmodels/badge.svg?branch=main
   :target: https://coveralls.io/github/statsmodels/statsmodels?branch=main
.. |pypi downloads| image:: https://img.shields.io/pypi/dm/statsmodels?label=pypi%20downloads
   :alt: pypi - downloads
   :target: https://pypi.org/project/statsmodels/
.. |conda downloads| image:: https://img.shields.io/conda/dn/conda-forge/statsmodels.svg?label=conda%20downloads
   :target: https://anaconda.org/conda-forge/statsmodels/
.. |pypi version| image:: https://img.shields.io/pypi/v/statsmodels.svg
   :target: https://pypi.org/project/statsmodels/
.. |conda version| image:: https://anaconda.org/conda-forge/statsmodels/badges/version.svg
   :target: https://anaconda.org/conda-forge/statsmodels/
.. |license| image:: https://img.shields.io/pypi/l/statsmodels.svg
   :target: https://github.com/statsmodels/statsmodels/blob/main/license.txt
"
"astropy","=======
astropy
=======

.. container::

    |actions status| |circleci status| |coverage status| |pypi status| |documentation status| |pre-commit| |isort status| |black| |zenodo|

the astropy project (http://astropy.org/) is a community effort to develop a
single core package for astronomy in python and foster interoperability between
python astronomy packages. this repository contains the core package which is
intended to contain much of the core functionality and some common tools needed
for performing astronomy and astrophysics with python.

releases are `registered on pypi <https://pypi.org/project/astropy>`_,
and development is occurring at the
`project's github page <http://github.com/astropy/astropy>`_.

for installation instructions, see the `online documentation <https://docs.astropy.org/>`_
or  `docs/install.rst <docs/install.rst>`_ in this source distribution.

contributing code, documentation, or feedback
---------------------------------------------

the astropy project is made both by and for its users, so we welcome and
encourage contributions of many kinds. our goal is to keep this a positive,
inclusive, successful, and growing community by abiding with the
`astropy community code of conduct <http://www.astropy.org/about.html#codeofconduct>`_.

more detailed information on contributing to the project or submitting feedback
can be found on the `contributions <http://www.astropy.org/contribute.html>`_
page. a `summary of contribution guidelines <contributing.md>`_ can also be
used as a quick reference when you are ready to start writing or validating
code for submission.

supporting the project
----------------------

|numfocus| |donate|

the astropy project is sponsored by numfocus, a 501(c)(3) nonprofit in the
united states. you can donate to the project by using the link above, and this
donation will support our mission to promote sustainable, high-level code base
for the astronomy community, open code development, educational materials, and
reproducible scientific research.

license
-------

astropy is licensed under a 3-clause bsd style license - see the
`license.rst <license.rst>`_ file.

.. |actions status| image:: https://github.com/astropy/astropy/workflows/ci/badge.svg
    :target: https://github.com/astropy/astropy/actions
    :alt: astropy's github actions ci status

.. |circleci status| image::  https://img.shields.io/circleci/build/github/astropy/astropy/main?logo=circleci&label=circleci
    :target: https://circleci.com/gh/astropy/astropy
    :alt: astropy's circleci status

.. |coverage status| image:: https://codecov.io/gh/astropy/astropy/branch/main/graph/badge.svg
    :target: https://codecov.io/gh/astropy/astropy
    :alt: astropy's coverage status

.. |pypi status| image:: https://img.shields.io/pypi/v/astropy.svg
    :target: https://pypi.org/project/astropy
    :alt: astropy's pypi status

.. |zenodo| image:: https://zenodo.org/badge/doi/10.5281/zenodo.4670728.svg
   :target: https://doi.org/10.5281/zenodo.4670728
   :alt: zenodo doi

.. |documentation status| image:: https://img.shields.io/readthedocs/astropy/latest.svg?logo=read%20the%20docs&logocolor=white&label=docs&version=stable
    :target: https://docs.astropy.org/en/stable/?badge=stable
    :alt: documentation status

.. |pre-commit| image:: https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logocolor=white
   :target: https://github.com/pre-commit/pre-commit
   :alt: pre-commit

.. |isort status| image:: https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelcolor=ef8336
    :target: https://pycqa.github.io/isort/
    :alt: isort status

.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black

.. |numfocus| image:: https://img.shields.io/badge/powered%20by-numfocus-orange.svg?style=flat&colora=e1523d&colorb=007d8a
    :target: http://numfocus.org
    :alt: powered by numfocus

.. |donate| image:: https://img.shields.io/badge/donate-to%20astropy-brightgreen.svg
    :target: https://numfocus.salsalabs.org/donate-to-astropy/index.html


if you locally cloned this repo before 7 apr 2021
-------------------------------------------------

the primary branch for this repo has been transitioned from ``master`` to
``main``.  if you have a local clone of this repository and want to keep your
local branch in sync with this repo, you'll need to do the following in your
local clone from your terminal::

   git fetch --all --prune
   # you can stop here if you don't use your local ""master""/""main"" branch
   git branch -m master main
   git branch -u origin/main main

if you are using a gui to manage your repos you'll have to find the equivalent
commands as it's different for different programs. alternatively, you can just
delete your local clone and re-clone!
"
"matplotlib","[![pypi](https://badge.fury.io/py/matplotlib.svg)](https://badge.fury.io/py/matplotlib)
[![downloads](https://pepy.tech/badge/matplotlib/month)](https://pepy.tech/project/matplotlib)
[![numfocus](https://img.shields.io/badge/powered%20by-numfocus-orange.svg?style=flat&colora=e1523d&colorb=007d8a)](https://numfocus.org)

[![discoursebadge](https://img.shields.io/badge/help_forum-discourse-blue.svg)](https://discourse.matplotlib.org)
[![gitter](https://badges.gitter.im/matplotlib/matplotlib.svg)](https://gitter.im/matplotlib/matplotlib)
[![githubissues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/matplotlib/matplotlib/issues)
[![gittutorial](https://img.shields.io/badge/pr-welcome-%23ff8300.svg?)](https://git-scm.com/book/en/v2/github-contributing-to-a-project)

[![githubactions](https://github.com/matplotlib/matplotlib/workflows/tests/badge.svg)](https://github.com/matplotlib/matplotlib/actions?query=workflow%3atests)
[![azurepipelines](https://dev.azure.com/matplotlib/matplotlib/_apis/build/status/matplotlib.matplotlib?branchname=main)](https://dev.azure.com/matplotlib/matplotlib/_build/latest?definitionid=1&branchname=main)
[![appveyor](https://ci.appveyor.com/api/projects/status/github/matplotlib/matplotlib?branch=main&svg=true)](https://ci.appveyor.com/project/matplotlib/matplotlib)
[![codecov](https://codecov.io/github/matplotlib/matplotlib/badge.svg?branch=main&service=github)](https://codecov.io/github/matplotlib/matplotlib?branch=main)

![image](https://matplotlib.org/_static/logo2.svg)

matplotlib is a comprehensive library for creating static, animated, and
interactive visualizations in python.

check out our [home page](https://matplotlib.org/) for more information.

![image](https://matplotlib.org/_static/readme_preview.png)

matplotlib produces publication-quality figures in a variety of hardcopy
formats and interactive environments across platforms. matplotlib can be
used in python scripts, python/ipython shells, web application servers,
and various graphical user interface toolkits.

## install

see the [install
documentation](https://matplotlib.org/stable/users/installing/index.html),
which is generated from `/doc/users/installing/index.rst`

## contribute

you've discovered a bug or something else you want to change -
excellent!

you've worked out a way to fix it -- even better!

you want to tell us about it -- best of all!

start at the [contributing
guide](https://matplotlib.org/devdocs/devel/contributing.html)!

## contact

[discourse](https://discourse.matplotlib.org/) is the discussion forum
for general questions and discussions and our recommended starting
point.

our active mailing lists (which are mirrored on discourse) are:

-   [users](https://mail.python.org/mailman/listinfo/matplotlib-users)
    mailing list: <matplotlib-users@python.org>
-   [announcement](https://mail.python.org/mailman/listinfo/matplotlib-announce)
    mailing list: <matplotlib-announce@python.org>
-   [development](https://mail.python.org/mailman/listinfo/matplotlib-devel)
    mailing list: <matplotlib-devel@python.org>

[gitter](https://gitter.im/matplotlib/matplotlib) is for coordinating
development and asking questions directly related to contributing to
matplotlib.

## citing matplotlib

if matplotlib contributes to a project that leads to publication, please
acknowledge this by citing matplotlib.

[a ready-made citation
entry](https://matplotlib.org/stable/users/project/citing.html) is
available.
"
"bokeh","<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/bokeh/pm/main/assets/logos/svg/bokeh-logo-white-text-no-padding.svg"">
  <img src=""https://raw.githubusercontent.com/bokeh/pm/main/assets/logos/svg/bokeh-logo-black-text-no-padding.svg"" alt=""bokeh logo -- text is white in dark theme and black in light theme"" height=60/>
</picture>

----

[bokeh](https://bokeh.org) is an¬†interactive visualization¬†library for modern web browsers. it provides elegant, concise¬†construction¬†of versatile graphics and affords high-performance interactivity across large or streaming datasets.¬†bokeh can help anyone who wants to create interactive plots, dashboards, and data applications quickly and easily.

<table>

<tr>

  <td>package</td>

  <td>
    <img src=""https://img.shields.io/pypi/v/bokeh?label=version&color=ecd078&style=for-the-badge""
         alt=""latest package version"" />
  </td>

  <td>
    <a href=""https://docs.bokeh.org/en/latest/docs/first_steps/installation.html"">
    <img src=""https://img.shields.io/pypi/pyversions/bokeh?color=ecd078&style=for-the-badge""
         alt=""supported python versions"" />
    </a>
  </td>

  <td>
    <a href=""https://github.com/bokeh/bokeh/blob/main/license.txt"">
    <img src=""https://img.shields.io/github/license/bokeh/bokeh.svg?color=ecd078&style=for-the-badge""
         alt=""bokeh license (bsd 3-clause)"" />
    </a>
  </td>

</tr>

<tr>

  <td>project</td>

  <td>
    <img src=""https://img.shields.io/github/contributors-anon/bokeh/bokeh?color=ecd078&style=for-the-badge""
         alt=""github contributors"" />
  </td>

  <td>
    <a href=""https://numfocus.org"">
    <img src=""https://img.shields.io/badge/sponsor-numfocus-ecd078?style=for-the-badge""
         alt=""link to numfocus"" />
    </a>
  </td>

  <td>
    <a href=""https://docs.bokeh.org/en/latest/"">
    <img src=""https://img.shields.io/badge/documentation-latest-ecd078?style=for-the-badge""
         alt=""link to documentation"" />
    </a>
  </td>

</tr>

<tr>

  <td>downloads</td>

  <td>
    <a href=""https://docs.bokeh.org/en/latest/docs/first_steps/installation.html"">
    <img src=""https://img.shields.io/pypi/dm/bokeh?color=d98b43&label=pypi&logo=python&logocolor=yellow&style=for-the-badge""
         alt=""pypi downloads per month"" />
    </a>
  </td>

  <td>
    <a href=""https://docs.bokeh.org/en/latest/docs/first_steps/installation.html"">
    <img src=""https://raw.githubusercontent.com/bokeh/badges/main/cache/bokeh-conda-monthly.svg""
         alt=""conda downloads per month"" />
    </a>
  </td>

</tr>

<tr>

  <td>build</td>

  <td>
    <a href=""https://github.com/bokeh/bokeh/actions"">
    <img src=""https://img.shields.io/github/actions/workflow/status/bokeh/bokeh/bokeh-ci.yml?label=bokeh-ci&logo=github&style=for-the-badge""
         alt=""current bokeh-ci github actions build status"" />
    </a>
  </td>

  <td>
    <a href=""https://github.com/bokeh/bokeh/actions"">
    <img src=""https://img.shields.io/github/actions/workflow/status/bokeh/bokeh/bokehjs-ci.yml?label=bokehjs-ci&logo=github&style=for-the-badge""
         alt=""current bokehjs-ci github actions build status"" />
    </a>
  </td>

  <td>
    <a href=""https://codecov.io/gh/bokeh/bokeh"" >
    <img src=""https://img.shields.io/codecov/c/github/bokeh/bokeh?logo=codecov&style=for-the-badge&token=bhezgkduaw""
         alt=""codecov coverage percentage"" />
    </a>
  </td>

</tr>

<tr>

  <td>community</td>

  <td>
    <a href=""https://discourse.bokeh.org"">
    <img src=""https://img.shields.io/discourse/https/discourse.bokeh.org/posts.svg?color=blue&logo=discourse&style=for-the-badge""
         alt=""community support on discourse.bokeh.org"" />
    </a>
  </td>

  <td>
    <a href=""https://stackoverflow.com/questions/tagged/bokeh"">
    <img src=""https://raw.githubusercontent.com/bokeh/badges/main/cache/bokeh-stackoverflow-total.svg""
         alt=""bokeh-tagged questions on stack overflow"" />
     </a>
  </td>

  <td>
    <a href=""https://twitter.com/bokeh"">
    <img src=""https://img.shields.io/badge/follow-%40bokeh-blue?logo=twitter&style=for-the-badge""
         alt=""follow bokeh on twitter"" />
    </a>
  </td>

</tr>


</table>

*consider [making a donation](https://opencollective.com/bokeh) if you enjoy using bokeh and want to support its development.*

![4x9 image grid of bokeh plots](https://user-images.githubusercontent.com/1078448/190840954-dc243c99-9295-44de-88e9-fafd0f4f7f8a.jpg)

## installation

to install bokeh and its required dependencies using `pip`, enter the following command at a bash or windows command prompt:
```
pip install bokeh
```

to install `conda`, enter the following command at a bash or windows command prompt:

```
conda install bokeh
```

refer to the [installation documentation](https://docs.bokeh.org/en/latest/docs/first_steps/installation.html) for more details.

## resources

once bokeh is installed, check out the [first steps guides](https://docs.bokeh.org/en/latest/docs/first_steps.html#first-steps-guides).

visit the [full documentation site](https://docs.bokeh.org) to view the [user's guide](https://docs.bokeh.org/en/latest/docs/user_guide.html) or [launch the bokeh tutorial](https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/head?labpath=index.ipynb) to learn about bokeh in live jupyter notebooks.

community support is available on the [project discourse](https://discourse.bokeh.org).

if you would like to contribute to bokeh, please review the [contributor guide](https://docs.bokeh.org/en/latest/docs/dev_guide.html) and [request an invitation to the bokeh dev slack workspace](https://slack-invite.bokeh.org/).

*note: everyone who engages in the bokeh project's discussion forums, codebases, and issue trackers is expected to follow the [code of conduct](https://github.com/bokeh/bokeh/blob/branch-3.0/docs/code_of_conduct.md).*

## follow us

follow us on twitter [@bokeh](https://twitter.com/bokeh)

## support

### fiscal support

the bokeh project is grateful for [individual contributions](https://opencollective.com/bokeh), as well as for monetary support from the organizations and companies listed below:

<table align=""center"">
<tr>

  <td>
    <a href=""https://www.numfocus.org/"">
    <img src=""https://static.bokeh.org/sponsor/numfocus.svg""
         alt=""numfocus logo"" width=""200""/>
    </a>
  </td>

  <td>
    <a href=""https://chanzuckerberg.com/"">
    <img src=""https://static.bokeh.org/sponsor/czi.svg""
         alt=""czi logo"" width=""200""/>
    </a>
  </td>

  <td colspan=""2"">
    <a href=""https://www.blackstone.com/the-firm/"">
    <img src=""https://static.bokeh.org/sponsor/blackstone.png""
         alt=""blackstone logo"" width=""400""/>
    </a>
  </td>

 </tr>
 <tr>

  <td>
    <a href=""https://tidelift.com/"">
    <img src=""https://static.bokeh.org/sponsor/tidelift.svg""
         alt=""tidelift logo"" width=""200""/>
    </a>
  </td>

  <td>
    <a href=""https://www.anaconda.com/"">
    <img src=""https://static.bokeh.org/sponsor/anaconda.png""
         alt=""anaconda logo"" width=""200""/>
    </a>
  </td>

  <td>
    <a href=""https://www.nvidia.com"">
    <img src=""https://static.bokeh.org/sponsor/nvidia.png""
         alt=""nvidia logo"" width=""200""/>
    </a>
  </td>

  <td>
    <a href=""https://developer.nvidia.com/rapids"">
    <img src=""https://static.bokeh.org/sponsor/rapids.png""
         alt=""rapids logo"" width=""200""/>
    </a>
  </td>

</tr>
</table>

if your company uses bokeh and is able to sponsor the project, please contact <a href=""info@bokeh.org"">info@bokeh.org</a>

*bokeh is a sponsored project of numfocus, a 501(c)(3) nonprofit charity in the united states. numfocus provides bokeh with fiscal, legal, and administrative support to help ensure the health and sustainability of the project. visit [numfocus.org](https://numfocus.org) for more information.*

*donations to bokeh are managed by numfocus. for donors in the united states, your gift is tax-deductible to the extent provided by law. as with any donation, you should consult with your tax adviser about your particular tax situation.*

### in-kind support

non-monetary support can help with development, collaboration, infrastructure, security, and vulnerability management. the bokeh project is grateful to the following companies for their donation of services:

* [amazon web services](https://aws.amazon.com/)
* [gitguardian](https://gitguardian.com/)
* [github](https://github.com/)
* [makepath](https://makepath.com/)
* [pentest tools](https://pentest-tools.com)
* [pingdom](https://www.pingdom.com/website-monitoring)
* [slack](https://slack.com)
* [questionscout](https://www.questionscout.com/)
* [1password](https://1password.com/)
"
"pygal","# pygal


[![build status](https://travis-ci.org/kozea/pygal.svg?branch=master)](https://travis-ci.org/kozea/pygal)
[![coverage status](https://coveralls.io/repos/kozea/pygal/badge.svg?branch=master&service=github)](https://coveralls.io/github/kozea/pygal?branch=master)
[![documentation status](https://readthedocs.org/projects/pygal/badge/?version=latest)](https://readthedocs.org/projects/pygal/?badge=latest)


- [pygal](#pygal)
    - [description](#description)
    - [installation](#installation)
    - [test](#test)
    - [contribute](#contribute)
    - [license](#license)

## description

**pygal** is a dynamic svg charting library written in python.
all the documentation is on [www.pygal.org](http://www.pygal.org)


## installation

as simple as:

```
    $ pip install pygal
```



## test

pygal is tested with py.test:


```
    $ pip install pytest
    $ py.test
```


## contribute

you are welcomed to fork the project and make pull requests.
be sure to create a branch for each feature, write tests if needed and run the current tests !


you can also support the project:

[![flattr](http://api.flattr.com/button/flattr-badge-large.png)](https://flattr.com/submit/auto?user_id=paradoxxx_zero&url=https://github.com/kozea/pygal&title=pygal&tags=github&category=software)
[![gittip](http://i.imgur.com/ikcqb2p.png)](https://www.gittip.com/paradoxxxzero/)



## license

copyright ¬© 2012-2016 kozea
lgplv3:

    this program is free software: you can redistribute it and/or modify
    it under the terms of the gnu general public license as published by
    the free software foundation, either version 3 of the license, or
    (at your option) any later version.

    this program is distributed in the hope that it will be useful,
    but without any warranty; without even the implied warranty of
    merchantability or fitness for a particular purpose.  see the
    gnu general public license for more details.

    you should have received a copy of the gnu general public license
    along with this program.  if not, see <http://www.gnu.org/licenses/>.
"
"Petrel","petrel
======

tools for writing, submitting, debugging, and monitoring storm topologies in pure python.

note: the base storm package provides storm.py, which supports python 2.6.
petrel, however, requires python 2.7 or 3.5.

if you like petrel and are interested in more extensive documentation and examples, see the
[book from packt](https://www.packtpub.com/big-data-and-business-intelligence/building-python-real-time-applications-storm).
the book is also available from
[amazon](https://www.amazon.com/building-python-real-time-applications-storm/dp/1784392855/ref=sr_1_1?).

i support petrel in my spare time, and your purchases motivate me to continue maintaining it.

overview
========

petrel offers some important improvements over the storm.py module provided with storm:

* topologies are implemented in 100% python
* petrel's packaging support automatically sets up a python virtual environment for your topology and makes it easy to install additional python packages.
* ""petrel.mock"" allows testing of single components or single chains of related components.
* petrel automatically sets up logging for every spout or bolt and logs a stack trace on unhandled errors.

here's a quick example. it implements word count, the classic big data demo application.

this code defines the topology. without petrel, you'd have to write this code in clojure or java. petrel re-implements the java ""topologybuilder"" api in python. if you've seen that class, this code will look very familiar:

<pre>
import randomsentence
import splitsentence
import wordcount

def create(builder):
    builder.setspout(""spout"", randomsentence.randomsentencespout(), 1)
    builder.setbolt(""split"", splitsentence.splitsentencebolt(), 1).shufflegrouping(""spout"")
    builder.setbolt(""count"", wordcount.wordcountbolt(), 1).fieldsgrouping(""split"", [""word""])
</pre>

this word count example is included in the petrel repository. here's how to run it. from the top-level directory of the petrel repository, run:

    cd samples/wordcount
    ./buildandrun --config topology.yaml

this will build a topology jar file and submit it to storm, running the topology in local mode. no ant, maven, leinengen, or clojure required.

    ./buildandrun --config topology.yaml

simply add the topology name to the command line to run on a real cluster instead:

    ./buildandrun --config topology.yaml wordcount
 
note: i'm working to improve the petrel documentation and tooling to make it easier for beginners to become productive with petrel quickly. if you have requests or suggestions, please log an issue in github.

installation
============

* python 2.7
* system packages
  * libyaml
  * thrift
* python packages (you install)
    * virtualenv
* python packages (installed automatically by setup.py)
    * simplejson 2.6.1
    * thrift 0.8.0
    * pyyaml 3.10

installing petrel as an egg
---------------------------

before installing petrel, make sure storm is installed and in your path. run the following command:

    storm version
    
this will print the version of storm active on your system, a number such as ""1.0.2"". you must use a version of petrel whose first 3 digits match this version.

install the egg:

easy_install petrel*.egg

this will download a few dependencies and then print a message like:

    finished processing dependencies for petrel==1.0.2.0.3

installing petrel from source
-----------------------------

if you plan to use use petrel by cloning its source code repository from github.com, follow these instructions.

ensure the following tools are installed:

* storm
    * test with ""storm version""
    * should print something like ""1.0.2""
* thrift compiler
    * test with ""thrift -version""
    * should print something like ""thrift version 0.9.3""
* maven (test with ""mvn -version"")

clone petrel from github. then run:

    cd petrel/petrel
    python setup.py develop

this will download a few dependencies and then print a message like:

    finished processing dependencies for petrel==1.0.2.0.3

topology configuration
======================

petrel's ""--config"" parameter accepts a yaml file with standard storm configuration options. the file can also include some petrel-specific settings. see below.

```
topology.message.timeout.secs: 150
topology.ackers: 1
topology.workers: 5
topology.max.spout.pending: 1
worker.childopts: ""-xmx4096m""
topology.worker.childopts: ""-xmx4096m""

# controls how petrel installs its own dependencies, e.g. simplejson, thrift, pyyaml.
petrel.pip_options: ""--no-index -f http://10.255.3.20/pip/""

# if you prefer, you can configure parallelism here instead of in setspout() or
# setbolt().
petrel.parallelism.splitsentence: 1
```

building and submitting topologies
==================================

use the following command to package and submit a topology to storm:

<pre>
petrel submit --sourcejar ../../jvmpetrel/target/storm-petrel-*-snapshot.jar --config localhost.yaml
</pre>

the above command builds and submits a topology in local mode. it will run until you stop it with control-c. this mode is useful for simple development and testing.

if you want to run the topology on a storm cluster, run the following command instead:

<pre>
petrel submit --sourcejar ../../jvmpetrel/target/storm-petrel-*-snapshot.jar --config localhost.yaml wordcount
</pre>

you can find instructions on setting up a storm cluster here:

https://github.com/nathanmarz/storm/wiki/setting-up-a-storm-cluster

build
-----

* get the topology definition by loading the create.py script and calling create().
* package a jar containing the topology definition, code, and configuration.
* files listed in manifest.txt, e.g. additional configuration files

deploy and run
--------------

to deploy and run a petrel topology on a storm cluster, each storm worker must have the following installed:

* python 2.7
* setuptools
* virtualenv

note that the worker machines don't require petrel itself to be installed. only the *submitting* machine needs to have petrel. each time you submit a topology using petrel, it creates a custom jar file with the petrel egg and and your python spout and bolt code. these files in the wordcount example show how this works:

* buildandrun
* manifest.txt

because petrel topologies are self contained, it is easy to run multiple versions of a topology on the same cluster, as long as the code differences are contained within virtualenv. before a spout or bolt starts up, petrel creates a new python virtualenv and runs the optional topology-specific setup.sh script to install python packages. this virtual environment is shared by all the spouts or bolts from that instance of the topology on that machine.

monitoring
==========

petrel provides a ""status"" command which lists the active topologies and tasks on a cluster. you can optionally filter by task name and storm port (i.e. worker slot) number.

<pre>
petrel status 10.255.1.58
</pre>

logging
=======

petrel does not write to the standard storm logs. instead it creates its own set of logs underneath the topology directory. for example, if you are running a topology in local mode, you'll find the petrel log in a subdirectory of the ""storm.local.dir"" directory (whose location you can find in the storm log). for example:

./supervisor/stormdist/test+topology-1-1365766701/resources/petrel28289_randomsentence.log
./supervisor/stormdist/test+topology-1-1365766701/resources/petrel28281_virtualenv.log
./supervisor/stormdist/test+topology-1-1365766701/resources/petrel28281_wordcount.log
./supervisor/stormdist/test+topology-1-1365766701/resources/petrel28285_splitsentence.log

petrel uses stdout to send json data to storm. any other code that writes to stdout (e.g. ""print"" statements) would cause the storm worker to crash. in order to avoid this, petrel automatically reassigns sys.stdout and sys.stderr so they write to the petrel (i.e. python) logger instead.

when storm is running on a cluster, it can be useful to send certain messages (e.g. errors) to a central machine. to help support this, petrel sets an environment variable ""nimbus_host"". for example, the following log file configuration declares a log handler which sends any worker log messages info or higher to the nimbus host.

<pre>
[handler_hand02]
class=handlers.sysloghandler
level=info
formatter=form02
args=((os.getenv('nimbus_host') or 'localhost',handlers.syslog_udp_port),handlers.sysloghandler.log_user)
</pre>

petrel also has a ""stormhandler"" class sends messages to the storm logger. this feature has not been thoroughly tested, but can be enabled by uncommenting the following line in petrel/util.py:

<pre>
# logging.stormhandler = stormhandler
</pre>

storm logging
=============

when running petrel applications in storm's local mode, the console output is a mixture of petrel and storm logging output. this results in a lot of messages and can be hard to follow. you can control the storm logging output by using petrel's ""--extrastormcp"" option. any directories specified to this option will be prepended to storm's java class path.

for example, create a file log4j.properties in the samples/wordcount directory, with the following contents:

```
# set root logger level to debug and its only appender to a1.
log4j.rootlogger=debug, a1
#
## a1 is set to be a consoleappender
log4j.appender.a1=org.apache.log4j.consoleappender
#
## a1 uses patternlayout.
log4j.appender.a1.layout=org.apache.log4j.patternlayout
log4j.appender.a1.layout.conversionpattern=[%d{dd mmm yyyy hh:mm:ss}] [%t] %-5p %c %x - %m%n
log4j.logger.org.apache=error
log4j.logger.backtype=error
log4j.logger.com.netflix=error
```

now run ""petrel submit"" like this:

```petrel submit --extrastormcp=`pwd` --config=topology.yaml ```

with this setting, the apache, backtype, and netflix logs will be configured at error level, suppressing most of the logger messages from storm.

testing
=======

petrel provides a ""mock"" module which mocks some of storm's features. this makes it possible to test individual components and simple topologies in pure python, without relying on the storm runtime.

<pre>
def test():
    bolt = wordcountbolt()
    
    from petrel import mock
    from randomsentence import randomsentencespout
    mock_spout = mock.mockspout(randomsentencespout.declareoutputfields(), [
        ['word'],
        ['other'],
        ['word'],
    ])
    
    result = mock.run_simple_topology([mock_spout, bolt], result_type=mock.list)
    assert_equal(2, bolt._count['word'])
    assert_equal(1, bolt._count['other'])
    assert_equal([['word', 1], ['other', 1], ['word', 2]], result[bolt])
</pre>

in petrel terms, a ""simple"" topology is one which only outputs to the default stream and has no branches or loops. run_simple_topology() assumes the first component in the list is a spout, and it passes the output of each component to the next component in the list.

license
=======

the use and distribution terms for this software are covered by the bsd 3-clause license 1.0 (http://opensource.org/licenses/bsd-3-clause) which can be found in the file license.txt at the root of this distribution. by using this software in any fashion, you are agreeing to be bound by the terms of this license. you must not remove this notice, or any other, from this software.

setup.sh
--------

a topology may optionally include a setup.sh script. if present, petrel will execute it before launching the spout or bolt. typically this script is used for installing additional python libraries. here's an example setup.sh script:

<pre>
set -e

# $1 will be non-zero if creating a new virtualenv, zero if reusing an existing one.
if [ $1 -ne 0 ]; then
    for f in shapely==1.2.15 pyproj==1.9.0 pycassa==1.7.0 \
             configobj==4.7.2 greenlet==0.4.0 gevent==1.0b3
    do
        echo ""installing $f""
        pip install $f
    done
fi
</pre>

"
"Blaze",".. image:: https://raw.github.com/blaze/blaze/master/docs/source/svg/blaze_med.png
   :align: center

|build status| |coverage status| |join the chat at
https://gitter.im/blaze/blaze|

**blaze** translates a subset of modified numpy and pandas-like syntax
to databases and other computing systems. blaze allows python users a
familiar interface to query data living in other data storage systems.

example
=======

we point blaze to a simple dataset in a foreign database (postgresql).
instantly we see results as we would see them in a pandas dataframe.

.. code:: python

    >>> import blaze as bz
    >>> iris = bz.data('postgresql://localhost::iris')
    >>> iris
        sepal_length  sepal_width  petal_length  petal_width      species
    0            5.1          3.5           1.4          0.2  iris-setosa
    1            4.9          3.0           1.4          0.2  iris-setosa
    2            4.7          3.2           1.3          0.2  iris-setosa
    3            4.6          3.1           1.5          0.2  iris-setosa

these results occur immediately. blaze does not pull data out of
postgres, instead it translates your python commands into sql (or
others.)

.. code:: python

    >>> iris.species.distinct()
               species
    0      iris-setosa
    1  iris-versicolor
    2   iris-virginica

    >>> bz.by(iris.species, smallest=iris.petal_length.min(),
    ...                      largest=iris.petal_length.max())
               species  largest  smallest
    0      iris-setosa      1.9       1.0
    1  iris-versicolor      5.1       3.0
    2   iris-virginica      6.9       4.5

this same example would have worked with a wide range of databases,
on-disk text or binary files, or remote data.

what blaze is not
=================

blaze does not perform computation. it relies on other systems like sql,
spark, or pandas to do the actual number crunching. it is not a
replacement for any of these systems.

blaze does not implement the entire numpy/pandas api, nor does it
interact with libraries intended to work with numpy/pandas. this is the
cost of using more and larger data systems.

blaze is a good way to inspect data living in a large database, perform
a small but powerful set of operations to query that data, and then
transform your results into a format suitable for your favorite python
tools.

in the abstract
===============

blaze separates the computations that we want to perform:

.. code:: python

    >>> accounts = symbol('accounts', 'var * {id: int, name: string, amount: int}')

    >>> deadbeats = accounts[accounts.amount < 0].name

from the representation of data

.. code:: python

    >>> l = [[1, 'alice',   100],
    ...      [2, 'bob',    -200],
    ...      [3, 'charlie', 300],
    ...      [4, 'denis',   400],
    ...      [5, 'edith',  -500]]

blaze enables users to solve data-oriented problems

.. code:: python

    >>> list(compute(deadbeats, l))
    ['bob', 'edith']

but the separation of expression from data allows us to switch between
different backends.

here we solve the same problem using pandas instead of pure python.

.. code:: python

    >>> df = dataframe(l, columns=['id', 'name', 'amount'])

    >>> compute(deadbeats, df)
    1      bob
    4    edith
    name: name, dtype: object

blaze doesn't compute these results, blaze intelligently drives other
projects to compute them instead. these projects range from simple pure
python iterators to powerful distributed spark clusters. blaze is built
to be extended to new systems as they evolve.

getting started
===============

blaze is available on conda or on pypi

::

    conda install blaze
    pip install blaze

development builds are accessible

::

    conda install blaze -c blaze
    pip install http://github.com/blaze/blaze --upgrade

you may want to view `the docs <http://blaze.pydata.org>`__, `the
tutorial <http://github.com/blaze/blaze-tutorial>`__, `some
blogposts <http://continuum.io/blog/tags/blaze>`__, or the `mailing list
archives <https://groups.google.com/a/continuum.io/forum/#!forum/blaze-dev>`__.


development setup
=================

the quickest way to install all blaze dependencies with ``conda`` is as
follows

::

    conda install blaze spark -c blaze -c anaconda-cluster -y
    conda remove odo blaze blaze-core datashape -y

after running these commands, clone ``odo``, ``blaze``, and ``datashape`` from
github directly.  these three projects release together.  run ``python setup.py
develop`` to make development installations of each.


license
=======

released under bsd license. see `license.txt <license.txt>`__ for
details.

blaze development is sponsored by continuum analytics.

.. |build status| image:: https://travis-ci.org/blaze/blaze.png
   :target: https://travis-ci.org/blaze/blaze
.. |coverage status| image:: https://coveralls.io/repos/blaze/blaze/badge.png
   :target: https://coveralls.io/r/blaze/blaze
.. |join the chat at https://gitter.im/blaze/blaze| image:: https://badges.gitter.im/join%20chat.svg
   :target: https://gitter.im/blaze/blaze?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
"
"emcee","emcee
=====

**the python ensemble sampling toolkit for affine-invariant mcmc**

.. image:: https://img.shields.io/badge/github-dfm%2femcee-blue.svg?style=flat
    :target: https://github.com/dfm/emcee
.. image:: https://github.com/dfm/emcee/workflows/tests/badge.svg
    :target: https://github.com/dfm/emcee/actions?query=workflow%3atests
.. image:: http://img.shields.io/badge/license-mit-blue.svg?style=flat
    :target: https://github.com/dfm/emcee/blob/main/license
.. image:: http://img.shields.io/badge/arxiv-1202.3665-orange.svg?style=flat
    :target: https://arxiv.org/abs/1202.3665
.. image:: https://coveralls.io/repos/github/dfm/emcee/badge.svg?branch=main&style=flat&v=2
    :target: https://coveralls.io/github/dfm/emcee?branch=main
.. image:: https://readthedocs.org/projects/emcee/badge/?version=latest
    :target: http://emcee.readthedocs.io/en/latest/?badge=latest


emcee is a stable, well tested python implementation of the affine-invariant
ensemble sampler for markov chain monte carlo (mcmc)
proposed by
`goodman & weare (2010) <http://cims.nyu.edu/~weare/papers/d13.pdf>`_.
the code is open source and has
already been used in several published projects in the astrophysics
literature.

documentation
-------------

read the docs at `emcee.readthedocs.io <http://emcee.readthedocs.io/>`_.

attribution
-----------

please cite `foreman-mackey, hogg, lang & goodman (2012)
<https://arxiv.org/abs/1202.3665>`_ if you find this code useful in your
research. the bibtex entry for the paper is::

    @article{emcee,
       author = {{foreman-mackey}, d. and {hogg}, d.~w. and {lang}, d. and {goodman}, j.},
        title = {emcee: the mcmc hammer},
      journal = {pasp},
         year = 2013,
       volume = 125,
        pages = {306-312},
       eprint = {1202.3665},
          doi = {10.1086/670067}
    }

license
-------

copyright 2010-2021 dan foreman-mackey and contributors.

emcee is free software made available under the mit license. for details see
the license file.
"
"vispy","vispy: interactive scientific visualization in python
-----------------------------------------------------

main website: http://vispy.org

|build status| |coverage status| |zenodo link| |contributor covenant|

----

vispy is a **high-performance interactive 2d/3d data visualization
library**. vispy leverages the computational power of modern **graphics
processing units (gpus)** through the **opengl** library to display very
large datasets. applications of vispy include:

-  high-quality interactive scientific plots with millions of points.
-  direct visualization of real-time data.
-  fast interactive visualization of 3d models (meshes, volume
   rendering).
-  opengl visualization demos.
-  scientific guis with fast, scalable visualization widgets (`qt <http://www.qt.io>`__ or
   `ipython notebook <http://ipython.org/notebook.html>`__ with webgl).

releases
--------

see `changelog.md <./changelog.md>`_.

announcements
-------------

see the `vispy website <https://vispy.org/news.html>`_.

using vispy
-----------

vispy is a young library under heavy development at this time. it
targets two categories of users:

1. **users knowing opengl**, or willing to learn opengl, who want to
   create beautiful and fast interactive 2d/3d visualizations in python
   as easily as possible.
2. **scientists without any knowledge of opengl**, who are seeking a
   high-level, high-performance plotting toolkit.

if you're in the first category, you can already start using vispy.
vispy offers a pythonic, numpy-aware, user-friendly interface for opengl
es 2.0 called **gloo**. you can focus on writing your glsl code instead
of dealing with the complicated opengl api - vispy takes care of that
automatically for you.

if you're in the second category, we're starting to build experimental
high-level plotting interfaces. notably, vispy now ships a very basic
and experimental opengl backend for matplotlib.


installation
------------

please follow the detailed
`installation instructions <http://vispy.org/installation.html>`_
on the vispy website.

structure of vispy
------------------

currently, the main subpackages are:

-  **app**: integrates an event system and offers a unified interface on
   top of many window backends (qt4, wx, glfw, jupyter notebook,
   and others). relatively stable api.
-  **gloo**: a pythonic, object-oriented interface to opengl. relatively
   stable api.
-  **scene**: this is the system underlying our upcoming high level
   visualization interfaces. under heavy development and still
   experimental, it contains several modules.

   -  **visuals** are graphical abstractions representing 2d shapes, 3d
      meshes, text, etc.
   -  **transforms** implement 2d/3d transformations implemented on both
      cpu and gpu.
   -  **shaders** implements a shader composition system for plumbing
      together snippets of glsl code.
   -  the **scene graph** tracks all objects within a transformation
      graph.
-  **plot**: high-level plotting interfaces.

the api of all public interfaces are subject to change in the future,
although **app** and **gloo** are *relatively* stable at this point.

code of conduct
---------------

the vispy community requires its members to abide by the
`code of conduct <./code_of_conduct.md>`_. in this coc you will find the
expectations of members, the penalties for violating these expectations, and
how violations can be reported to the members of the community in charge of
enforcing this code of conduct.

governance
----------

the vispy project maintainers make decisions about the project based on a
simple consensus model. this is described in more detail on the
`governance page <https://vispy.org/governance/governance.html>`_ of the vispy
website as well as the
`list of maintainers <https://vispy.org/governance/maintainers.html>`_.

in addition to decisions about the vispy project, there is also a steering
committee for the overall vispy organization. more information about this
committee can also be found on the `steering committee page <https://vispy.org/org/steering-committee.html>`_
of the vispy website,
along with the organization's `charter <https://vispy.org/org/charter.html>`_ and
other related documents (linked in the charter).

genesis
-------

vispy began when four developers with their own visualization libraries
decided to team up:
`luke campagnola <http://luke.campagnola.me/>`__ with `pyqtgraph <http://www.pyqtgraph.org/>`__,
`almar klein <http://www.almarklein.org/>`__ with `visvis <https://github.com/almarklein/visvis>`__,
`cyrille rossant <http://cyrille.rossant.net>`__ with `galry <https://github.com/rossant/galry>`__,
`nicolas rougier <http://www.loria.fr/~rougier/index.html>`__ with `glumpy <https://github.com/rougier/glumpy>`__.

now vispy looks to build on the expertise of these developers and the
broader open-source community to build a high-performance opengl library.

----

external links
--------------

-  `user mailing
   list <https://groups.google.com/forum/#!forum/vispy>`__
-  `dev mailing
   list <https://groups.google.com/forum/#!forum/vispy-dev>`__
-  `chat room <https://gitter.im/vispy/vispy>`__
-  `developer chat room <https://gitter.im/vispy/vispy-dev>`__
-  `wiki <http://github.com/vispy/vispy/wiki>`__
-  `gallery <http://vispy.org/gallery.html>`__
-  `documentation <http://vispy.readthedocs.org>`__

.. |build status| image:: https://github.com/vispy/vispy/workflows/ci/badge.svg
   :target: https://github.com/vispy/vispy/actions
.. |coverage status| image:: https://img.shields.io/coveralls/vispy/vispy/main.svg
   :target: https://coveralls.io/r/vispy/vispy?branch=main
.. |zenodo link| image:: https://zenodo.org/badge/5822/vispy/vispy.svg
   :target: http://dx.doi.org/10.5281/zenodo.17869
.. |contributor covenant| image:: https://img.shields.io/badge/contributor%20covenant-2.0-4baaaa.svg
   :target: code_of_conduct.md
"
"Seaborn","<img src=""https://raw.githubusercontent.com/mwaskom/seaborn/master/doc/_static/logo-wide-lightbg.svg""><br>

--------------------------------------

seaborn: statistical data visualization
=======================================

[![pypi version](https://img.shields.io/pypi/v/seaborn.svg)](https://pypi.org/project/seaborn/)
[![license](https://img.shields.io/pypi/l/seaborn.svg)](https://github.com/mwaskom/seaborn/blob/master/license)
[![doi](https://joss.theoj.org/papers/10.21105/joss.03021/status.svg)](https://doi.org/10.21105/joss.03021)
[![tests](https://github.com/mwaskom/seaborn/workflows/ci/badge.svg)](https://github.com/mwaskom/seaborn/actions)
[![code coverage](https://codecov.io/gh/mwaskom/seaborn/branch/master/graph/badge.svg)](https://codecov.io/gh/mwaskom/seaborn)

seaborn is a python visualization library based on matplotlib. it provides a high-level interface for drawing attractive statistical graphics.


documentation
-------------

online documentation is available at [seaborn.pydata.org](https://seaborn.pydata.org).

the docs include a [tutorial](https://seaborn.pydata.org/tutorial.html), [example gallery](https://seaborn.pydata.org/examples/index.html), [api reference](https://seaborn.pydata.org/api.html), [faq](https://seaborn.pydata.org/faq), and other useful information.

to build the documentation locally, please refer to [`doc/readme.md`](doc/readme.md).

dependencies
------------

seaborn supports python 3.8+.

installation requires [numpy](https://numpy.org/), [pandas](https://pandas.pydata.org/), and [matplotlib](https://matplotlib.org/). some advanced statistical functionality requires [scipy](https://www.scipy.org/) and/or [statsmodels](https://www.statsmodels.org/).


installation
------------

the latest stable release (and required dependencies) can be installed from pypi:

    pip install seaborn

it is also possible to include optional statistical dependencies:

    pip install seaborn[stats]

seaborn can also be installed with conda:

    conda install seaborn

note that the main anaconda repository lags pypi in adding new releases, but conda-forge (`-c conda-forge`) typically updates quickly.

citing
------

a paper describing seaborn has been published in the [journal of open source software](https://joss.theoj.org/papers/10.21105/joss.03021). the paper provides an introduction to the key features of the library, and it can be used as a citation if seaborn proves integral to a scientific publication.

testing
-------

testing seaborn requires installing additional dependencies; they can be installed with the `dev` extra (e.g., `pip install .[dev]`).

to test the code, run `make test` in the source directory. this will exercise the unit tests (using [pytest](https://docs.pytest.org/)) and generate a coverage report.

code style is enforced with `flake8` using the settings in the [`setup.cfg`](./setup.cfg) file. run `make lint` to check. alternately, you can use `pre-commit` to automatically run lint checks on any files you are committing: just run `pre-commit install` to set it up, and then commit as usual going forward.

development
-----------

seaborn development takes place on github: https://github.com/mwaskom/seaborn

please submit bugs that you encounter to the [issue tracker](https://github.com/mwaskom/seaborn/issues) with a reproducible example demonstrating the problem. questions about usage are more at home on stackoverflow, where there is a [seaborn tag](https://stackoverflow.com/questions/tagged/seaborn).
"
"Superset","<!--
licensed to the apache software foundation (asf) under one
or more contributor license agreements.  see the notice file
distributed with this work for additional information
regarding copyright ownership.  the asf licenses this file
to you under the apache license, version 2.0 (the
""license""); you may not use this file except in compliance
with the license.  you may obtain a copy of the license at

  http://www.apache.org/licenses/license-2.0

unless required by applicable law or agreed to in writing,
software distributed under the license is distributed on an
""as is"" basis, without warranties or conditions of any
kind, either express or implied.  see the license for the
specific language governing permissions and limitations
under the license.
-->

# superset

[![license](https://img.shields.io/badge/license-apache%202.0-blue.svg)](https://opensource.org/licenses/apache-2.0)
[![github release (latest semver)](https://img.shields.io/github/v/release/apache/superset?sort=semver)](https://github.com/apache/superset/tree/latest)
[![build status](https://github.com/apache/superset/workflows/python/badge.svg)](https://github.com/apache/superset/actions)
[![pypi version](https://badge.fury.io/py/apache-superset.svg)](https://badge.fury.io/py/apache-superset)
[![coverage status](https://codecov.io/github/apache/superset/coverage.svg?branch=master)](https://codecov.io/github/apache/superset)
[![pypi](https://img.shields.io/pypi/pyversions/apache-superset.svg?maxage=2592000)](https://pypi.python.org/pypi/apache-superset)
[![get on slack](https://img.shields.io/badge/slack-join-orange.svg)](http://bit.ly/join-superset-slack)
[![documentation](https://img.shields.io/badge/docs-apache.org-blue.svg)](https://superset.apache.org)

<img
  src=""https://github.com/apache/superset/raw/master/superset-frontend/src/assets/branding/superset-logo-horiz-apache.png""
  alt=""superset""
  width=""500""
/>

a modern, enterprise-ready business intelligence web application.

[**why superset?**](#why-superset) |
[**supported databases**](#supported-databases) |
[**installation and configuration**](#installation-and-configuration) |
[**release notes**](releasing/readme.md#release-notes-for-recent-releases) |
[**get involved**](#get-involved) |
[**contributor guide**](#contributor-guide) |
[**resources**](#resources) |
[**organizations using superset**](resources/inthewild.md)

## why superset?

superset is a modern data exploration and data visualization platform. superset can replace or augment proprietary business intelligence tools for many teams. superset integrates well with a variety of data sources.

superset provides:

- a **no-code interface** for building charts quickly
- a powerful, web-based **sql editor** for advanced querying
- a **lightweight semantic layer** for quickly defining custom dimensions and metrics
- out of the box support for **nearly any sql** database or data engine
- a wide array of **beautiful visualizations** to showcase your data, ranging from simple bar charts to geospatial visualizations
- lightweight, configurable **caching layer** to help ease database load
- highly extensible **security roles and authentication** options
- an **api** for programmatic customization
- a **cloud-native architecture** designed from the ground up for scale

## screenshots & gifs

**large gallery of visualizations**

<kbd><img title=""gallery"" src=""superset-frontend/src/assets/images/screenshots/gallery.jpg""/></kbd><br/>

**craft beautiful, dynamic dashboards**

<kbd><img title=""view dashboards"" src=""superset-frontend/src/assets/images/screenshots/slack_dash.jpg""/></kbd><br/>

**no-code chart builder**

<kbd><img title=""slice & dice your data"" src=""superset-frontend/src/assets/images/screenshots/explore.jpg""/></kbd><br/>

**powerful sql editor**

<kbd><img title=""sql lab"" src=""superset-frontend/src/assets/images/screenshots/sql_lab.jpg""/></kbd><br/>

## supported databases

superset can query data from any sql-speaking datastore or data engine (presto, trino, athena, [and more](https://superset.apache.org/docs/databases/installing-database-drivers/)) that has a python db-api driver and a sqlalchemy dialect.

here are some of the major database solutions that are supported:

<p align=""center"">
  <img src=""superset-frontend/src/assets/images/redshift.png"" alt=""redshift"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/google-biquery.png"" alt=""google-biquery"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/snowflake.png"" alt=""snowflake"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/trino.png"" alt=""trino"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/presto.png"" alt=""presto"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/databricks.png"" alt=""databricks"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/druid.png"" alt=""druid"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/firebolt.png"" alt=""firebolt"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/timescale.png"" alt=""timescale"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/rockset.png"" alt=""rockset"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/postgresql.png"" alt=""postgresql"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/mysql.png"" alt=""mysql"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/mssql-server.png"" alt=""mssql-server"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/db2.png"" alt=""db2"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/sqlite.png"" alt=""sqlite"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/sybase.png"" alt=""sybase"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/mariadb.png"" alt=""mariadb"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/vertica.png"" alt=""vertica"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/oracle.png"" alt=""oracle"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/firebird.png"" alt=""firebird"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/greenplum.png"" alt=""greenplum"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/clickhouse.png"" alt=""clickhouse"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/exasol.png"" alt=""exasol"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/monet-db.png"" alt=""monet-db"" border=""0"" width=""200"" height=""80"" />
  <img src=""superset-frontend/src/assets/images/apache-kylin.png"" alt=""apache-kylin"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/hologres.png"" alt=""hologres"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/netezza.png"" alt=""netezza"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/pinot.png"" alt=""pinot"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/teradata.png"" alt=""teradata"" border=""0"" width=""200"" height=""80""/>
  <img src=""superset-frontend/src/assets/images/yugabyte.png"" alt=""yugabyte"" border=""0"" width=""200"" height=""80""/>
</p>

**a more comprehensive list of supported databases** along with the configuration instructions can be found [here](https://superset.apache.org/docs/databases/installing-database-drivers).

want to add support for your datastore or data engine? read more [here](https://superset.apache.org/docs/frequently-asked-questions#does-superset-work-with-insert-database-engine-here) about the technical requirements.

## installation and configuration

[extended documentation for superset](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose)

## get involved

- ask and answer questions on [stackoverflow](https://stackoverflow.com/questions/tagged/apache-superset) using the **apache-superset** tag
- [join our community's slack](http://bit.ly/join-superset-slack)
  and please read our [slack community guidelines](https://github.com/apache/superset/blob/master/code_of_conduct.md#slack-community-guidelines)
- [join our dev@superset.apache.org mailing list](https://lists.apache.org/list.html?dev@superset.apache.org)

## contributor guide

interested in contributing? check out our
[contributing.md](https://github.com/apache/superset/blob/master/contributing.md)
to find resources around contributing along with a detailed guide on
how to set up a development environment.

## resources

superset 2.0!
- [superset 2.0 meetup](https://preset.io/events/superset-2-0-meetup/)
- [superset 2.0 release notes](https://github.com/apache/superset/tree/master/releasing/release-notes-2-0)

understanding the superset points of view
- [the case for dataset-centric visualization](https://preset.io/blog/dataset-centric-visualization/)
- [understanding the superset semantic layer](https://preset.io/blog/understanding-superset-semantic-layer/)


- getting started with superset
  - [superset in 2 minutes using docker compose](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose#installing-superset-locally-using-docker-compose)
  - [installing database drivers](https://superset.apache.org/docs/databases/docker-add-drivers/)
  - [building new database connectors](https://preset.io/blog/building-database-connector/)
  - [create your first dashboard](https://superset.apache.org/docs/creating-charts-dashboards/first-dashboard)
  - [comprehensive tutorial for contributing code to apache superset
  ](https://preset.io/blog/tutorial-contributing-code-to-apache-superset/)
- [resources to master superset by preset](https://preset.io/resources/)

- deploying superset
  - [official docker image](https://hub.docker.com/r/apache/superset)
  - [helm chart](https://github.com/apache/superset/tree/master/helm/superset)

- recordings of past [superset community events](https://preset.io/events)
  - [mixed time series charts](https://preset.io/events/mixed-time-series-visualization-in-superset-workshop/)  
  - [how the bing team customized superset for the internal self-serve data & analytics platform](https://preset.io/events/how-the-bing-team-heavily-customized-superset-for-their-internal-data/)
  - [live demo: visualizing mongodb and pinot data using trino](https://preset.io/events/2021-04-13-visualizing-mongodb-and-pinot-data-using-trino/)
	- [introduction to the superset api](https://preset.io/events/introduction-to-the-superset-api/)
	- [building a database connector for superset](https://preset.io/events/2021-02-16-building-a-database-connector-for-superset/)

- visualizations
  - [creating viz plugins](https://superset.apache.org/docs/contributing/creating-viz-plugins/)
  - [managing and deploying custom viz plugins](https://medium.com/nmc-techblog/apache-superset-manage-custom-viz-plugins-in-production-9fde1a708e55)
  - [why apache superset is betting on apache echarts](https://preset.io/blog/2021-4-1-why-echarts/)

- [superset api](https://superset.apache.org/docs/rest-api)
"
"visualize_ML","visualize\_ml
=============

visualize\_ml is a python package made to visualize some of the steps involved while dealing with a machine learning problem. it is build on libraries like matplotlib for visualization and sklearn,scipy for statistical computations.

table of content:
~~~~~~~~~~~~~~~~~

-  requirements
-  install
-  let‚Äôs code

   -  explore module
   -  relation module

-  contribute
-  licence
-  copyright

let‚Äôs code
----------

when we start dealing with a machine learning problem some of the
initial steps involved are data exploration,analysis followed by feature
selection.below are the modules for these tasks.

1) data exploration
~~~~~~~~~~~~~~~~~~~

at this stage, we explore variables one by one using **uni-variate
analysis** which depends on whether the variable type is categorical or
continuous .to deal with this we have the **explore** module.

>>>explore module
~~~~~~~~~~~~~~~~~~

::

    visualize_ml.explore.plot(data_input,categorical_name=[],drop=[],plot_columns_size=4,bin_size=20,
    bar_width=0.2,wspace=0.5,hspace=0.8)

**continuous variables** : in case of continous variables it plots the
*histogram* for every variable and gives descriptive statistics for
them.

**categorical variables** : in case on categorical variables with 2 or
more classes it plots the *bar chart* for every variable and gives
descriptive statistics for them.

+---------------------+-----------------+---------------------------------------+
| parameters          | type            | description                           |
+=====================+=================+=======================================+
| data\_input         | dataframe       | this is the input dataframe with all  |
|                     |                 | data.(right now the input can be only |
|                     |                 | be a dataframe input.)                |
+---------------------+-----------------+---------------------------------------+
| categorical\_name   | list (default=[ | names of all categorical variable     |
|                     | ])              | columns with more than 2 classes, to  |
|                     |                 | distinguish them with the continuous  |
|                     |                 | variablesemply list implies that      |
|                     |                 | there are no categorical features     |
|                     |                 | with more than 2 classes.             |
+---------------------+-----------------+---------------------------------------+
| drop                | list default=[  | names of columns to be dropped.       |
|                     | ]               |                                       |
+---------------------+-----------------+---------------------------------------+
| plot\_columns\_size | int (default=4) | number of plots to display vertically |
|                     |                 | in the display window.the row size is |
|                     |                 | adjusted accordingly.                 |
+---------------------+-----------------+---------------------------------------+
| bin\_size           | int             | number of bins for the histogram      |
|                     | (default=‚Äúauto‚Äù | displayed in the categorical vs       |
|                     | )               | categorical category.                 |
+---------------------+-----------------+---------------------------------------+
| wspace              | float32         | horizontal padding between subplot on |
|                     | (default = 0.5) | the display window.                   |
+---------------------+-----------------+---------------------------------------+
| hspace              | float32         | vertical padding between subplot on   |
|                     | (default = 0.8) | the display window.                   |
+---------------------+-----------------+---------------------------------------+

**code snippet**

.. code :: python

    /* the data set is taken from famous titanic data(kaggle)*/

    import pandas as pd
    from visualize_ml import explore
    df = pd.read_csv(""dataset/train.csv"")

    explore.plot(df,[""survived"",""pclass"",""sex"",""sibsp"",""ticket"",""embarked""],drop=[""passengerid"",""name""])

.. figure:: /images/explore1.png?raw=true
   :alt: optional title

   graph made using explore module using matplotlib.

see the [dataset](https://www.kaggle.com/c/titanic/data)

**note:** while plotting all the rows with **nan** values and columns
with **character** values are removed(except if values are true and false ) only numeric data is plotted.

2) feature selection
~~~~~~~~~~~~~~~~~~~~

this is one of the challenging task to deal with for a ml task.here we
have to do **bi-variate analysis** to find out the relationship between
two variables. here, we look for association and disassociation between
variables at a pre-defined


**relation** module helps in visualizing the analysis done on various
combination of variables and see relation between them.

>>>relation module
~~~~~~~~~~~~~~~~~~~

::

    visualize_ml.relation.plot(data_input,target_name="""",categorical_name=[],drop=[],bin_size=10)

**continuous vs continuous variables:** to do the bi-variate analysis
*scatter plots* are made as their pattern indicates the relationship
between variables. to indicates the strength of relationship amongst
them we use correlation between them.

the graph displays the correlation coefficient along with other
information.

::

    correlation = covariance(x,y) / sqrt( var(x)*var(y))

-  -1: perfect negative linear correlation
-  +1:perfect positive linear correlation and
-  0: no correlation

**categorical vs categorical variables**: *stacked column charts* are
made to visualize the relation.\ **chi square test** is used to derive
the statistical significance of relationship between the variables. it
returns *probability* for the computed chi-square distribution with the
degree of freedom. for more information on chi test see `this`_

probability of 0: it indicates that both categorical variable are
dependent

probability of 1: it shows that both variables are independent.

the graph displays the *p\_value* along with other information. if it is
leass than **0.05** it states that the variables are dependent.

**categorical vs continuous variables:** to explore the relation between
categorical and continuous variables,box plots re drawn at each level of
categorical variables. if levels are small in number, it will not show
the statistical significance. **anova test** is used to derive the
statistical significance of relationship between the variables.

the graph displays the *p\_value* along with other information. if it is
leass than **0.05** it states that the variables are dependent.

for more information on anova test see
`this <https://onlinecourses.science.psu.edu/stat200/book/export/html/66>`__

+----------------+-----------+-------------------------------------------------+
| parameters     | type      | description                                     |
+================+===========+=================================================+
| data\_input    | dataframe | this is the input dataframe with all            |
|                |           | data.(right now the input can be only be a      |
|                |           | dataframe input.)                               |
+----------------+-----------+-------------------------------------------------+
| target\_name   | string    | the name of the target column.                  |
+----------------+-----------+-------------------------------------------------+
| categorical\_n | list      | names of all categorical variable columns with  |
| ame            | (default= | more than 2 classes, to distinguish them with   |
|                | [         | the continuous variablesemply list implies that |
|                | ])        | there are no categorical features with more     |
|                |           | than 2 classes.                                 |
+----------------+-----------+-------------------------------------------------+
| drop           | list      | names of columns to be dropped.                 |
|                | default=[ |                                                 |
|                | ]         |                                                 |
+----------------+-----------+-------------------------------------------------+
| plot\_columns\ | int       | number of plots to display vertically in the    |
| _size          | (default= | display window.the row size is adjusted         |
|                | 4)        | accordingly.                                    |
+----------------+-----------+-------------------------------------------------+
| bin\_size      | int       | number of bins for the histogram displayed in   |
|                | (default= | the categorical vs categorical category.        |
|                | ‚Äúauto‚Äù)   |                                                 |
+----------------+-----------+-------------------------------------------------+
| wspace         | float32   | horizontal padding between subplot on the       |
|                | (default  | display window.                                 |
|                | = 0.5)    |                                                 |
+----------------+-----------+-------------------------------------------------+
| hspace         | float32   | vertical padding between subplot on the display |
|                | (default  | window.                                         |
|                | = 0.8)    |                                                 |
+----------------+-----------+-------------------------------------------------+

**code snippet**

.. code :: python

    /* the data set is taken from famous titanic data(kaggle)*/
    import pandas as pd
    from visualize_ml import relation
    df = pd.read_csv(""dataset/train.csv"")

    relation.plot(df,""survived"",[""survived"",""pclass"",""sex"",""sibsp"",""ticket"",""embarked""],drop=[""passengerid"",""name""],bin_size=10)

.. figure:: /images/relation1.png?raw=true
   :alt: optional title

   graph made using relation module using matplotlib.

see the [dataset](https://www.kaggle.com/c/titanic/data)

**note:** while plotting all the rows with **nan** values and columns
with **non numeric** values are removed only numeric data is
plotted.only categorical taget variable with string values are allowed.

contribute
----------

if you want to contribute and add new feature feel free to send pull
request `here`_

this project is still under development so to report any bugs or request new features, head over to the issues page

licence
-------
licensed under `the mit license (mit)`_.

copyright
---------
ayush1997(c) 2016

.. _here: https://github.com/ayush1997/visualize_ml
.. _the mit license (mit): https://github.com/ayush1997/visualize_ml/blob/master/license.txt
"
"Bowtie","`installation`_ |
`documentation <https://bowtie-py.readthedocs.io/en/stable>`__ |
`gitter chat <https://gitter.im/bowtie-py/lobby>`__ |
`google group <https://groups.google.com/forum/#!forum/bowtie-py>`__

======
bowtie
======

|build status| |documentation status| |pypi version| |conda version| |pypi| |codecov| |prettier|

.. figure:: https://cloud.githubusercontent.com/assets/86304/20045988/69e5678a-a45a-11e6-853b-7f60a615c9da.gif
   :alt: bowtie demo
   :target: https://github.com/jwkvam/bowtie-demo/blob/master/example.py

introduction
------------

bowtie is a library for writing dashboards in python. no need to know
web frameworks or javascript, focus on building functionality in python.
interactively explore your data in new ways! deploy and share with
others!

demo
----

see a live `example <https://bowtie-demo.herokuapp.com/>`__ generated
from this
`code <https://github.com/jwkvam/bowtie-demo/blob/master/example.py>`__!

gallery
-------

for more examples, check out the
`gallery <https://github.com/jwkvam/bowtie/wiki/gallery>`__ and
`repos <https://github.com/jwkvam/bowtie/wiki/repos>`__.
feel free to add your own creations!

installation
------------

if you use ``conda``, you can install with::

    conda install -c conda-forge bowtie-py

if you use ``pip``, you can install with::

    pip install bowtie

requirements
^^^^^^^^^^^^

bowtie uses `yarn <https://yarnpkg.com>`__ to manage node packages.
if you installed bowtie through ``conda``, yarn was also installed as a dependency.
yarn can be installed through conda::

    conda install -c conda-forge yarn

otherwise follow `install
instructions <https://yarnpkg.com/en/docs/install>`__ for yarn for your
os.

documentation
-------------

available `here <https://bowtie-py.readthedocs.io/en/latest/>`__.

jupyter integration
-------------------

an early integration with jupyter has been prototyped.
i encourage you to try it out and share feedback.
i hope this will make it easier to make bowtie apps.

read the
`documentation <https://bowtie-py.readthedocs.io/en/latest/jupyter.html>`__
for more details.

docker
------

docker images are provided as an alternative way to use bowtie. they are
available on `docker hub <https://hub.docker.com/r/jwkvam/bowtie/>`__::

    docker pull jwkvam/bowtie

read the
`documentation <https://bowtie-py.readthedocs.io/en/latest/docker.html>`__
for more details.

the goal
--------

.. figure:: https://cloud.githubusercontent.com/assets/86304/18606859/8ced55a6-7c70-11e6-8b5e-fba0ffcd78da.png
      :alt: @astrobiased @treycausey @vagabondjack the lack of a comprehensive production-grade shiny-alike for python is a big problem

contributing
------------

you can help bowtie in many ways including:

- try it `out <http://bowtie-py.readthedocs.io/en/latest/quickstart.html>`__ and report bugs or what was difficult.
- help improve the `documentation <https://github.com/jwkvam/bowtie/tree/master/doc>`__.
- write new `widgets <http://bowtie-py.readthedocs.io/en/latest/newcomponents.html>`__.
- provide hosting for apps in the gallery.
- say `thanks <https://saythanks.io/to/jwkvam>`__!

|coffee|

.. |join the chat at https://gitter.im/bowtie-py/lobby| image:: https://badges.gitter.im/bowtie-py/lobby.svg
   :target: https://gitter.im/bowtie-py/lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
.. |forum| image:: https://img.shields.io/badge/-google%20group-blue.svg
   :target: https://groups.google.com/forum/#!forum/bowtie-py
.. |documentation status| image:: https://readthedocs.org/projects/bowtie-py/badge/?version=latest
   :target: http://bowtie-py.readthedocs.io/en/latest/?badge=latest
.. |build status| image:: https://travis-ci.org/jwkvam/bowtie.svg?branch=master
   :target: https://travis-ci.org/jwkvam/bowtie
.. |pypi version| image:: https://badge.fury.io/py/bowtie.svg
   :target: https://badge.fury.io/py/bowtie
.. |conda version| image:: https://anaconda.org/conda-forge/bowtie-py/badges/version.svg
   :target: https://anaconda.org/conda-forge/bowtie-py
.. |pypi| image:: https://img.shields.io/pypi/pyversions/bowtie.svg
   :target: https://pypi.python.org/pypi/bowtie/
.. |codecov| image:: https://codecov.io/gh/jwkvam/bowtie/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/jwkvam/bowtie
.. |coffee| image:: https://www.buymeacoffee.com/assets/img/custom_images/purple_img.png
   :target: https://www.buymeacoffee.com/jwkvam
.. |prettier| image:: https://img.shields.io/badge/code_style-prettier-ff69b4.svg
   :target: https://github.com/prettier/prettier
"
"PyCM","<div align=""center"">
<img src=""https://github.com/sepandhaghighi/pycm/raw/master/otherfiles/logo.png"" width=""550"">
<h1>pycm: python confusion matrix</h1>
<br/>
<a href=""https://www.python.org/""><img src=""https://img.shields.io/badge/built%20with-python3-green.svg"" alt=""built with python3"" /></a>
<a href=""/document""><img src=""https://img.shields.io/badge/doc-latest-orange.svg""></a>
<a href=""https://codecov.io/gh/sepandhaghighi/pycm"">
  <img src=""https://codecov.io/gh/sepandhaghighi/pycm/branch/master/graph/badge.svg"" />
</a>
<a href=""https://badge.fury.io/py/pycm""><img src=""https://badge.fury.io/py/pycm.svg"" alt=""pypi version"" height=""18""></a>
<a href=""https://anaconda.org/sepandhaghighi/pycm""><img src=""https://anaconda.org/sepandhaghighi/pycm/badges/version.svg""></a>
<a href=""https://colab.research.google.com/github/sepandhaghighi/pycm/blob/master"">
  <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""document""/>
</a>
<a href=""https://discord.com/invite/zqpu2b3j3f"">
  <img src=""https://img.shields.io/discord/901883546162065408.svg"" alt=""discord channel"">
</a>
</div>

## table of contents
   * [overview](https://github.com/sepandhaghighi/pycm#overview)
   * [installation](https://github.com/sepandhaghighi/pycm#installation)
   * [usage](https://github.com/sepandhaghighi/pycm#usage)
   * [document](https://github.com/sepandhaghighi/pycm/tree/master/document)
   * [try pycm in your browser](https://github.com/sepandhaghighi/pycm#try-pycm-in-your-browser)
   * [issues & bug reports](https://github.com/sepandhaghighi/pycm#issues--bug-reports)
   * [todo](https://github.com/sepandhaghighi/pycm/blob/master/todo.md)
   * [contribution](https://github.com/sepandhaghighi/pycm/blob/master/.github/contributing.md)
   * [acknowledgments](https://github.com/sepandhaghighi/pycm#acknowledgments)
   * [cite](https://github.com/sepandhaghighi/pycm#cite)
   * [authors](https://github.com/sepandhaghighi/pycm/blob/master/authors.md)
   * [license](https://github.com/sepandhaghighi/pycm/blob/master/license)
   * [show your support](https://github.com/sepandhaghighi/pycm#show-your-support)
   * [changelog](https://github.com/sepandhaghighi/pycm/blob/master/changelog.md)
   * [code of conduct](https://github.com/sepandhaghighi/pycm/blob/master/.github/code_of_conduct.md)

## overview

<p align=""justify"">	
pycm is a multi-class confusion matrix library written in python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters.
pycm is the swiss-army knife of confusion matrices, targeted mainly at data scientists that need a broad array of metrics for predictive models and accurate evaluation of a large variety of classifiers.

</p>

<div align=""center"">
<img src=""https://github.com/sepandhaghighi/pycm/raw/master/otherfiles/block_diagram.jpg"">
<p>fig1. confusionmatrix block diagram</p>
</div>

<table>
	<tr>
		<td align=""center"">open hub</td>
		<td align=""center""><a href=""https://www.openhub.net/p/pycm""><img src=""https://www.openhub.net/p/pycm/widgets/project_thin_badge.gif""></a></td>
	</tr>
	<tr>
		<td align=""center"">pypi counter</td>
		<td align=""center""><a href=""http://pepy.tech/project/pycm""><img src=""http://pepy.tech/badge/pycm""></a></td>
	</tr>
	<tr>
		<td align=""center"">github stars</td>
		<td align=""center""><a href=""https://github.com/sepandhaghighi/pycm""><img src=""https://img.shields.io/github/stars/sepandhaghighi/pycm.svg?style=social&label=stars""></a></td>
	</tr>
</table>



<table>
	<tr> 
		<td align=""center"">branch</td>
		<td align=""center"">master</td>
		<td align=""center"">dev</td>
	</tr>
    <tr>
		<td align=""center"">ci</td>
		<td align=""center""><img src=""https://github.com/sepandhaghighi/pycm/workflows/ci/badge.svg?branch=master""></td>
		<td align=""center""><img src=""https://github.com/sepandhaghighi/pycm/workflows/ci/badge.svg?branch=dev""></td>
	</tr>
</table>


<table>
	<tr> 
		<td align=""center"">code quality</td>
		<td align=""center""><a class=""badge-align"" href=""https://www.codacy.com/app/sepand-haghighi/pycm?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=sepandhaghighi/pycm&amp;utm_campaign=badge_grade""><img src=""https://api.codacy.com/project/badge/grade/5d9463998a0040d09afc2b80c389365c""/></a></td>
		<td align=""center""><a href=""https://www.codefactor.io/repository/github/sepandhaghighi/pycm/overview/dev""><img src=""https://www.codefactor.io/repository/github/sepandhaghighi/pycm/badge/dev"" alt=""codefactor"" /></a></td>
		<td align=""center""><a href=""https://codebeat.co/projects/github-com-sepandhaghighi-pycm-dev""><img alt=""codebeat badge"" src=""https://codebeat.co/badges/f6642af1-c343-48c2-bd3e-eee802facf39"" /></a></td>
	</tr>
</table>

## installation

‚ö†Ô∏è  pycm 2.4 is the last version to support **python 2.7** & **python 3.4**

‚ö†Ô∏è  plotting capability requires **matplotlib (>= 3.0.0)** or **seaborn (>= 0.9.1)**

### source code
- download [version 3.8](https://github.com/sepandhaghighi/pycm/archive/v3.8.zip) or [latest source ](https://github.com/sepandhaghighi/pycm/archive/dev.zip)
- run `pip install -r requirements.txt` or `pip3 install -r requirements.txt` (need root access)
- run `python3 setup.py install` or `python setup.py install` (need root access)

### pypi

- check [python packaging user guide](https://packaging.python.org/installing/)
- run `pip install pycm==3.8` or `pip3 install pycm==3.8` (need root access)

### conda

- check [conda managing package](https://conda.io/)
- update conda using `conda update conda` (need root access)
- run `conda install -c sepandhaghighi pycm` (need root access)

### easy install

- run `easy_install --upgrade pycm` (need root access)

### matlab

- download and install [matlab](https://www.mathworks.com/products/matlab.html) (>=8.5, 64/32 bit)
- download and install [python3.x](https://www.python.org/downloads/) (>=3.5, 64/32 bit)
	- [x] select `add to path` option
	- [x] select `install pip` option
- run `pip install pycm` or `pip3 install pycm` (need root access)
- configure python interpreter

```matlab
>> pyversion python_executable_full_path
```

- visit [matlab examples](https://github.com/sepandhaghighi/pycm/tree/master/matlab)

## usage

### from vector

```pycon
>>> from pycm import *
>>> y_actu = [2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2]
>>> y_pred = [0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2]
>>> cm = confusionmatrix(actual_vector=y_actu, predict_vector=y_pred)
>>> cm.classes
[0, 1, 2]
>>> cm.table
{0: {0: 3, 1: 0, 2: 0}, 1: {0: 0, 1: 1, 2: 2}, 2: {0: 2, 1: 1, 2: 3}}
>>> cm.print_matrix()
predict 0       1       2       
actual
0       3       0       0       

1       0       1       2       

2       2       1       3   

>>> cm.print_normalized_matrix()
predict       0             1             2             
actual
0             1.0           0.0           0.0           

1             0.0           0.33333       0.66667       

2             0.33333       0.16667       0.5          

>>> cm.stat(summary=true)
overall statistics : 

acc macro                                                         0.72222
f1 macro                                                          0.56515
fpr macro                                                         0.22222
kappa                                                             0.35484
overall acc                                                       0.58333
ppv macro                                                         0.56667
soa1(landis & koch)                                               fair
tpr macro                                                         0.61111
zero-one loss                                                     5

class statistics :

classes                                                           0             1             2             
acc(accuracy)                                                     0.83333       0.75          0.58333       
auc(area under the roc curve)                                     0.88889       0.61111       0.58333       
auci(auc value interpretation)                                    very good     fair          poor          
f1(f1 score - harmonic mean of precision and sensitivity)         0.75          0.4           0.54545       
fn(false negative/miss/type 2 error)                              0             2             3             
fp(false positive/type 1 error/false alarm)                       2             1             2             
fpr(fall-out or false positive rate)                              0.22222       0.11111       0.33333       
n(condition negative)                                             9             9             6             
p(condition positive or support)                                  3             3             6             
pop(population)                                                   12            12            12            
ppv(precision or positive predictive value)                       0.6           0.5           0.6           
tn(true negative/correct rejection)                               7             8             4             
ton(test outcome negative)                                        7             10            7             
top(test outcome positive)                                        5             2             5             
tp(true positive/hit)                                             3             1             3             
tpr(sensitivity, recall, hit rate, or true positive rate)         1.0           0.33333       0.5 

```

### direct cm

```pycon
>>> from pycm import *
>>> cm2 = confusionmatrix(matrix={""class1"": {""class1"": 1, ""class2"":2}, ""class2"": {""class1"": 0, ""class2"": 5}})
>>> cm2
pycm.confusionmatrix(classes: ['class1', 'class2'])
>>> cm2.classes
['class1', 'class2']
>>> cm2.print_matrix()
predict      class1       class2       
actual
class1       1            2            

class2       0            5            

>>> cm2.print_normalized_matrix()
predict       class1        class2        
actual
class1        0.33333       0.66667       

class2        0.0           1.0 

>>> cm2.stat(summary=true)
overall statistics : 

acc macro                                                         0.75
f1 macro                                                          0.66667
fpr macro                                                         0.33333
kappa                                                             0.38462
overall acc                                                       0.75
ppv macro                                                         0.85714
soa1(landis & koch)                                               fair
tpr macro                                                         0.66667
zero-one loss                                                     2

class statistics :

classes                                                           class1        class2        
acc(accuracy)                                                     0.75          0.75          
auc(area under the roc curve)                                     0.66667       0.66667       
auci(auc value interpretation)                                    fair          fair          
f1(f1 score - harmonic mean of precision and sensitivity)         0.5           0.83333       
fn(false negative/miss/type 2 error)                              2             0             
fp(false positive/type 1 error/false alarm)                       0             2             
fpr(fall-out or false positive rate)                              0.0           0.66667       
n(condition negative)                                             5             3             
p(condition positive or support)                                  3             5             
pop(population)                                                   8             8             
ppv(precision or positive predictive value)                       1.0           0.71429       
tn(true negative/correct rejection)                               5             1             
ton(test outcome negative)                                        7             1             
top(test outcome positive)                                        1             7             
tp(true positive/hit)                                             1             5             
tpr(sensitivity, recall, hit rate, or true positive rate)         0.33333       1.0
     
```

* `matrix()` and `normalized_matrix()` renamed to `print_matrix()` and `print_normalized_matrix()` in `version 1.5`

### activation threshold

`threshold` is added in `version 0.9` for real value prediction.
for more information visit [example3](http://www.pycm.io/doc/example3.html ""example3"")

### load from file

`file` is added in `version 0.9.5` in order to load saved confusion matrix with `.obj` format generated by `save_obj` method.

for more information visit [example4](http://www.pycm.io/doc/example4.html ""example4"")

### sample weights

`sample_weight` is added in `version 1.2`

for more information visit [example5](http://www.pycm.io/doc/example5.html ""example5"")

### transpose

`transpose` is added in `version 1.2` in order to transpose input matrix (only in `direct cm` mode)

### relabel

`relabel` method is added in `version 1.5` in order to change confusionmatrix classnames.

```pycon
>>> cm.relabel(mapping={0:""l1"",1:""l2"",2:""l3""})
>>> cm
pycm.confusionmatrix(classes: ['l1', 'l2', 'l3'])
```

### position

`position` method is added in `version 2.8` in order to find the indexes of observations in `predict_vector` which made tp, tn, fp, fn.

```pycon
>>> cm.position()
{0: {'fn': [], 'fp': [0, 7], 'tp': [1, 4, 9], 'tn': [2, 3, 5, 6, 8, 10, 11]}, 1: {'fn': [5, 10], 'fp': [3], 'tp': [6], 'tn': [0, 1, 2, 4, 7, 8, 9, 11]}, 2: {'fn': [0, 3, 7], 'fp': [5, 10], 'tp': [2, 8, 11], 'tn': [1, 4, 6, 9]}}
```

### to array

`to_array` method is added in `version 2.9` in order to returns the confusion matrix in the form of a numpy array. this can be helpful to apply different operations over the confusion matrix for different purposes such as aggregation, normalization, and combination.

```pycon
>>> cm.to_array()
array([[3, 0, 0],
       [0, 1, 2],
       [2, 1, 3]])
>>> cm.to_array(normalized=true)
array([[1.     , 0.     , 0.     ],
       [0.     , 0.33333, 0.66667],
       [0.33333, 0.16667, 0.5    ]])
>>> cm.to_array(normalized=true, one_vs_all=true, class_name=""l1"")
array([[1.     , 0.     ],
       [0.22222, 0.77778]])
```

### combine

`combine` method is added in `version 3.0` in order to merge two confusion matrices. this option will be useful in mini-batch learning.

```pycon
>>> cm_combined = cm2.combine(cm3)
>>> cm_combined.print_matrix()
predict      class1       class2       
actual
class1       2            4            

class2       0            10           

```

### plot

`plot` method is added in `version 3.0` in order to plot a confusion matrix using matplotlib or seaborn.

```pycon
>>> cm.plot()
```

<img src=""https://github.com/sepandhaghighi/pycm/raw/master/otherfiles/plot1.png"">

```pycon
>>> from matplotlib import pyplot as plt
>>> cm.plot(cmap=plt.cm.greens, number_label=true, plot_lib=""matplotlib"")
```

<img src=""https://github.com/sepandhaghighi/pycm/raw/master/otherfiles/plot2.png"">		

```pycon
>>> cm.plot(cmap=plt.cm.reds, normalized=true, number_label=true, plot_lib=""seaborn"")
```

<img src=""https://github.com/sepandhaghighi/pycm/raw/master/otherfiles/plot3.png"">

### roc curve

`roccurve`, added in `version 3.7`, is devised to compute the receiver operating characteristic (roc) or simply roc curve. in roc curves, the y axis represents the true positive rate, and the x axis represents the false positive rate. thus, the ideal point is located at the top left of the curve, and a larger area under the curve represents better performance. roc curve is a graphical representation of binary classifiers' performance. in pycm, `roccurve` binarizes the output based on the ""one vs. rest"" strategy to provide an extension of roc for multi-class classifiers. getting the actual labels vector, the target probability estimates of the positive classes, and the list of ordered labels of classes, this method is able to compute and plot tpr-fpr pairs for different discrimination thresholds and compute the area under the roc curve.

```pycon
>>> crv = roccurve(actual_vector=np.array([1, 1, 2, 2]), probs=np.array([[0.1, 0.9], [0.4, 0.6], [0.35, 0.65], [0.8, 0.2]]), classes=[2, 1])
>>> crv.thresholds
[0.1, 0.2, 0.35, 0.4, 0.6, 0.65, 0.8, 0.9]
>>> auc_trp = crv.area()
>>> auc_trp[1]
0.75
>>> auc_trp[2]
0.75
```

### precision-recall curve

`prcurve`, added in `version 3.7`, is devised to compute the precision-recall curve in which the y axis represents the precision, and the x axis represents the recall of a classifier. thus, the ideal point is located at the top right of the curve, and a larger area under the curve represents better performance. precision-recall curve is a graphical representation of binary classifiers' performance. in pycm, `prcurve` binarizes the output based on the ""one vs. rest"" strategy to provide an extension of this curve for multi-class classifiers. getting the actual labels vector, the target probability estimates of the positive classes, and the list of ordered labels of classes, this method is able to compute and plot precision-recall pairs for different discrimination thresholds and compute the area under the curve.

```pycon
>>> crv = prcurve(actual_vector=np.array([1, 1, 2, 2]), probs=np.array([[0.1, 0.9], [0.4, 0.6], [0.35, 0.65], [0.8, 0.2]]), classes=[2, 1])
>>> crv.thresholds
[0.1, 0.2, 0.35, 0.4, 0.6, 0.65, 0.8, 0.9]
>>> auc_trp = crv.area()
>>> auc_trp[1]
0.29166666666666663
>>> auc_trp[2]
0.29166666666666663
```

### parameter recommender

this option has been added in `version 1.9` to recommend the most related parameters considering the characteristics of the input dataset.
the suggested parameters are selected according to some characteristics of the input such as being balance/imbalance and binary/multi-class.
all suggestions can be categorized into three main groups: imbalanced dataset, binary classification for a balanced dataset, and multi-class classification for a balanced dataset.
the recommendation lists have been gathered according to the respective paper of each parameter and the capabilities which had been claimed by the paper.

```pycon
>>> cm.imbalance
false
>>> cm.binary
false
>>> cm.recommended_list
['mcc', 'tpr micro', 'acc', 'ppv macro', 'bcd', 'overall mcc', 'hamming loss', 'tpr macro', 'zero-one loss', 'err', 'ppv micro', 'overall acc']

```

`is_imbalanced` parameter has been added in `version 3.3`, so the user can indicate whether the concerned dataset is imbalanced or not. as long as the user does not provide any information in this regard, the automatic detection algorithm will be used.

```pycon
>>> cm = confusionmatrix(y_actu, y_pred, is_imbalanced = true)
>>> cm.imbalance
true
>>> cm = confusionmatrix(y_actu, y_pred, is_imbalanced = false)
>>> cm.imbalance
false
```

### compare

in `version 2.0`, a method for comparing several confusion matrices is introduced. this option is a combination of several overall and class-based benchmarks. each of the benchmarks evaluates the performance of the classification algorithm from good to poor and give them a numeric score. the score of good and poor performances are 1 and 0, respectively.

after that, two scores are calculated for each confusion matrices, overall and class-based. the overall score is the average of the score of seven overall benchmarks which are landis & koch, cramer, matthews, goodman-kruskal's lambda a, goodman-kruskal's lambda b, krippendorff's alpha, and pearson's c. in the same manner, the class-based score is the average of the score of six class-based benchmarks which are positive likelihood ratio interpretation, negative likelihood ratio interpretation, discriminant power interpretation, auc value interpretation, matthews correlation coefficient interpretation and yule's q interpretation. it should be noticed that if one of the benchmarks returns none for one of the classes, that benchmarks will be eliminated in total averaging. if the user sets weights for the classes, the averaging over the value of class-based benchmark scores will transform to a weighted average.

if the user sets the value of `by_class` boolean input `true`, the best confusion matrix is the one with the maximum class-based score. otherwise, if a confusion matrix obtains the maximum of both overall and class-based scores, that will be reported as the best confusion matrix, but in any other case, the compared object doesn‚Äôt select the best confusion matrix.

```pycon
>>> cm2 = confusionmatrix(matrix={0:{0:2, 1:50, 2:6}, 1:{0:5, 1:50, 2:3}, 2:{0:1, 1:7, 2:50}})
>>> cm3 = confusionmatrix(matrix={0:{0:50, 1:2, 2:6}, 1:{0:50, 1:5, 2:3}, 2:{0:1, 1:55, 2:2}})
>>> cp = compare({""cm2"":cm2, ""cm3"":cm3})
>>> print(cp)
best : cm2

rank  name   class-score       overall-score
1     cm2    0.50278           0.58095
2     cm3    0.33611           0.52857

>>> cp.best
pycm.confusionmatrix(classes: [0, 1, 2])
>>> cp.sorted
['cm2', 'cm3']
>>> cp.best_name
'cm2'
```

### online help

`online_help` function is added in `version 1.1` in order to open each statistics definition in web browser

```pycon
>>> from pycm import online_help
>>> online_help(""j"")
>>> online_help(""soa1(landis & koch)"")
>>> online_help(2)
```

* list of items are available by calling `online_help()` (without argument)
* if pycm website is not available, set `alt_link = true` (new in `version 2.4`)

### acceptable data types

**confusionmatrix**

1. `actual_vector` : python `list` or numpy `array` of any stringable objects
2. `predict_vector` : python `list` or numpy `array` of any stringable objects
3. `matrix` : `dict`
4. `digit`: `int`
5. `threshold` : `functiontype (function or lambda)`
6. `file` : `file object`
7. `sample_weight` : python `list` or numpy `array` of numbers
8. `transpose` : `bool`
9. `classes` : python `list`
10. `is_imbalanced` : `bool`

* run `help(confusionmatrix)` for `confusionmatrix` object details

**compare**

1. `cm_dict` : python `dict` of `confusionmatrix` object (`str` : `confusionmatrix`)
2. `by_class` : `bool`
3. `class_weight` : python `dict` of class weights (`class_name` : `float`)
4. `class_benchmark_weight`: python `dict` of class benchmark weights (`class_benchmark_name` : `float`)
5. `overall_benchmark_weight`: python `dict` of overall benchmark weights (`overall_benchmark_name` : `float`)
6. `digit`: `int`

* run `help(compare)` for `compare` object details

**roccurve**

1. `actual_vector` : python `list` or numpy `array` of any stringable objects
2. `probs` : python `list` or numpy `array`
3. `classes` : python `list`
4. `thresholds`: python `list` or numpy `array`
5. `sample_weight`: python `list` or numpy `array`

**prcurve**

1. `actual_vector` : python `list` or numpy `array` of any stringable objects
2. `probs` : python `list` or numpy `array`
3. `classes` : python `list`
4. `thresholds`: python `list` or numpy `array`
5. `sample_weight`: python `list` or numpy `array`

for more information visit [here](https://github.com/sepandhaghighi/pycm/tree/master/document ""document"")

<div align=""center"">

<a href=""https://asciinema.org/a/171863"" target=""_blank""><img src=""https://asciinema.org/a/171863.png""/></a>
</div>

## try pycm in your browser!

pycm can be used online in interactive jupyter notebooks via the binder or colab services! try it out now! :

[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sepandhaghighi/pycm/master)

[![google colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sepandhaghighi/pycm/blob/master)

* check `examples` in `document` folder

## issues & bug reports

1. fill an issue and describe it. we'll check it asap!
    - please complete the issue template
2. discord : [https://discord.com/invite/zqpu2b3j3f](https://discord.com/invite/zqpu2b3j3f)
3. website : [https://www.pycm.io](https://www.pycm.io)
4. mailing list : [https://mail.python.org/mailman3/lists/pycm.python.org/](https://mail.python.org/mailman3/lists/pycm.python.org/)
5. email : [info@pycm.io](mailto:info@pycm.io ""info@pycm.io"")

## acknowledgments

[nlnet foundation](https://nlnet.nl) has supported the pycm project from version **3.6** to **4.0** through the [ngi assure](https://nlnet.nl/assure) fund. this fund is set up by [nlnet foundation](https://nlnet.nl) with funding from the european commission's [next generation internet program](https://ngi.eu), administered by dg communications networks, content, and technology under grant agreement [**no 957073**](https://nlnet.nl/project/pycm/).

<a href=""https://nlnet.nl""><img src=""https://github.com/sepandhaghighi/pycm/raw/master/otherfiles/nlnet.svg"" height=""50px"" alt=""nlnet foundation""></a> &nbsp;  <a href=""https://nlnet.nl/assure""><img src=""https://github.com/sepandhaghighi/pycm/raw/master/otherfiles/ngiassure.svg"" height=""50px"" alt=""ngi assure""></a>

[python software foundation (psf)](https://www.python.org/psf/) grants pycm library partially for version **3.7**. [psf](https://www.python.org/psf/) is the organization behind python. their mission is to promote, protect, and advance the python programming language and to support and facilitate the growth of a diverse and international community of python programmers.

<a href=""https://www.python.org/psf/""><img src=""https://github.com/sepandhaghighi/pycm/raw/master/otherfiles/psf.png"" height=""55px"" alt=""python software foundation""></a>

## cite

if you use pycm in your research, we would appreciate citations to the following paper :

<pre>
haghighi, s., jasemi, m., hessabi, s. and zolanvari, a. (2018). pycm: multiclass confusion matrix library in python. journal of open source software, 3(25), p.729.
</pre>
<pre>

@article{haghighi2018,
  doi = {10.21105/joss.00729},
  url = {https://doi.org/10.21105/joss.00729},
  year  = {2018},
  month = {may},
  publisher = {the open journal},
  volume = {3},
  number = {25},
  pages = {729},
  author = {sepand haghighi and masoomeh jasemi and shaahin hessabi and alireza zolanvari},
  title = {{pycm}: multiclass confusion matrix library in python},
  journal = {journal of open source software}
}

</pre>

download [pycm.bib](http://www.pycm.io/pycm.bib)

<table>
	<tr> 
		<td align=""center"">joss</td>
		<td align=""center""><a href=""https://doi.org/10.21105/joss.00729""><img src=""http://joss.theoj.org/papers/10.21105/joss.00729/status.svg""></a></td>
	</tr>
	<tr>
		<td align=""center"">zenodo</td>
		<td align=""center""><a href=""https://doi.org/10.5281/zenodo.1157173""><img src=""https://zenodo.org/badge/doi/10.5281/zenodo.1157173.svg"" alt=""doi""></a></td>
	</tr>
	<tr>
		<td align=""center"">researchgate</td>
		<td align=""center""><a href=""https://www.researchgate.net/project/pycm-python-confusion-matrix""><img src=""https://img.shields.io/badge/researchgate-pycm-yellow.svg""></a></td>
	</tr>
</table>

## show your support

### star this repo

give a ‚≠êÔ∏è if this project helped you!

### donate to our project

if you do like our project and we hope that you do, can you please support us? our project is not and is never going to be working for profit. we need the money just so we can continue doing what we do ;-) .

<a href=""http://www.pycm.io/donate.html"" target=""_blank""><img src=""http://www.pycm.io/images/donate-button.png"" height=""90px"" width=""270px"" alt=""pycm donation""></a>
"
"Lambdo","[![license: mit](https://img.shields.io/badge/license-mit-brightgreen.svg)](https://github.com/asavinov/lambdo/blob/master/license)
[![python 3.6](https://img.shields.io/badge/python-3.6-brightgreen.svg)](https://www.python.org/downloads/release/python-360/)
[![pypi version](https://img.shields.io/pypi/v/python-lambdo.svg)](https://pypi.python.org/pypi/python-lambdo)

[![paper](https://img.shields.io/badge/paper-pdf-blueviolet.svg)](https://www.researchgate.net/publication/348079767_on_the_importance_of_functions_in_data_modeling)
[![paper](https://img.shields.io/badge/paper-pdf-blueviolet.svg)](https://www.researchgate.net/publication/337336089_concept-oriented_model_modeling_and_processing_data_using_functions)
[![paper](https://img.shields.io/badge/paper-pdf-blueviolet.svg)](https://www.researchgate.net/publication/316551218_from_group-by_to_accumulation_data_aggregation_revisited)
[![paper](https://img.shields.io/badge/paper-pdf-blueviolet.svg)](https://www.researchgate.net/publication/303840097_concept-oriented_model_the_functional_view)
[![paper](https://img.shields.io/badge/paper-pdf-blueviolet.svg)](https://www.researchgate.net/publication/301764816_joins_vs_links_or_relational_join_considered_harmful)

# feature engineering and machine learning: together at last!

lambdo is a workflow engine which significantly simplifies data analysis by *unifying* feature engineering and machine learning operations. lambdo data analysis workflow does not distinguish between them and any node can be treated either as a feature or as prediction, and both of them can be trained.

such a unification is possible because of the underlying *column-oriented* data processing paradigm which treats columns as first-class elements of the data processing pipeline having the same rights as tables. in lambdo, a workflow consist of *table population* operations which process sets of records and *column evaluation* operations which produce new columns (features) from existing columns. this radically changes the way data is processed. the same approach is used also in bistro: <https://github.com/asavinov/bistro>

here are some unique distinguishing features of lambdo:

* **no difference between features and models.** lambdo unifies feature engineering and machine learning so that a workflow involves many feature definitions and many machine learning algorithms. it is especially important for deep learning where abstract intermediate features have to be learned.
* **one workflow for both prediction and training.** lambdo nodes combine applying a transformation with training its model so that nodes of a workflow can be re-trained when required. this also guarantees that the same features will be used for both learning phase and prediction phase.
* **columns first.** lambdo workflow use column operations along with table operations which makes many operations much simpler.
* **user-defined functions for extensibility.** lambdo relies on user-defined functions which can be as simple as format conversion and as complex as deep neural networks.
* **analysis of time-series and forecasting made easy.** lambdo makes time series analysis much simpler by providing many using mechanisms like column families (for example, several moving averages with different window sizes), window-awareness (generation of windows is a built-in function), pre-defined functions for extracting goals.
* **as flexible as programming and as easy as ide.** lambdo is positioned between (python) programming and interactive environments (like knime)

# contents

* [why lambdo?](#why-lambdo)
  * [why feature engineering?](#why-feature-engineering)
  * [uniting feature engineering with data mining](#uniting-feature-engineering-with-data-mining)
  * [any transformation model has an automatic training procedure](#any-transformation-model-has-an-automatic-training-procedure)
  * [columns first: column-orientation as a basis for feature engineering](#columns-first-column-orientation-as-a-basis-for-feature-engineering)
  * [time series first: time series analysis and forecasting made easy](#time-series-first-time-series-analysis-and-forecasting-made-easy)

* [getting started with lambdo](#getting-started-with-lambdo)
  * [workflow definition](#workflow-definition)
    * [workflow structure](#workflow-structure)
    * [imports](#imports)
  * [table definition](#table-definition)
    * [table population function](#table-population-function)
    * [column filter](#column-filter)
    * [row filter](#row-filter)
  * [column definition](#column-definition)
    * [column evaluation function](#column-evaluation-function)
    * [function window](#function-window)
    * [training a model](#training-a-model)

* [examples and analysis templates](#examples-and-analysis-templates)
  * [example 1: input and output](#example-1-input-and-output)
  * [example 2: record-based features](#example-2-record-based-features)
  * [example 3: user-defined record-based features](#example-3-user-defined-record-based-features)
  * [example 4: table-based features](#example-4-table-based-features)
  * [example 5: window-based rolling aggregation](#example-5-window-based-rolling-aggregation)
  * [example 6: training a model](#example-6-training-a-model)
  * [example 7: reading and writing models](#example-7-reading-and-writing-models)
  * [example 8: joining input tables](#example-8-joining-input-tables)
  * [example 9: train and apply](#example-9-train-and-apply)

* [how to install](#how-to-install)
  * [install from source code](#install-from-source-code)
  * [install from package](#install-from-package)

* [how to test](#how-to-test)

* [how to use](#how-to-use)

# why lambdo?

## why feature engineering?

in many cases, defining good features is more important than choosing and tuning a machine learning algorithm to be applied to the data. hence, the quality of the data analysis result depends more on the quality of the generated features than on the machine learning model.

such high importance of having good features is explained by the following factors:

*	it is a quite rare situation when you have enough data and even if you have it then it then probably you do not have enough computing resources to process it. in this situation, manually defined or automatically mined features compensate this lack of data or computing resources to process it. essentially, we combine expert systems with data mining.

*	feature engineering is a mechanism of creating new levels of abstraction in knowledge representation because each (non-trivial) feature extract and makes explicit some piece of knowledge hidden in the data. it is almost precisely what deep learning is intended for. in this sense, feature engineering does what hidden layers of a neural network do or what the convolutional layer of a neural network does.

## uniting feature engineering with data mining

let us assume that we want to compute moving average for a stock price. for each record, we compute an average value of this and some previous prices. this operation is interpreted as a transformation which generates a new feature stored as a column. its result is defined by one parameter: window size (the number of days to be averaged including this day), and this number is essentially our transformation model.

now let us assume that we want to add a second column which stores prices for tomorrow predicted using some algorithm from this and some previous values. we could develop a rather simple model which extrapolates price using previous values. this forecasting model, when applied, will also generate a new column with some values. its result could depend on how many previous values are used by the extrapolation algorithm and this number is essentially our forecasting model.

an important observation here is that there is no difference between generating a new feature using some transformation model and generating a prediction using some forecast model. technically, these are simply some transformations using some parameters, and these parameters are referred to as a model. although there exist some exceptions where this analogy does not work, lambdo assumes that it is so and follows the principle that

> both generating a feature and applying a machine learning algorithm are data transformations parameterized by their specific models

lambdo simply does not distinguish between them by assuming that any transformation that needs to be done is described by its (python) function name and a (python) model object. lambdo will execute this function but it is unaware of its purpose. it can be a procedure for extracting dates from a string or it can be a prediction using a deep neural network model. in all these cases, the function will add new (derived) column to the existing table.

## any transformation model has an automatic training procedure

one difference between feature generation and machine learning is that machine learning models cannot exist without an algorithm for their training - the whole idea of machine learning is that models are learned automatically from data rather than defined manually. on the hand, features can be defined either manually or learned from data. since lambdo is intended for unifying feature engineering and machine learning, it makes the following assumption:

> any transformation has an accompanying function for generating its models

this means that first we define some transformation which is supposed to be applied to the data and produce a new feature or analysis result. however, the model for this transformation can be generated automatically (if necessary) before applying it. for example, even for computing moving averages an important question is what window size to choose, and instead of guessing we can delegate this question to the training procedure (which could use auto-regression to choose the best window size). importantly, both procedures ‚Äì applying a transformation and training its model ‚Äì are part of the same workflow where they can be intermediate nodes and not only the final predicting node.

## columns first: column-orientation as a basis for feature engineering

assume that we compute a moving average for a time series. the result of this operation is a new column. now assume that we apply a clustering algorithm to records from a data set. in this case the result is again a new column. in fact, generating new columns is opposed to generating a new table and we can see these two operations in many data processing and data analysis approaches. formally, we distinguish between set operations and operations with functions. lambdo uses the following principle:

> lambdo workflow consists of a graph of table definitions and each tables consists of a number of column definitions

a table definition describes how some set is populated using data in already existing tables. a typical table definition is a read/write operation or resampling time series or pivoting. a column definition describes how new values are evaluated using data in other columns in this and other tables.

this approach relies on the principles of the concept-oriented data model which also underlies [bistro](https://github.com/asavinov/bistro) ‚Äì an open source system for batch and stream data processing.

## time series first: time series analysis and forecasting made easy

most existing machine learning algorithms are not time-aware and they cannot be directly applied to time series. although lambdo is a general purpose workflow engine which can be applied to any data, its functionality was developed with time series analysis and forecasting in mind. therefore, it includes many mechanisms which make feature engineering much easier when working with time series:

*	easily defining a family of columns which are features with only minor changes in their definitions. a typical but wide spread example is a family of features which use different window sizes.

*	predefined column definitions for typical goal functions to be predicted or used for training intermediate features. note that time series analysis is almost always supervised learning but there are different formulations for what we want to forecast.

*	lambdo is window-aware workflow engine and for any transformation it is necessary to define its window which is the number of rows the function will be applied to. this parameter is essentially the length of the history (number of previous records to be processed by the function).

*	lambdo is going to be also object-aware which means it can partition the whole data set according to the value of the selected column interpreted as an object. this allows us to analyze data coming from multiple objects like devices, sensors, stock symbols etc.

*	easy control of when to train which nodes. the problem here is that frequently a workflow has to be re-trained periodically but we do not want to re-train all nodes. this mechanism allows us to specify criteria for re-training its models.

# getting started with lambdo

lambdo implements a novel *column-oriented* approach to data analysis by unifying various kinds of data processing: feature engineering, data transformations and data mining.

## workflow definition

### workflow structure

lambdo is intended for processing data by using two types of transformations:

* table transformations produce a new table given one or more input tables.
* column transformations produce a new column in the table given one or more input columns.

a workflow is a number of table definitions each having a number of column definitions. these tables and columns compose a graph where edges are dependencies. if an element (table or column) in this graph has another element as its inputs then this dependency is an edge. if an element (table or column) does not have inputs then it is a data source. if an element does not have dependents then it is a data sink.

this data processing logic of lambdo is represented in json format and stored in a workflow file and having the following structure: 

```javascript
{
  ""tables"": [
    ""table"": { ""function"": ""my_table_func_1"", ""columns"": [...] }
    ""table"": {
      ""function"": ""my_table_func_2"",
      ""columns"": {
        ""column"": { ""function"": ""my_column_func_1"", ... }
        ""column"": { ""function"": ""my_column_func_2"", ... }
      }
    ""table"": { ""function"": ""my_table_func_3"", ""columns"": [...] }
  ]
}
```

each table and column definition has to specify a (python) function name which will actually do data processing. table definition will use functions for data population. column definitions will use functions for evaluating new columns. when lambo executes a workflow, it populates tables according to their definitions and evaluates columns (within tables) according to their definitions. here it is important to understand that tables are used for set operations while columns are used for operations with mathematical functions.

### imports

data processing in lambdo relies on python functions which do real data processing. before these functions can be executed, they have to be imported. the location of functions to be imported is specified in a special field. for example, if the functions we want to use for data processing are in the `examples.example1` module and `datetime` module then we specify them as follows:

```json
{
  ""id"": ""example 1"",
  ""imports"": [""examples.example1"", ""datetime""],
}
```

now we can use functions from these modules in the workflow table and column definitions.

## table definition

### table population function

a table definition has to provide some python function which will *populate* this table. this function can be standard (built-in) python function or it could be part of an imported module like `scale` function from the `sklearn.preprocessing` module or `baselibsvm.predict` function from the `sklearn.svm.base` module. functions can be also defined for this specific workflow if they encode some domain-specific feature definition.

for example, if we want to read data then such a table could be defined as follows:

```json
{
  ""id"": ""my table"",
  ""function"": ""pandas:read_csv"",
  ""inputs"": [],
  ""model"": {
    ""filepath_or_buffer"": ""my_file.csv"",
    ""nrows"": 100
  }
}
```

here we used a standard function from `pandas` but it could be any other function which returns a `dataframe`.

any function takes parameters which are referred to as a *model* and passed to the function. in the above example, we passed input file name and maximum umber of records to be read.

### column filter

frequently, it is necessary to generate some intermediate features (columns) which are not needed for the final analysis. such features should be removed from the table. this can be done by specifying a *column filter* and this selection of necessary columns is performed always when all features within this table have been generated.

we can specify a list of columns, which have to be selected and passed to the next nodes in the graph:

```json
{
  ""id"": ""my table"",
  ""function"": ""pandas:read_csv"",
  ""column_filter"": [""open"", ""close""]
},
```

alternatively, we can specify columns, which have to be excluded from the selected features to be passed to the next nodes:

```json
{
  ""id"": ""my table"",
  ""function"": ""pandas:read_csv"",
  ""column_filter"": {""exclude"": [""date""]}
},
```

the next table will then receive a table with all columns generated in this table excepting the `date` column (which contains time stamps not needed for analysis).

### row filter

not all records in the table need to be analyzed and such records can be excluded before the table is passed to the next node for processing. records to be removed are specified in the row filter which provides several methods for removal.

many analysis algorithms cannot deal with `nan` values and the simplest way to solve this problem is to remove all records which have at least one `nan`:

```json
{
  ""id"": ""my table"",
  ""function"": ""pandas:read_csv"",
  ""row_filter"": {""dropna"": true}
},
```

the `dropna` can also specify a list of columns and then only the values of these columns will be checked.

another way to filter rows is to specify columns with boolean values and then the result table will retain only rows with `true` in these columns:

```json
{
  ""id"": ""my table"",
  ""function"": ""pandas:read_csv"",
  ""row_filter"": {""predicate"": [""selection""]}
},
```

a column with binary values can be defined precisely as any other derived column using a function, which knows which records are needed for analysis. (this column can be then removed by using a column filter.)

it is also possible to reandomly shuffle records by specifying the portion we want to keep in the table. this filter will keep only 80% of randomly selected records:

```json
{
  ""id"": ""my table"",
  ""function"": ""pandas:read_csv"",
  ""row_filter"": {""sample"": {""frac"": 0.8}
},
```

you can specify `""sample"":true` if all records have to be shuffled.

the records can be also selected by specifying their integer position: start, end (exclusive) and step. the following filter will select every second record:

```json
{
  ""id"": ""my table"",
  ""function"": ""pandas:read_csv"",
  ""row_filter"": {""slice"": {""start"": 0, ""step"": 2}
},
```

## column definition

### column evaluation function

a column definition specifies how its values are computed from the values stored in other columns. the way these values are computed is implemented by some python function which can be either a standard python function, a function from some existing module or a user-defined function. lambdo simply gets the name of this function from the workflow and then calls it to generate this column values.

a function is specified as a pair of its module and function name separated by a colon:

```javascript
""function"": ""my_module:my_function""
```

it is assumed that the first argument of the function is data to be processed and the second argument is a model which parameterizes this transformation. note however that some function can take other parameters and also the type of these arguments can vary.

### function window

what data a transformation function receives in its first argument? there are different options starting from a single value and ending with a whole input table. this is determined by the column definition parameter called `window` which takes the following values:

* window `one` or `1` means that lambdo will apply this function to every row of the input table and the function is expected to return a single value stored as the column value for this record. type of data passed to the function depends on how many columns the `input` has.
  * if `input` has only one column then the function will receive a single value.
  * if `input` has more than 1 columns then the function will receive a `series` object with their field values.
* window `all` means that the function will be applied to all rows of the table, that is, there will be one call and the whole table will be passed as a parameter. type of the argument is `dataframe`.
* otherwise the system assume that the function has to be applied to all subsets of the table rows, called windows. size of the window (number of records in one group) is window value. for example, `window: 5` means that each window will consists of 5 records. type of this group depends on the number of columns in the `input`: 
  * if `input` has only one column then the function will receive a `series` of values.
  * if `input` has more than 1 columns then the function will receive a `dataframe` object with the records from the group.

### training a model

a new feature is treated as a transformation, which results in a new column with the values derived from the data in other columns. this transformation is performed using some *model*, which is simply a set of parameters. a model can be specified explicitly by-value if we know these parameters. however, model parameters can be derived from the data using a separate procedure, called *training*. the transformation is then applied *after* the training.

how a model is trained is specified in a block within a column definition:

```json
{
  ""id"": ""prediction"",
  ""function"": ""examples.example1:gb_predict"",
  ""window"": ""all"",
  ""inputs"": {""exclude"": [""labels""]},
  ""train"": {
    ""function"": ""examples.example1:gb_fit"",
    ""model"": {""n_estimators"": 500, ""max_depth"": 4, ""min_samples_split"": 2, ""learning_rate"": 0.01, ""loss"": ""ls""},
    ""outputs"": [""labels""]
  }
}
```

here we need to specify a function which will perform such a training: `""function"": ""examples.example1:gb_fit""`. the training function also needs its own hyper-parameters, for example: `""model"": {""max_depth"": 4}`. finally, the training procedure (in the case of supervised learning) needs labels: `""outputs"": [""labels""]`. note also that excluded the `labels` from the input so that they are not used as features for training.

lambdo will first train a model by using the input data and then use this model for prediction.

# examples

## example 1: input and output

assume that the data is stored in a csv file and we want to use this data to produce new features or for data analysis. loading data is a table population operation which is defined in some table node of the workflow. how the table is populated depends on the `function` of this definition. in our example, we want to re-use a standard `pandas` for loading csv files. such a table node is defined as follows:

```json
{
  ""id"": ""source table"",
  ""function"": ""pandas:read_csv"",
  ""inputs"": [],
  ""model"": {
    ""filepath_or_buffer"": ""my_file.csv"",
    ""nrows"": 100
  }
}
```

after executing this node, it will store the data from this file. we could also use any other function for loading or generating data. for example, it could a function which produces random data.

data output can also be performed by using a standard `pandas` function:

```json
{
  ""id"": ""source table"",
  ""function"": ""pandas:dataframe.to_csv"",
  ""inputs"": ""source table"",
  ""model"": {
    ""path_or_buf"": ""my_output.csv"",
    ""index"": false
  }
}
```

note that the `inputs` fields points to the table which needs to be processed. the result of its execution will be a new csv file.

run this example from command line by executing:

```console
$ lambdo examples/example1.json
```

another useful standard function for storing a table is `to_json` with a possible model like `{""path_or_buf"": ""my_file.json.gz"", ""orient""=""records"", ""lines""=true, ""compression""=""gzip""}` (the file will be compressed). to read a json file into a table, use the function `read_json`.

## example 2: record-based features

the table definition where we load data has no column definitions. however, we can easily add them. a typical use case is where we want to change the format or data type of some columns. for example, if the source file has a text field with a time stamp then we might want to convert it the `datetime` object which is done by defining a new column:

```json
{
    ""id"": ""source table"",
    ""function"": ""pandas:read_csv"",
    ""inputs"": [],
    ""model"": {
        ""filepath_or_buffer"": ""my_file.csv""
    },
    ""columns"": [
      {
          ""id"": ""datetime"",
          ""function"": ""pandas.core.tools.datetimes:to_datetime"",
          ""window"": ""one"",
          ""inputs"": ""date""
      }
    ]
}
```

the most important parameter in this column definition is `window`. if it is `one` (or `1`) then the function will be applied to each row of the table. in other words, this function will get *one* row as its first argument. after evaluating this column definition, the table will get a new column `datetime` storing time stamp objects (which are more convenient for further data transformations). 

if we do not need the source (string) column then the new column may get the same name `""id"": ""date""` and it will overwrite the already existing column. also, if the source column has a non-standard format then it can be specified in the model `""model"": {""format"": ""%y-%m-%d""}` which will be passed to the function.

```json
{
    ""id"": ""datetime"",
    ""function"": ""pandas.core.tools.datetimes:to_datetime"",
    ""window"": ""one"",
    ""inputs"": ""date"",
    ""model"": {""format"": ""%y-%m-%d""}
}
```

if some source or intermediate columns are not needed for later analysis then they can be excluded by adding a column filter to the table definition where we can specify columns to retain as a list like `""column_filter"": [""open"", ""high"", ""low"", ""close"", ""volume""]` or to exclude like `""column_filter"": {""exclude"": [""adj close""]}`.

execute this workflow as follows:

```console
$ lambdo examples/example2.json
```

## example 3: user-defined record-based features

let us now assume that we want to analyze the difference between high and low daily prices and hence we need to derive such a column from two input columns `high` and `low`. there is no such a standard function and hence we need to define our own domain-specific function which will return the derived value given some input values. this user-defined function is defined in a python source file:

```python
def diff_fn(x):
    """"""
    difference between first and second fields of the input series.
    """"""
    if len(x) < 2: return none
    if not x[0] or not x[1]: return none
    if pd.isna(x[0]) or pd.isna(x[1]): return none
    return x[0] - x[1]
```

this function will get a series object for each row of the input table. for each pair of numbers it will return their difference.

in order for the workflow to load this function definition, we need to specify its location in the workflow:

```json
{
  ""id"": ""example 3"",
  ""imports"": [""examples.example3""],
}
```

the column definition, which uses this function is defined as follows:

```json
{
  ""id"": ""diff_high_low"",
  ""function"": ""examples.example3:diff_fn"",
  ""inputs"": [""high"", ""low""]
}
```

we specified two columns which have to be passed as parameters to the user-defined functions: `""inputs"": [""high"", ""low""]`. the same function could be also applied to other column where we want to find difference. this function will be called for each row of the table and its return values will be stored in the new column.

each function including this one can accept additional arguments via its `model` (similar to how we passed data format in the previous example).

## example 4: table-based features

a record-based function with window 1 will be applied to each row of the table and get this row fields in arguments. there will be as many calls as there are rows in the table. if `window` is equal to `all` then the function will be called only one time and it will get all rows it has to process. earlier, we described how string dates can be converted to datetime object by applying the transformation function to each row. the same result can be obtained if we pass the whole column to the transformation function. the only field that has to be changed in this definition is `window`, which is now equals `all`:

```json
{
  ""id"": ""datetime"",
  ""function"": ""pandas.core.tools.datetimes:to_datetime"",
  ""window"": ""all"",
  ""inputs"": ""date"",
  ""model"": {""format"": ""%y-%m-%d""}
}
```

the result will be the same but this column will be evaluated faster.

such functions which get all rows have to know how to iterate over the rows and they return one column rather than a single value. such functions can apply any kind of computations because they have the whole data set. therefore, such functions are used for more complex transformations including forecasts using some model.

another example of applying a function to all rows is shifting a column. for example, if our goal is forecasting the closing price tomorrow then we need shift this column one step backwards:

```json
{
  ""id"": ""close_tomorrow"",
  ""function"": ""pandas.core.series:series.shift"",
  ""window"": ""all"",
  ""inputs"": [""close""],
  ""model"": {""periods"": -1}
}
```

values of this new column will be equal to the value of the specified input column taken from the next record.

## example 5: window-based rolling aggregation

lambdo is focused on time series analysis where important pieces of behavior (features) are hidden in sequences of events. therefore, one of the main goals of feature engineering is making such features explicitly as attribute values by extracting data from the history. normally it is done by using rolling aggregation where a function is applied to some historic window of recent events and returns one value, which characterizes the behavior. in lambdo, it is possible to specify an arbitrary (python) function, which encodes some domain-specific logic specific for this feature.

for window-based columns, the most important parameter is `window` which is an integer specifying the number of events to be passed to the function as its first argument. for example, if `""window"": 10` then the python function will always get 10 elements of the time series: this element and 9 previous elements. it can be a series of 10 values or a sub-table with 10 rows depending on other parameters. the function then analyzes these 10 events and returns one single value, which is stored as a value of this derived column.

assume that we want to find running average volume for 10 days. this can be done as follows:

```json
{
  ""id"": ""mean_volume"",
  ""function"": ""numpy.core.fromnumeric:mean"",
  ""window"": 10,
  ""inputs"": [""volume""]
}
```

each value of the derived column `mean_volume` will store average volume for the last 10 days. 

note that instead of `mean` we could use arbitrary python function including user-defined functions. such a function will be called for each row in the table and it will get 10 values of the volume for the last 10 days (including this one). for example, we could write a function which counts the number of peaks (local maximums) in volume or we could find some more complex pattern. also, if `inputs` has more columns then the functions will get a data frame as input with the columns specified in `inputs`.

typically in time series analysis we use several running aggregations with different window sizes. such columns can can be defined independently but their definitions will differ only in one parameter: `window`. in order to simplify such definitions lambdo allows for defining a base definition and extensions. for example, if we want to define average volumes with windows 10, 5 and 2 then this can be done by defining windows in the extensions:

```json
{
  ""id"": ""mean_volume"",
  ""function"": ""numpy.core.fromnumeric:mean"",
  ""inputs"": [""volume""],
  ""extensions"": [
    {""window"": ""10""},
    {""window"": ""5""},
    {""window"": ""2""}
  ]
}
```

the number of columns defined is equal to the number of extensions, that is, three columns in this examples. the names of the columns by default will be `id` of this family definition and the suffix `_n` where `n` is an index of the extension. in our example, three columns will be added after evaluating this definition: `mean_volume_0`, `mean_volume_1` and `mean_volume_2`.

moving averages can produce empty values, which we want to exclude from analysis, for example, because other analysis algorithms are not able to process them. each table definition allows for filtering its records at the end before the table data is passed to the next node. in order to exclude all rows with empty values we add this block to the end of the table definition:

```json
""row_filter"": {""dropna"": true}
```

run this example and check out its result which will contain three new columns with moving averages of the volume:

```console
$ lambdo examples/example5.json
```

## example 6: training a model

all previous examples assumed that a column definition is treated as a data transformation performed via a python function which also takes parameters of this transformation, which is called a model. the model describes how specifically the transformation has to be performed. one of the main features of lambdo is that it treats such transformations as applying a data mining model. in other words, the result of applying a data mining model is a new column. for example, this column could store the cluster number this row belongs to or likelihood this object (row) is some object. what is specific to data mining is that its models are not specified explicitly but rather are trained from the data. this possibility to train a model (as opposed to providing an explicit model) is provided by lambdo for any kind of column definition. if a model is absent and the training function is provided, then the model will be trained before it is applied to the data.

how a model has to be train is specified in a workflow using the `train` block of a column definition:

```json
""columns"": [
  {
    ""id"": ""my column"",
    ""function"": ""my_transform_function"",

    ""train"": {
      ""function"": ""my_train_function"",
      ""model"": {""hyper_param"": 123}
    }
  }
]
```

this column definition does not have a model specified but it does specify a function for generating (training) such a model. it also provides a hyper-model for this training function which specifies how to do training. the training function gets the data and the hyper-model in its arguments and returns a trained model which is then used for generating the column data.

here is an example of a column definition which trains and applies a gradient boosting data mining model:

```json
{
  ""id"": ""close_tomorrow_predict"",
  ""function"": ""examples.example6:gb_predict"",
  ""window"": ""all"",
  ""data_type"": ""ndarray"",
  ""inputs"": {""exclude"": [""date"", ""close_tomorrow""]},
  ""train"": {
    ""function"": ""examples.example6:gb_fit"",
    ""row_filter"": {""slice"": {""end"": 900}},
    ""model"": {""n_estimators"": 500, ""max_depth"": 4, ""min_samples_split"": 2, ""learning_rate"": 0.01, ""loss"": ""ls""},
    ""outputs"": [""close_tomorrow""]
  }
}
```

this definition has the following new elements:

* `data_type` field indicates that the functions accepts `ndarray` and not `dataframe`.
* `inputs` field allows us to select columns we want to use. we want to exclude the `date` column because its data type is not supported and `close_tomorrow` column which is a goal
* `row_filter` is used to limit the number of records for training
* `model` in the training section provides hyper-parameters for the training function
* `outputs` field specifies labels for the training procedure

thus in this definition we want to use 900 records and all columns except for `date` for training by using `close_tomorrow` as labels. the resulted model is then applied to *all* the data and the predictions are stored as the `close_tomorrow_predict` column.

run this example and check out its results in the last column with predictions:

```console
$ lambdo examples/example6.json
```

## example 7: reading and writing models

if a column (feature definition) model is not specified then lambdo will try to generate it by using the train function. this trained model will be then applied to the input data. however, after finishing executing the workflow, this model will be lost. in many scenarios we would like to retain some trained models. in particular, it is necessary if we explicitly separate two phases: training and predicting. the goal of the training phase is to generate a prediction model by using some (typically large amount of) input data. the model resulted from the training phase can be then used for prediction (by this same workflow because we want to have the same features). therefore, we do not want to train it again but rather load it from the location where it was stored during training.

the mechanism of storing and loading models is implemented by lambdo as one of its main features. the idea is that workflow field values can specified either by-value or by-reference. specifying a value by-value means that we simply provide a json object for the corresponding json field. however, we can also point to values by providing a reference which will be used by the system for reading or writing it.

the general rule is that if a json field value is a string which starts from `$` sign then it is a reference. if we want to store some model in a file (and not directly in the workflow) then the location is specified as follows:

```json
""model"": ""$file:my_model.pkl""
```

now lambdo will try to load this model from the file. if it succeeds then the model will be used for transformation (no training needed). if it fails, for example, the file does not exist, then lambdo will generate this model by using the training function, store the model in the file and then use it for generating the column as usual.

example 7 has one small modification with respect to example 6: its trained model is stored in a file. as a result, we can apply this workflow to a large data set for training, and then this same workflow with the present model can be applied to smaller data sets for prediction.

## example 8: joining input tables

frequently it is necessary to load data from many different data sources and merge them into one table the columns of which can be then used for generating features and analysis. lambdo provides a standard table function `lambdo.std:join`  which populates a new table by joining data from a list of input tables. for example, assume that we want to analyze daily quotes for some symbol but in addition we want to load another quote data for the same days. the two input tables are specified in the `input` field. the first table `gspc` in this list is treated as a main table while the second table `vix` is attached to it:

```json
{
  ""id"": ""merged table"",
  ""function"": ""lambdo.std:join"",
  ""inputs"": [""gspc"", ""vix""],
  ""model"": {""suffixes"": ["""", ""_vix""]},
}
```

this new table will contain as many records as the first table contains but in addition to its columns it will have also columns from the second table. the model of the join function allows for specifying suffixes for columns.

the `join` function by default join using the row numbers. if it is necessary to join by using columns then they can be specified in the `keys` field of the table model as a list of column names.

example 8 demonstrate how to load quotes from two different files and then predict closing price of one symbol taking into account the data for the second symbol.

## example 9: train and apply

in the previous examples, we trained models with the only purpose: generate new features. therefore, the new (feature) models were applied to the *same* data that was used for training. it is a scenario of pure feature engineering where the main result is a new data set, which is supposed to be analyzed by some other data mining algorithm within some other framework (including a separate lambdo workflow).

in this example, we show how we can generate features and also train a final data mining model, which is applied to previously unseen data. this workflow combines the steps for generating new features (possibly by training feature models), training a final data mining model, and applying this model to some portion of data which has not been used for training (but which was transformed using the same features). this workflow can be used for validating various scenarios or for tuning various parameters of the workflow.

the general goal is to predict the price. but simply using future (numeric) prices is a somewhat naive approach. we implement a more realistic scenario where the goal is to determine whether the price will exceed a threshold during some future interval (both the threshold and the length of the interval are parameters). for example, we might be interested to determine whether the price will be 2% higher (at least once) during next 10 days. such a derived goal column has to store 1 if the price exceeds the threshold and 0 otherwise.

deriving such a column is performed by performing the following steps:

* find maximum price for the previous 10 days by applying the standard function `amax` to the column `high` with the window 10.
* shift the column, which essentially means that now it represents the maximum future price.
* find the relative increase of this future maximum price (in percent) by applying a user-defined function `rel_diff_fn`.
* compare the relative increase with the given threshold and return either 1 or 0 by applying a user-defined function `ge_fn`.

after we computed the target column, we need to train a classification model using some portion of the input data set and then apply this model to the whole data set. according to the lambdo conception, it is performed by defining a new column, which will store the result of classification but the model can be trained using the input data:

```json
{
  ""id"": ""high_growth_lr"",
  ""function"": ""examples.example9:c_predict"",
  ""window"": ""all"",
  ""inputs"": {""exclude"": [""max_price_future"", ""high_growth""]},
  ""model"": ""$file:example9_model_lr.pkl"",  // read and use model from this file
  ""train"": {  // if file with model is not available then train the model

    ""row_filter"": {""slice"": {""end"": 6000}},  // use only part of the data set for training

    ""function"": ""examples.example9:lr_fit"",
    ""model"": {},

    ""outputs"": [""high_growth""]  // goal variable for training (labels)
  }
},
```

this column definition will try to load the model (of classification) from the specified file. if this file is present then no training will be done. if the file is not found then the model will be trained and the result will be stored in the file as well as used for classification. note that the model is trained on only a subset of all input data because we defined a `row_filter`.

it is important to note that the training procedure uses the same features which will be used during forecast, that is, the previous parts of the workflow are reused. another interesting option is that we actually can define several classification algorithms simultaneously the results of which can be then compared or even used as normal derived feature for further analysis.

# how to install

## install from source code

check out the source code and execute this command in the project directory (where `setup.py` is located):

```console
$ pip install .
```

or alternatively:

```console
$ python setup.py install
```

## install from package

create wheel package:

```console
$ python setup.py bdist_wheel
```

the `whl` package will be stored in the `dist` folder and can then be installed using `pip`.

execute this command by specifying the `whl` file as a parameter:

```console
$ pip install dist\lambdo-0.4.0-py3-none-any.whl
```

# how to test

run tests:

```console
$ python -m unittest discover -s ./tests
```

or

```console
$ python setup.py test
```

# how to use

if you execute `lambdo` without any options then it will return a short help by describing its usage.

a workflow file is needed to analyze data. very simple workflows for test purposes can be found in the `tests` directory. more complex workflows can be found in the `examples` directory. to execute a workflow start `lambdo` with this workflow file name as a parameter:

```console
$ lambdo examples/example1.json
```

the workflow reads a csv file, computes some features from the time series data, trains a model by applying it them to the data and finally writes the result to an output csv file.

lambdo can be used from within another python program:

```python
from lambdo.workflow import *
wf_json = {...}  # some workflow
wf = workflow(wf_json)  # create lambdo object
wf.execute()  # execute workflow
```
"
"dowel","[![build status](https://travis-ci.com/rlworkgroup/dowel.svg?branch=master)](https://travis-ci.com/rlworkgroup/dowel)
[![codecov](https://codecov.io/gh/rlworkgroup/dowel/branch/master/graph/badge.svg)](https://codecov.io/gh/rlworkgroup/dowel)
[![docs](https://readthedocs.org/projects/dowel/badge)](http://dowel.readthedocs.org/en/latest/)
[![license](https://img.shields.io/badge/license-mit-blue.svg)](https://github.com/rlworkgroup/dowel/blob/master/license)
[![pypi version](https://badge.fury.io/py/dowel.svg)](https://badge.fury.io/py/dowel)

# dowel

dowel is a little logger for machine learning research.

## installation
```shell
pip install dowel
```

## usage
```python
import dowel
from dowel import logger, tabular

logger.add_output(dowel.stdoutput())
logger.add_output(dowel.tensorboardoutput('tensorboard_logdir'))

logger.log('starting up...')
for i in range(1000):
    logger.push_prefix('itr {}'.format(i))
    logger.log('running training step')

    tabular.record('itr', i)
    tabular.record('loss', 100.0 / (2 + i))
    logger.log(tabular)

    logger.pop_prefix()
    logger.dump_all()

logger.remove_all()
```
"
"CAEs for Data Assimilation","# vardacae
this module is used to create convolutional autoencoders for variational data assimilation. a  user can define, create and train an ae for data assimilation with just a few lines of code. it is the accompanying code to the paper [here](https://arxiv.org/abs/2101.02121), published in _computer methods in applied mechanics and engineering_.

## introduction

data assimilation (da) is an uncertainty quantification technique used to reduce the error in  predictions by combining forecasting data with observation of the state. the most common techniques for da are variational approaches and kalman filters.

in this work, we propose a method of using autoencoders to model the background error covariance matrix, to greatly reduce the computational cost of solving 3d variational da **while increasing the quality of the data assimilation**.

## data
the data used in this paper is owned by the data science institute, imperial college, london. if you do not have access to this data, please see the section below on training a model with your own data.
## installation

1. install `vtk` by navigating to [this link](https://vtk.org/download/) and installing the version applicable to your system.

2. navigate to the base directory and run:

   ```bash
   pip install -e .
   ```

3. run `pytest` from the home directory to ensure correct installation.

## tests
from the project home directory run `pytest`.

## getting started
to train and evaluate a [tucodec](http://openaccess.thecvf.com/content_cvprw_2019/papers/clic%202019/zhou_end-to-end_optimized_image_compression_with_attention_mechanism_cvprw_2019_paper.pdf ""tucodec clic-2019 paper"") model on fluidity data:

```python
from vardacae import trainae, batchda
from vardacae.settings.models.clic import clic

model_kwargs = {""model_name"": ""tucodec"", ""block_type"": ""next"", ""cstd"": 64}

settings = clic(**model_kwargs)    # settings describing experimental setup
expdir = ""experiments/expt1/""      # dir to save results data and models

trainer = trainae(settings, expdir, batch_sz=16)
model = trainer.train(num_epochs=150)   # this will take approximately 8 hrs on a k80

# evaluate da on the test set:
results_df = batchda(settings, aemodel=model).run()

```
## settings instance
the api is based around a monolithic ```settings``` object that is used to define all configuration parameters, from the model definition to the seed. this single point of truth is used so that, an experiment can be repeated _exactly_ by simply loading a pickled  ```settings``` object. all key classes like ```trainae``` and ```batchda``` require a ```settings``` object at initialisation.

## train a model on your *own* data

to train a model on your own 3d data you must do the following:
* override the default ```get_x(...)``` method in the ```getdata``` loader class:

```python
from vardacae import getdata

class newloaderclass(getdata):
    def get_x(self, settings):
        ""arguments:
               settings: (a settings.config class)
        returns:
            np.array of dimensions b x nx x ny x nz ""

        # ... calculate / load or download x
        # for an example see vardacae.data.load.getdata.get_x""""""
        return x
```

* create a new settings class that inherits from your desired model's settings class (e.g. `vardacae.settings.models.clic.clic`) and update the data dimensions:

```python
from vardacae.settings.models.clic import clic

class newconfig(clic):
    def __init__(self, clic_kwargs, opt_kwargs):
        super(clic, self).__init__(**clic_kwargs)
        self.n3d = (100, 200, 300)  # define input domain size
                                    # this is used by convscheduler
        self.x_fp = ""set_if_req_by_get_x""
        # ... use opt_kwargs as desired

clic_kwargs =  {""model_name"": ""tucodec"", ""block_type"": ""next"",
                ""cstd"": 64, ""loader"": newloaderclass}
                # note: do not initialize newloaderclass

settings = newconfig(clic_kwargs, opt_kwargs)

```

this ```settings``` object can now be used to train a model with the `trainae` method as shown above.
"
"data-science-ipython-notebooks","<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/readme_1200x800.gif"">
</p>

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/coversmall_alt.png"">
  <br/>
</p>

# data-science-ipython-notebooks

## index

* [deep-learning](#deep-learning)
    * [tensorflow](#tensor-flow-tutorials)
    * [theano](#theano-tutorials)
    * [keras](#keras-tutorials)
    * [caffe](#deep-learning-misc)
* [scikit-learn](#scikit-learn)
* [statistical-inference-scipy](#statistical-inference-scipy)
* [pandas](#pandas)
* [matplotlib](#matplotlib)
* [numpy](#numpy)
* [python-data](#python-data)
* [kaggle-and-business-analyses](#kaggle-and-business-analyses)
* [spark](#spark)
* [mapreduce-python](#mapreduce-python)
* [amazon web services](#aws)
* [command lines](#commands)
* [misc](#misc)
* [notebook-installation](#notebook-installation)
* [credits](#credits)
* [contributing](#contributing)
* [contact-info](#contact-info)
* [license](#license)

<br/>
<p align=""center"">
  <img src=""http://i.imgur.com/zhkxrkz.png"">
</p>

## deep-learning

ipython notebook(s) demonstrating deep learning functionality.

<br/>
<p align=""center"">
  <img src=""https://avatars0.githubusercontent.com/u/15658638?v=3&s=100"">
</p>

### tensor-flow-tutorials

additional tensorflow tutorials:

* [pkmital/tensorflow_tutorials](https://github.com/pkmital/tensorflow_tutorials)
* [nlintz/tensorflow-tutorials](https://github.com/nlintz/tensorflow-tutorials)
* [alrojo/tensorflow-tutorial](https://github.com/alrojo/tensorflow-tutorial)
* [binroot/tensorflow-book](https://github.com/binroot/tensorflow-book)
* [tuanavu/tensorflow-basic-tutorials](https://github.com/tuanavu/tensorflow-basic-tutorials)

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [tsf-basics](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/1_intro/basic_operations.ipynb) | learn basic operations in tensorflow, a library for various kinds of perceptual and language understanding tasks from google. |
| [tsf-linear](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/linear_regression.ipynb) | implement linear regression in tensorflow. |
| [tsf-logistic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/logistic_regression.ipynb) | implement logistic regression in tensorflow. |
| [tsf-nn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/nearest_neighbor.ipynb) | implement nearest neighboars in tensorflow. |
| [tsf-alex](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/alexnet.ipynb) | implement alexnet in tensorflow. |
| [tsf-cnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/convolutional_network.ipynb) | implement convolutional neural networks in tensorflow. |
| [tsf-mlp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/multilayer_perceptron.ipynb) | implement multilayer perceptrons in tensorflow. |
| [tsf-rnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/recurrent_network.ipynb) | implement recurrent neural networks in tensorflow. |
| [tsf-gpu](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/4_multi_gpu/multigpu_basics.ipynb) | learn about basic multi-gpu computation in tensorflow. |
| [tsf-gviz](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/graph_visualization.ipynb) | learn about graph visualization in tensorflow. |
| [tsf-lviz](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/loss_visualization.ipynb) | learn about loss visualization in tensorflow. |

### tensor-flow-exercises

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [tsf-not-mnist](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/1_notmnist.ipynb) | learn simple data curation by creating a pickle with formatted datasets for training, development and testing in tensorflow. |
| [tsf-fully-connected](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/2_fullyconnected.ipynb) | progressively train deeper and more accurate models using logistic regression and neural networks in tensorflow. |
| [tsf-regularization](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/3_regularization.ipynb) | explore regularization techniques by training fully connected networks to classify notmnist characters in tensorflow. |
| [tsf-convolutions](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/4_convolutions.ipynb) | create convolutional neural networks in tensorflow. |
| [tsf-word2vec](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/5_word2vec.ipynb) | train a skip-gram model over text8 data in tensorflow. |
| [tsf-lstm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/6_lstm.ipynb) | train a lstm character model over text8 data in tensorflow. |

<br/>
<p align=""center"">
  <img src=""http://www.deeplearning.net/software/theano/_static/theano_logo_allblue_200x46.png"">
</p>

### theano-tutorials

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [theano-intro](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/intro_theano/intro_theano.ipynb) | intro to theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. it can use gpus and perform efficient symbolic differentiation. |
| [theano-scan](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/scan_tutorial/scan_tutorial.ipynb) | learn scans, a mechanism to perform loops in a theano graph. |
| [theano-logistic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/intro_theano/logistic_regression.ipynb) | implement logistic regression in theano. |
| [theano-rnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/rnn_tutorial/simple_rnn.ipynb) | implement recurrent neural networks in theano. |
| [theano-mlp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/theano_mlp/theano_mlp.ipynb) | implement multilayer perceptrons in theano. |

<br/>
<p align=""center"">
  <img src=""http://i.imgur.com/l45q8c2.jpg"">
</p>

### keras-tutorials

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| keras | keras is an open source neural network library written in python. it is capable of running on top of either tensorflow or theano. |
| [setup](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/0.%20preamble.ipynb) | learn about the tutorial goals and how to set up your keras environment. |
| [intro-deep-learning-ann](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.1%20introduction%20-%20deep%20learning%20and%20ann.ipynb) | get an intro to deep learning with keras and artificial neural networks (ann). |
| [theano](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.2%20introduction%20-%20theano.ipynb) | learn about theano by working with weights matrices and gradients. |
| [keras-otto](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.3%20introduction%20-%20keras.ipynb) | learn about keras by looking at the kaggle otto challenge. |
| [ann-mnist](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.4%20%28extra%29%20a%20simple%20implementation%20of%20ann%20for%20mnist.ipynb) | review a simple implementation of ann for mnist using keras. |
| [conv-nets](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.1%20supervised%20learning%20-%20convnets.ipynb) | learn about convolutional neural networks (cnns) with keras. |
| [conv-net-1](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.2.1%20supervised%20learning%20-%20convnet%20handson%20part%20i.ipynb) | recognize handwritten digits from mnist using keras - part 1. |
| [conv-net-2](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.2.2%20supervised%20learning%20-%20convnet%20handson%20part%20ii.ipynb) | recognize handwritten digits from mnist using keras - part 2. |
| [keras-models](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.3%20supervised%20learning%20-%20famous%20models%20with%20keras.ipynb) | use pre-trained models such as vgg16, vgg19, resnet50, and inception v3 with keras. |
| [auto-encoders](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.1%20unsupervised%20learning%20-%20autoencoders%20and%20embeddings.ipynb) | learn about autoencoders with keras. |
| [rnn-lstm](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.2%20rnn%20and%20lstm.ipynb) | learn about recurrent neural networks (rnns) with keras. |
| [lstm-sentence-gen](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.3%20%28extra%29%20lstm%20for%20sentence%20generation.ipynb) |  learn about rnns using long short term memory (lstm) networks with keras. |

### deep-learning-misc

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [deep-dream](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/deep-dream/dream.ipynb) | caffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images. |

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scikitlearn.png"">
</p>

## scikit-learn

ipython notebook(s) demonstrating scikit-learn functionality.

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [intro](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-intro.ipynb) | intro notebook to scikit-learn.  scikit-learn adds python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |
| [knn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-intro.ipynb#k-nearest-neighbors-classifier) | implement k-nearest neighbors in scikit-learn. |
| [linear-reg](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-linear-reg.ipynb) | implement linear regression in scikit-learn. |
| [svm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-svm.ipynb) | implement support vector machine classifiers with and without kernels in scikit-learn. |
| [random-forest](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-random-forest.ipynb) | implement random forest classifiers and regressors in scikit-learn. |
| [k-means](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-k-means.ipynb) | implement k-means clustering in scikit-learn. |
| [pca](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-pca.ipynb) | implement principal component analysis in scikit-learn. |
| [gmm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-gmm.ipynb) | implement gaussian mixture models in scikit-learn. |
| [validation](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-validation.ipynb) | implement validation and model selection in scikit-learn. |

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scipy.png"">
</p>

## statistical-inference-scipy

ipython notebook(s) demonstrating statistical inference with scipy functionality.

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| scipy | scipy is a collection of mathematical algorithms and convenience functions built on the numpy extension of python. it adds significant power to the interactive python session by providing the user with high-level commands and classes for manipulating and visualizing data. |
| [effect-size](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/effect_size.ipynb) | explore statistics that quantify effect size by analyzing the difference in height between men and women.  uses data from the behavioral risk factor surveillance system (brfss) to estimate the mean and standard deviation of height for adult women and men in the united states. |
| [sampling](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/sampling.ipynb) | explore random sampling by analyzing the average weight of men and women in the united states using brfss data. |
| [hypothesis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/hypothesis.ipynb) | explore hypothesis testing by analyzing the difference of first-born babies compared with others. |

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/pandas.png"">
</p>

## pandas

ipython notebook(s) demonstrating pandas functionality.

| notebook | description |
|--------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [pandas](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/pandas.ipynb) | software library written for data manipulation and analysis in python. offers data structures and operations for manipulating numerical tables and time series. |
| [github-data-wrangling](https://github.com/donnemartin/viz/blob/master/githubstats/data_wrangling.ipynb) | learn how to load, clean, merge, and feature engineer by analyzing github data from the [`viz`](https://github.com/donnemartin/viz) repo. |
| [introduction-to-pandas](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.00-introduction-to-pandas.ipynb) | introduction to pandas. |
| [introducing-pandas-objects](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.01-introducing-pandas-objects.ipynb) | learn about pandas objects. |
| [data indexing and selection](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.02-data-indexing-and-selection.ipynb) | learn about data indexing and selection in pandas. |
| [operations-in-pandas](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.03-operations-in-pandas.ipynb) | learn about operating on data in pandas. |
| [missing-values](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.04-missing-values.ipynb) | learn about handling missing data in pandas. |
| [hierarchical-indexing](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.05-hierarchical-indexing.ipynb) | learn about hierarchical indexing in pandas. |
| [concat-and-append](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.06-concat-and-append.ipynb) | learn about combining datasets: concat and append in pandas. |
| [merge-and-join](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.07-merge-and-join.ipynb) | learn about combining datasets: merge and join in pandas. |
| [aggregation-and-grouping](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.08-aggregation-and-grouping.ipynb) | learn about aggregation and grouping in pandas. |
| [pivot-tables](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.09-pivot-tables.ipynb) | learn about pivot tables in pandas. |
| [working-with-strings](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.10-working-with-strings.ipynb) | learn about vectorized string operations in pandas. |
| [working-with-time-series](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.11-working-with-time-series.ipynb) | learn about working with time series in pandas. |
| [performance-eval-and-query](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.12-performance-eval-and-query.ipynb) | learn about high-performance pandas: eval() and query() in pandas. |

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/matplotlib.png"">
</p>

## matplotlib

ipython notebook(s) demonstrating matplotlib functionality.

| notebook | description |
|-----------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|
| [matplotlib](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/matplotlib.ipynb) | python 2d plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. |
| [matplotlib-applied](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/matplotlib-applied.ipynb) | apply matplotlib visualizations to kaggle competitions for exploratory data analysis.  learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots. |
| [introduction-to-matplotlib](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.00-introduction-to-matplotlib.ipynb) | introduction to matplotlib. |
| [simple-line-plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.01-simple-line-plots.ipynb) | learn about simple line plots in matplotlib. |
| [simple-scatter-plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.02-simple-scatter-plots.ipynb) | learn about simple scatter plots in matplotlib. |
| [errorbars.ipynb](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.03-errorbars.ipynb) | learn about visualizing errors in matplotlib. |
| [density-and-contour-plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.04-density-and-contour-plots.ipynb) | learn about density and contour plots in matplotlib. |
| [histograms-and-binnings](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.05-histograms-and-binnings.ipynb) | learn about histograms, binnings, and density in matplotlib. |
| [customizing-legends](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.06-customizing-legends.ipynb) | learn about customizing plot legends in matplotlib. |
| [customizing-colorbars](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.07-customizing-colorbars.ipynb) | learn about customizing colorbars in matplotlib. |
| [multiple-subplots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.08-multiple-subplots.ipynb) | learn about multiple subplots in matplotlib. |
| [text-and-annotation](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.09-text-and-annotation.ipynb) | learn about text and annotation in matplotlib. |
| [customizing-ticks](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.10-customizing-ticks.ipynb) | learn about customizing ticks in matplotlib. |
| [settings-and-stylesheets](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.11-settings-and-stylesheets.ipynb) | learn about customizing matplotlib: configurations and stylesheets. |
| [three-dimensional-plotting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.12-three-dimensional-plotting.ipynb) | learn about three-dimensional plotting in matplotlib. |
| [geographic-data-with-basemap](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.13-geographic-data-with-basemap.ipynb) | learn about geographic data with basemap in matplotlib. |
| [visualization-with-seaborn](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.14-visualization-with-seaborn.ipynb) | learn about visualization with seaborn. |

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/numpy.png"">
</p>

## numpy

ipython notebook(s) demonstrating numpy functionality.

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [numpy](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/numpy.ipynb) | adds python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |
| [introduction-to-numpy](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.00-introduction-to-numpy.ipynb) | introduction to numpy. |
| [understanding-data-types](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.01-understanding-data-types.ipynb) | learn about data types in python. |
| [the-basics-of-numpy-arrays](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.02-the-basics-of-numpy-arrays.ipynb) | learn about the basics of numpy arrays. |
| [computation-on-arrays-ufuncs](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.03-computation-on-arrays-ufuncs.ipynb) | learn about computations on numpy arrays: universal functions. |
| [computation-on-arrays-aggregates](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.04-computation-on-arrays-aggregates.ipynb) | learn about aggregations: min, max, and everything in between in numpy. |
| [computation-on-arrays-broadcasting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.05-computation-on-arrays-broadcasting.ipynb) | learn about computation on arrays: broadcasting in numpy. |
| [boolean-arrays-and-masks](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.06-boolean-arrays-and-masks.ipynb) | learn about comparisons, masks, and boolean logic in numpy. |
| [fancy-indexing](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.07-fancy-indexing.ipynb) | learn about fancy indexing in numpy. |
| [sorting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.08-sorting.ipynb) | learn about sorting arrays in numpy. |
| [structured-data-numpy](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.09-structured-data-numpy.ipynb) | learn about structured data: numpy's structured arrays. |

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/python.png"">
</p>

## python-data

ipython notebook(s) demonstrating python functionality geared towards data analysis.

| notebook | description |
|-----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|
| [data structures](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/structs.ipynb) | learn python basics with tuples, lists, dicts, sets. |
| [data structure utilities](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/structs_utils.ipynb) | learn python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions. |
| [functions](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/functions.ipynb) | learn about more advanced python features: functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools. |
| [datetime](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/datetime.ipynb) | learn how to work with python dates and times: datetime, strftime, strptime, timedelta. |
| [logging](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/logs.ipynb) | learn about python logging with rotatingfilehandler and timedrotatingfilehandler. |
| [pdb](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/pdb.ipynb) | learn how to debug in python with the interactive source code debugger. |
| [unit tests](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/unit_tests.ipynb) | learn how to test in python with nose unit tests. |

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/kaggle.png"">
</p>

## kaggle-and-business-analyses

ipython notebook(s) used in [kaggle](https://www.kaggle.com/) competitions and business analyses.

| notebook | description |
|-------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|
| [titanic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb) | predict survival on the titanic.  learn data cleaning, exploratory data analysis, and machine learning. |
| [churn-analysis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/analyses/churn.ipynb) | predict customer churn.  exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  includes discussions of confusion matrices, roc plots, feature importances, prediction probabilities, and calibration/descrimination.|

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/spark.png"">
</p>

## spark

ipython notebook(s) demonstrating spark and hdfs functionality.

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|
| [spark](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/spark/spark.ipynb) | in-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms. |
| [hdfs](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/spark/hdfs.ipynb) | reliably stores very large files across machines in a large cluster. |

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/mrjob.png"">
</p>

## mapreduce-python

ipython notebook(s) demonstrating hadoop mapreduce with mrjob functionality.

| notebook | description |
|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|
| [mapreduce-python](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/mapreduce/mapreduce-python.ipynb) | runs mapreduce jobs in python, executing jobs locally or on hadoop clusters. demonstrates hadoop streaming in python code with unit test and [mrjob](https://github.com/yelp/mrjob) config file to analyze amazon s3 bucket logs on elastic mapreduce.  [disco](https://github.com/discoproject/disco/) is another python-based alternative.|

<br/>

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png"">
</p>

## aws

ipython notebook(s) demonstrating amazon web services (aws) and aws tools functionality.


also check out:

* [saws](https://github.com/donnemartin/saws): a supercharged aws command line interface (cli).
* [awesome aws](https://github.com/donnemartin/awesome-aws): a curated list of libraries, open source repos, guides, blogs, and other resources.

| notebook | description |
|------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [boto](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#boto) | official aws sdk for python. |
| [s3cmd](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3cmd) | interacts with s3 through the command line. |
| [s3distcp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3distcp) | combines smaller files and aggregates them together by taking in a pattern and target file.  s3distcp can also be used to transfer large volumes of data from s3 to your hadoop cluster. |
| [s3-parallel-put](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3-parallel-put) | uploads multiple files to s3 in parallel. |
| [redshift](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#redshift) | acts as a fast data warehouse built on top of technology from massive parallel processing (mpp). |
| [kinesis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#kinesis) | streams data in real time with the ability to process thousands of data streams per second. |
| [lambda](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#lambda) | runs code in response to events, automatically managing compute resources. |

<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/commands.png"">
</p>

## commands

ipython notebook(s) demonstrating various command lines for linux, git, etc.

| notebook | description |
|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [linux](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/linux.ipynb) | unix-like and mostly posix-compliant computer operating system.  disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and vim.|
| [anaconda](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#anaconda) | distribution of the python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment. |
| [ipython notebook](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#ipython-notebook) | web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. |
| [git](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#git) | distributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows. |
| [ruby](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#ruby) | used to interact with the aws command line and for jekyll, a blog framework that can be hosted on github pages. |
| [jekyll](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#jekyll) | simple, blog-aware, static site generator for personal, project, or organization sites.  renders markdown or textile and liquid templates, and produces a complete, static website ready to be served by apache http server, nginx or another web server. |
| [pelican](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#pelican) | python-based alternative to jekyll. |
| [django](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#django) | high-level python web framework that encourages rapid development and clean, pragmatic design. it can be useful to share reports/analyses and for blogging. lighter-weight alternatives include [pyramid](https://github.com/pylons/pyramid), [flask](https://github.com/pallets/flask), [tornado](https://github.com/tornadoweb/tornado), and [bottle](https://github.com/bottlepy/bottle).

## misc

ipython notebook(s) demonstrating miscellaneous functionality.

| notebook | description |
|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [regex](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/misc/regex.ipynb) | regular expression cheat sheet useful in data wrangling.|
[algorithmia](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/misc/algorithmia.ipynb) | algorithmia is a marketplace for algorithms. this notebook showcases 4 different algorithms: face detection, content summarizer, latent dirichlet allocation and optical character recognition.|

## notebook-installation

### anaconda

anaconda is a free distribution of the python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.

follow instructions to install [anaconda](https://docs.continuum.io/anaconda/install) or the more lightweight [miniconda](http://conda.pydata.org/miniconda.html).

### dev-setup

for detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the [dev-setup](https://github.com/donnemartin/dev-setup) repo.

### running-notebooks

to view interactive content or to modify elements within the ipython notebooks, you must first clone or download the repository then run the notebook.  more information on ipython notebooks can be found [here.](http://ipython.org/notebook.html)

    $ git clone https://github.com/donnemartin/data-science-ipython-notebooks.git
    $ cd data-science-ipython-notebooks
    $ jupyter notebook

notebooks tested with python 2.7.x.

## credits

* [python for data analysis: data wrangling with pandas, numpy, and ipython](http://www.amazon.com/python-data-analysis-wrangling-ipython/dp/1449319793) by wes mckinney
* [pycon 2015 scikit-learn tutorial](https://github.com/jakevdp/sklearn_pycon2015) by jake vanderplas
* [python data science handbook](https://github.com/jakevdp/pythondatasciencehandbook) by jake vanderplas
* [parallel machine learning with scikit-learn and ipython](https://github.com/ogrisel/parallel_ml_tutorial) by olivier grisel
* [statistical interference using computational methods in python](https://github.com/allendowney/compstats) by allen downey
* [tensorflow examples](https://github.com/aymericdamien/tensorflow-examples) by aymeric damien
* [tensorflow tutorials](https://github.com/pkmital/tensorflow_tutorials) by parag k mital
* [tensorflow tutorials](https://github.com/nlintz/tensorflow-tutorials) by nathan lintz
* [tensorflow tutorials](https://github.com/alrojo/tensorflow-tutorial) by alexander r johansen
* [tensorflow book](https://github.com/binroot/tensorflow-book) by nishant shukla
* [summer school 2015](https://github.com/mila-udem/summerschool2015) by mila-udem
* [keras tutorials](https://github.com/leriomaggio/deep-learning-keras-tensorflow) by valerio maggio
* [kaggle](https://www.kaggle.com/)
* [yhat blog](http://blog.yhat.com/)

## contributing

contributions are welcome!  for bug reports or requests please [submit an issue](https://github.com/donnemartin/data-science-ipython-notebooks/issues).

## contact-info

feel free to contact me to discuss any issues, questions, or comments.

* email: [donne.martin@gmail.com](mailto:donne.martin@gmail.com)
* twitter: [@donne_martin](https://twitter.com/donne_martin)
* github: [donnemartin](https://github.com/donnemartin)
* linkedin: [donnemartin](https://www.linkedin.com/in/donnemartin)
* website: [donnemartin.com](http://donnemartin.com)

## license

this repository contains a variety of content; some developed by donne martin, and some from third-parties.  the third-party content is distributed under the license provided by those parties.

the content developed by donne martin is distributed under the following license:

*i am providing code and resources in this repository to you under an open source license.  because this is my personal repository, the license you receive to my code and resources is from me and not my employer (facebook).*

    copyright 2015 donne martin

    licensed under the apache license, version 2.0 (the ""license"");
    you may not use this file except in compliance with the license.
    you may obtain a copy of the license at

       http://www.apache.org/licenses/license-2.0

    unless required by applicable law or agreed to in writing, software
    distributed under the license is distributed on an ""as is"" basis,
    without warranties or conditions of any kind, either express or implied.
    see the license for the specific language governing permissions and
    limitations under the license.
"
"Keras Tuner","# kerastuner

[![](https://github.com/keras-team/keras-tuner/workflows/tests/badge.svg?branch=master)](https://github.com/keras-team/keras-tuner/actions?query=workflow%3atests+branch%3amaster)
[![codecov](https://codecov.io/gh/keras-team/keras-tuner/branch/master/graph/badge.svg)](https://codecov.io/gh/keras-team/keras-tuner)
[![pypi version](https://badge.fury.io/py/keras-tuner.svg)](https://badge.fury.io/py/keras-tuner)

kerastuner is an easy-to-use, scalable hyperparameter optimization framework
that solves the pain points of hyperparameter search. easily configure your
search space with a define-by-run syntax, then leverage one of the available
search algorithms to find the best hyperparameter values for your models.
kerastuner comes with bayesian optimization, hyperband, and random search algorithms
built-in, and is also designed to be easy for researchers to extend in order to
experiment with new search algorithms.

official website: [https://keras.io/keras_tuner/](https://keras.io/keras_tuner/)

## quick links

* [getting started with kerastuner](https://keras.io/guides/keras_tuner/getting_started)
* [kerastuner developer guides](https://keras.io/guides/keras_tuner/)
* [kerastuner api reference](https://keras.io/api/keras_tuner/)


## installation

kerastuner requires **python 3.7+** and **tensorflow 2.0+**.

install the latest release:

```
pip install keras-tuner --upgrade
```

you can also check out other versions in our
[github repository](https://github.com/keras-team/keras-tuner).


## quick introduction

import kerastuner and tensorflow:

```python
import keras_tuner
from tensorflow import keras
```

write a function that creates and returns a keras model.
use the `hp` argument to define the hyperparameters during model creation.

```python
def build_model(hp):
  model = keras.sequential()
  model.add(keras.layers.dense(
      hp.choice('units', [8, 16, 32]),
      activation='relu'))
  model.add(keras.layers.dense(1, activation='relu'))
  model.compile(loss='mse')
  return model
```

initialize a tuner (here, `randomsearch`).
we use `objective` to specify the objective to select the best models,
and we use `max_trials` to specify the number of different models to try.

```python
tuner = keras_tuner.randomsearch(
    build_model,
    objective='val_loss',
    max_trials=5)
```

start the search and get the best model:

```python
tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))
best_model = tuner.get_best_models()[0]
```

to learn more about kerastuner, check out [this starter guide](https://keras.io/guides/keras_tuner/getting_started/).

## contributing guide

please refer to the [contributing.md](https://github.com/keras-team/keras-tuner/blob/master/contributing.md) for the contributing guide.

thank all the contributors!

[![the contributors](https://raw.githubusercontent.com/keras-team/keras-tuner/master/docs/contributors.svg)](https://github.com/keras-team/keras-tuner/graphs/contributors)

## community

ask your questions on our [github discussions](https://github.com/keras-team/keras-tuner/discussions).

## citing kerastuner

if kerastuner helps your research, we appreciate your citations.
here is the bibtex entry:

```bibtex
@misc{omalley2019kerastuner,
	title        = {kerastuner},
	author       = {o'malley, tom and bursztein, elie and long, james and chollet, fran\c{c}ois and jin, haifeng and invernizzi, luca and others},
	year         = 2019,
	howpublished = {\url{https://github.com/keras-team/keras-tuner}}
}
```
"
"nn_builder","[![downloads](https://pepy.tech/badge/nn-builder)](https://pepy.tech/project/nn-builder) ![image](https://travis-ci.org/p-christ/nn_builder.svg?branch=master) [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/dwyl/esta/issues)  


![nn_builder](miscellaneous/material_for_readme/nn_builder_new.png)



**nn_builder lets you build neural networks with less boilerplate code**. you specify the type of network you want and it builds it.

### install

`pip install nn_builder`

### support

| network type       | **nn**  | **cnn** | **rnn** |
| ------- | ------- | ------- | ------- |
| pytorch     | :heavy_check_mark: | :heavy_check_mark:    | :heavy_check_mark:    |
| tensorflow 2.0  |        :heavy_check_mark:  |  :heavy_check_mark: | :heavy_check_mark: |                             |


### examples

on the left is how you can create the pytorch neural network on the right in only 1 line of code using nn_builder:

![screenshot](miscellaneous/material_for_readme/nn_builder_use_case.png)

similarly for tensorflow on the left is how you can create the cnn on the right in only 1 line of code using nn_builder: 

![screenshot](miscellaneous/material_for_readme/tf_nn_builder_example.png)

### usage

see this [colab notebook](https://colab.research.google.com/drive/1udmt3avsv0l5rq11nylhxmsvttvzryhw) for lots of examples of how to use the module. 
3 types of pytorch and tensorflow network are currently supported: nn, cnn and rnn. each network takes the following arguments:

| field | description | default |
| :---: | :----------: | :---: |
| *input_dim*| dimension of the input into the network. see below for more detail. not needed for tensorflow.  | n/a |
| *layers_info* | list to indicate the layers of the network you want. exact requirements depend on network type, see below for more detail  | n/a |
| *output_activation* | string to indicate the activation function you want the output to go through. provide a list of strings if you want multiple output heads | no activation |                              
| *hidden_activations* | string or list of string to indicate the activations you want used on the output of hidden layers (not including the output layer), default is relu and for example ""tanh"" would have tanh applied on all hidden layer activations | relu after every hidden layer |
| *dropout* | float to indicate what dropout probability you want applied after each hidden layer | 0 |
| *initialiser* | string to indicate which initialiser you want used to initialise all the parameters | pytorch & tf default |
| *batch_norm* | boolean to indicate whether you want batch norm applied to the output of every hidden layer | false |
| *columns of_data_to_be_embedded* | list to indicate the column numbers of the data that you want to be put through an embedding layer before being fed through the hidden layers of the network | no embeddings |
| *embedding_dimensions* | if you have categorical variables you want embedded before flowing through the network then you specify the embedding dimensions here with a list of the form: [ [embedding_input_dim_1, embedding_output_dim_1], [embedding_input_dim_2, embedding_output_dim_2] ...] | no embeddings |
| *y_range* | tuple of float or integers of the form (y_lower, y_upper) indicating the range you want to restrict the output values to in regression tasks | no range |
| *random_seed* | integer to indicate the random seed you want to use | 0 |
| *return_final_seq_only* | only needed for rnn. boolean to indicate whether you only want to return the output for the final timestep (true) or if you want to return the output for all timesteps (false) | true |

each network type has slightly different requirements for **input_dim** and **layers_info** as explained below:

--- 

### 1. nn

* **input_dim**: # features in pytorch, not needed for tensorflow
* **layers_info**: list of integers to indicate number of hidden units you want per linear layer. 
* for example:

```
from nn_builder.pytorch.nn import nn   
model = nn(input_dim=5, layers_info=[10, 10, 1], output_activation=none, hidden_activations=""relu"", 
           dropout=0.0, initialiser=""xavier"", batch_norm=false)            
```
--- 
### 2. cnn

* **input_dim**: (# channels, height, width) in pytorch, not needed for tensorflow
* **layers_info**: we expect the field *layers_info* to be a list of lists indicating the size and type of layers that you want. each layer in a  cnn can be one of these 4 forms: 
    * [""conv"", channels, kernel size, stride, padding] 
    * [""maxpool"", kernel size, stride, padding]
    * [""avgpool"", kernel size, stride, padding]
    * [""linear"", units]
* for a pytorch network kernel size, stride, padding and units must be integers. for tensorflow they must all be integers except for padding which must be one of {‚Äúvalid‚Äù, ‚Äúsame‚Äù} 
* for example:
```
from nn_builder.pytorch.cnn import cnn   
model = cnn(input_dim=(3, 64, 64), 
            layers_info=[[""conv"", 32, 3, 1, 0], [""maxpool"", 2, 2, 0], 
                         [""conv"", 64, 3, 1, 2], [""avgpool"", 2, 2, 0], 
                         [""linear"", 10]],
            hidden_activations=""relu"", output_activation=""softmax"", dropout=0.0,
            initialiser=""xavier"", batch_norm=true)
```
--- 
### 3. rnn

* **input_dim**: # features in pytorch, not needed for tensorflow
* **layers_info**: we expect the field *layers_info* to be a list of lists indicating the size and type of layers that you want. each layer in a  cnn can be one of these 4 forms: 
    * [""lstm"", units] 
    * [""gru"", units]
    * [""linear"", units]
* for example:

```
from nn_builder.pytorch.cnn import cnn   
model = rnn(input_dim=5, layers_info=[[""gru"", 50], [""lstm"", 10], [""linear"", 2]],
            hidden_activations=""relu"", output_activation=""softmax"", 
            batch_norm=false, dropout=0.0, initialiser=""xavier"")
```
--- 
## contributing

anyone is very welcome to contribute via a pull request. please see the [issues](https://github.com/p-christ/nn_builder/issues) 
page for ideas on the best areas to contribute to and try to:
1. add tests to the tests folder that cover any code you write
1. write comments for every function
1. create a colab notebook demonstrating how any extra functionality you created works

to help you remember things you learn about machine learning in general checkout [save all](https://www.saveall.ai/landing/github_links)
 

 



"
"Jina AI","<p align=""center"">
<br><br><br>
<a href=""https://docs.jina.ai""><img src=""https://github.com/jina-ai/jina/blob/master/docs/_static/logo-light.svg?raw=true"" alt=""jina logo: build multimodal ai services via cloud native technologies ¬∑ neural search ¬∑ generative ai ¬∑ cloud native"" width=""150px""></a>
<br><br><br>
</p>

<p align=""center"">
<b>build multimodal ai services via cloud native technologies</b>
</p>


<p align=center>
<a href=""https://pypi.org/project/jina/""><img alt=""pypi"" src=""https://img.shields.io/pypi/v/jina?label=release&style=flat-square""></a>
<a href=""https://codecov.io/gh/jina-ai/jina""><img alt=""codecov branch"" src=""https://img.shields.io/codecov/c/github/jina-ai/jina/master?&logo=codecov&logocolor=white&style=flat-square""></a>
<a href=""https://jina.ai/slack""><img src=""https://img.shields.io/badge/slack-3.6k-blueviolet?logo=slack&amp;logocolor=white&style=flat-square""></a>
<a href=""https://pypistats.org/packages/jina""><img alt=""pypi - downloads from official pypistats"" src=""https://img.shields.io/pypi/dm/jina?style=flat-square""></a>
<a href=""https://github.com/jina-ai/jina/actions/workflows/cd.yml""><img alt=""github cd status"" src=""https://github.com/jina-ai/jina/actions/workflows/cd.yml/badge.svg""></a>
</p>

<!-- start jina-description -->

jina is a mlops framework that empowers anyone to build multimodal ai services via cloud native technologies. it uplifts a local poc into a production-ready service. jina handles the infrastructure complexity, making advanced solution engineering and cloud-native technologies accessible to every developer. 

applications built with jina enjoy the following features out of the box:

üåå **universal**
  - build applications that deliver fresh insights from multiple data types such as text, image, audio, video, 3d mesh, pdf with [lf's docarray](https://github.com/docarray/docarray).
  - support all mainstream deep learning frameworks.
  - polyglot gateway that supports grpc, websockets, http, graphql protocols with tls.

‚ö° **performance**
  - intuitive design pattern for high-performance microservices.
  - scaling at ease: set replicas, sharding in one line. 
  - duplex streaming between client and server.
  - async and non-blocking data processing over dynamic flows.

‚òÅÔ∏è **cloud native**
  - seamless docker container integration: sharing, exploring, sandboxing, versioning and dependency control via [executor hub](https://cloud.jina.ai).
  - full observability via opentelemetry, prometheus and grafana.
  - fast deployment to kubernetes, docker compose.

üç± **ecosystem**
  - improved engineering efficiency thanks to the jina ai ecosystem, so you can focus on innovating with the data applications you build.
  - free cpu/gpu hosting via [jina ai cloud](https://cloud.jina.ai).

<!-- end jina-description -->

<p align=""center"">
<a href=""#""><img src=""https://github.com/jina-ai/jina/blob/master/.github/readme/core-tree-graph.svg?raw=true"" alt=""jina in jina ai neural search ecosystem"" width=""100%""></a>
</p>




## [documentation](https://docs.jina.ai)

## install 

```bash
pip install jina
```

find more install options on [apple silicon](https://docs.jina.ai/get-started/install/apple-silicon-m1-m2/)/[windows](https://docs.jina.ai/get-started/install/windows/).


## get started


### basic concepts

document, executor and flow are three fundamental concepts in jina.

- [**document**](https://docarray.jina.ai/) is the fundamental data structure.
- [**executor**](https://docs.jina.ai/concepts/executor/) is a python class with functions that use documents as io.
- [**flow**](https://docs.jina.ai/concepts/flow/) ties executors together into a pipeline and exposes it with an api gateway.

[the full glossary is explained here.](https://docs.jina.ai/concepts/preliminaries/#)


---

<p align=""center"">
<a href=""https://docs.jina.ai""><img src=""https://github.com/jina-ai/jina/blob/master/.github/readme/streamline-banner.png?raw=true"" alt=""jina: streamline ai & ml product delivery"" width=""100%""></a>
</p>

### streamline ai & ml product delivery

a new project starts from local. with jina, you can easily leverage existing deep learning stacks, improve the quality and quickly build the poc.

```python
import torch
from jina import documentarray

model = torch.nn.sequential(
    torch.nn.linear(
        in_features=128,
        out_features=128,
    ),
    torch.nn.relu(),
    torch.nn.linear(in_features=128, out_features=32),
)


docs = documentarray.from_files('left/*.jpg')
docs.embed(model)
```

moving to production, jina enhances the poc with service endpoint, scalability and adds cloud-native features, making it ready for production without refactoring.

<table>
<tr>
<td>

```python
from jina import documentarray, executor, requests
from .embedding import model


class myexec(executor):
    @requests(on='/embed')
    async def embed(self, docs: documentarray, **kwargs):
        docs.embed(model)
```

</td>
<td>
    
```yaml
jtype: flow
with:
  port: 12345
executors:
- uses: myexec
  replicas: 2
```
</td>
</tr>
</table>


finally, the project can be easily deployed to the cloud and serve real traffic.

```bash
jina cloud deploy ./
```

---

<p align=""center"">
<a href=""https://docs.jina.ai""><img src=""https://github.com/jina-ai/jina/blob/master/.github/readme/no-complexity-banner.png?raw=true"" alt=""jina: no infrastructure complexity, high engineering efficiency"" width=""100%""></a>
</p>

### hello world example

leveraging these three concepts, let's look at a simple example below:

```python
from jina import documentarray, executor, flow, requests


class myexec(executor):
    @requests
    async def add_text(self, docs: documentarray, **kwargs):
        for d in docs:
            d.text += 'hello, world!'


f = flow().add(uses=myexec).add(uses=myexec)

with f:
    r = f.post('/', documentarray.empty(2))
    print(r.texts)
```

- the first line imports three concepts we just introduced;
- `myexec` defines an async function `add_text` that receives `documentarray` from network requests and appends `""hello, world""` to `.text`;
- `f` defines a flow streamlined two executors in a chain;
- the `with` block opens the flow, sends an empty documentarray to the flow, and prints the result.

running it gives you:

<p align=""center"">
<a href=""#""><img src=""https://github.com/jina-ai/jina/blob/master/.github/readme/run-hello-world.gif?raw=true"" alt=""running a simple hello-world program"" width=""70%""></a>
</p>

at the last line we see its output `['hello, world!hello, world!', 'hello, world!hello, world!']`.


while you could use standard python with the same number of lines and get the same output, jina accelerates time to market of your application by making it more scalable and cloud-native. jina also handles the infrastructure complexity in production and other day-2 operations so that you can focus on the data application itself.  

---

<p align=""center"">
<a href=""https://docs.jina.ai""><img src=""https://github.com/jina-ai/jina/blob/master/.github/readme/scalability-banner.png?raw=true"" alt=""jina: scalability and concurrency at ease"" width=""100%""></a>
</p>

### scalability and concurrency at ease

the example above can be refactored into a python executor file and a flow yaml file:

<table>
<tr>
<th> <code>toy.yml</code> </th> 
<th> executor.py </th>
</tr>
<tr>
<td> 

```yaml
jtype: flow
with:
  port: 51000
  protocol: grpc
executors:
- uses: myexec
  name: foo
  py_modules:
    - executor.py
- uses: myexec
  name: bar
  py_modules:
    - executor.py
```
     
</td>
<td>

```python
from jina import documentarray, executor, requests


class myexec(executor):
    @requests
    async def add_text(self, docs: documentarray, **kwargs):
        for d in docs:
            d.text += 'hello, world!'
```

</td>
</tr>
</table>


run the following command in the terminal:

```bash
jina flow --uses toy.yml
```

<p align=""center"">
<a href=""#""><img src=""https://github.com/jina-ai/jina/blob/master/.github/readme/flow-block.png?raw=true"" alt=""running a simple hello-world program"" width=""50%""></a>
</p>

the server is successfully started, and you can now use a client to query it.

```python
from jina import client, document

c = client(host='grpc://0.0.0.0:51000')
c.post('/', document())
```

this simple refactoring allows developers to write an application in the client-server style. the separation of flow yaml and executor python file does not only make the project more maintainable but also brings scalability and concurrency to the next level:
- the data flow on the server is non-blocking and async. new request is handled immediately when an executor is free, regardless if previous request is still being processed.
- scalability can be easily achieved by the keywords `replicas` and `needs` in yaml/python. load-balancing is automatically added when necessary to ensure the maximum throughput.

<table>
<tr>
<th> <code>toy.yml</code> </th> 
<th> flowchart </th>
</tr>
<tr>
<td> 

```yaml
jtype: flow
with:
  port: 51000
  protocol: grpc
executors:
- uses: myexec
  name: foo
  py_modules:
    - executor.py
  replicas: 2
- uses: myexec
  name: bar
  py_modules:
    - executor.py
  replicas: 3
  needs: gateway
- needs: [foo, bar]
  name: baz
```
     
</td>
<td>

<p align=""center"">
<a href=""#""><img src=""https://github.com/jina-ai/jina/blob/master/.github/readme/scale-flow.svg?raw=true"" alt=""running a simple hello-world program"" width=""70%""></a>
</p>

</td>
</tr>
</table>

- you now have an api gateway that supports grpc (default), websockets, and http protocols with tls.
- the communication between clients and the api gateway is duplex.
- the api gateway allows you to route request to a specific executor while other parts of the flow are still busy, via `.post(..., target_executor=...)`

---

<p align=""center"">
<a href=""https://docs.jina.ai""><img src=""https://github.com/jina-ai/jina/blob/master/.github/readme/container-banner.png?raw=true"" alt=""jina: seamless container integration"" width=""100%""></a>
</p>

### seamless container integration

without having to worry about dependencies, you can easily share your executors with others; or use public/private executors in your project thanks to [executor hub](https://cloud.jina.ai).

to create an executor:

```bash
jina hub new 
```

to push it to executor hub:

```bash
jina hub push .
```

to use a hub executor in your flow:

|        | docker container                           | sandbox                                     | source                              |
|--------|--------------------------------------------|---------------------------------------------|-------------------------------------|
| yaml   | `uses: jinaai+docker://<username>/myexecutor`        | `uses: jinaai+sandbox://<username>/myexecutor`        | `uses: jinaai://<username>/myexecutor`        |
| python | `.add(uses='jinaai+docker://<username>/myexecutor')` | `.add(uses='jinaai+sandbox://<username>/myexecutor')` | `.add(uses='jinaai://<username>/myexecutor')` |

behind this smooth experience is advanced management of executors:
- automated builds on the cloud
- store, deploy, and deliver executors cost-efficiently;
- automatically resolve version conflicts and dependencies;
- instant delivery of any executor via sandbox without pulling anything to local.

---

<p align=""center"">
<a href=""https://docs.jina.ai""><img src="".github/readme/cloud-native-banner.png?raw=true"" alt=""jina: seamless container integration"" width=""100%""></a>
</p>

### fast-lane to cloud-native

using kubernetes becomes easy:

```bash
jina export kubernetes flow.yml ./my-k8s
kubectl apply -r -f my-k8s
```

using docker compose becomes easy:

```bash
jina export docker-compose flow.yml docker-compose.yml
docker-compose up
```

tracing and monitoring with opentelemetry is straightforward:

```python
from jina import executor, requests, documentarray


class myexec(executor):
    @requests
    def encode(self, docs: documentarray, **kwargs):
        with self.tracer.start_as_current_span(
            'encode', context=tracing_context
        ) as span:
            with self.monitor(
                'preprocessing_seconds', 'time preprocessing the requests'
            ):
                docs.tensors = preprocessing(docs)
            with self.monitor(
                'model_inference_seconds', 'time doing inference the requests'
            ):
                docs.embedding = model_inference(docs.tensors)
```

you can integrate jaeger or any other distributed tracing tools to collect and visualize request-level and application level service operation attributes. this helps you analyze request-response lifecycle, application behavior and performance.

to use grafana, [download this json](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow-histogram-metrics.json) and import it into grafana:

<p align=""center"">
<a href=""https://docs.jina.ai""><img src="".github/readme/grafana-histogram-metrics.png?raw=true"" alt=""jina: seamless container integration"" width=""70%""></a>
</p>

to trace requests with jaeger:
<p align=""center"">
<a href=""https://docs.jina.ai""><img src="".github/readme/jaeger-tracing-example.png?raw=true"" alt=""jina: seamless container integration"" width=""70%""></a>
</p>


what cloud-native technology is still challenging to you? [tell us](https://github.com/jina-ai/jina/issues), we will handle the complexity and make it easy for you.

<!-- start support-pitch -->

## support

- join our [slack community](https://jina.ai/slack) and chat with other community members about ideas.
- join our [engineering all hands](https://youtube.com/playlist?list=pl3ubbwouvhfyrua_gpyykbqeako4sxmne) meet-up to discuss your use case and learn jina's new features.
    - **when?** the second tuesday of every month
    - **where?**
      zoom ([see our public events calendar](https://calendar.google.com/calendar/embed?src=c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com&ctz=europe%2fberlin)/[.ical](https://calendar.google.com/calendar/ical/c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com/public/basic.ics))
      and [live stream on youtube](https://youtube.com/c/jina-ai)
- subscribe to the latest video tutorials on our [youtube channel](https://youtube.com/c/jina-ai)

## join us

jina is backed by [jina ai](https://jina.ai) and licensed under [apache-2.0](./license).

<!-- end support-pitch -->
"
"SLM Lab","# slm lab <br> ![github tag (latest semver)](https://img.shields.io/github/tag/kengz/slm-lab) ![ci](https://github.com/kengz/slm-lab/workflows/ci/badge.svg) [![maintainability](https://api.codeclimate.com/v1/badges/20c6a124c468b4d3e967/maintainability)](https://codeclimate.com/github/kengz/slm-lab/maintainability) [![test coverage](https://api.codeclimate.com/v1/badges/20c6a124c468b4d3e967/test_coverage)](https://codeclimate.com/github/kengz/slm-lab/test_coverage)


<p align=""center"">
  <i>modular deep reinforcement learning framework in pytorch.</i>
  <br><br>
  <b>documentation:</b><br>
  <a href=""https://slm-lab.gitbook.io/slm-lab/"">https://slm-lab.gitbook.io/slm-lab/</a>
  <br><br>
</p>

>note: the `book` branch has been updated for issue fixes. for the original code in the book _foundations of deep reinforcement learning_, check out to git tag `v4.1.1`

|||||
|:---:|:---:|:---:|:---:|
| ![ppo beamrider](https://user-images.githubusercontent.com/8209263/63994698-689ecf00-caaa-11e9-991f-0a5e9c2f5804.gif) | ![ppo breakout](https://user-images.githubusercontent.com/8209263/63994695-650b4800-caaa-11e9-9982-2462738caa45.gif) | ![ppo kungfumaster](https://user-images.githubusercontent.com/8209263/63994690-60469400-caaa-11e9-9093-b1cd38cee5ae.gif) | ![ppo mspacman](https://user-images.githubusercontent.com/8209263/63994685-5cb30d00-caaa-11e9-8f35-78e29a7d60f5.gif) |
| beamrider | breakout | kungfumaster | mspacman |
| ![ppo pong](https://user-images.githubusercontent.com/8209263/63994680-59b81c80-caaa-11e9-9253-ed98370351cd.gif) | ![ppo qbert](https://user-images.githubusercontent.com/8209263/63994672-54f36880-caaa-11e9-9757-7780725b53af.gif) | ![ppo seaquest](https://user-images.githubusercontent.com/8209263/63994665-4dcc5a80-caaa-11e9-80bf-c21db818115b.gif) | ![ppo spaceinvaders](https://user-images.githubusercontent.com/8209263/63994624-15c51780-caaa-11e9-9c9a-854d3ce9066d.gif) |
| pong | qbert | seaquest | sp.invaders |
| ![sac ant](https://user-images.githubusercontent.com/8209263/63994867-ff6b8b80-caaa-11e9-971e-2fac1cddcbac.gif) | ![sac halfcheetah](https://user-images.githubusercontent.com/8209263/63994869-01354f00-caab-11e9-8e11-3893d2c2419d.gif) | ![sac hopper](https://user-images.githubusercontent.com/8209263/63994871-0397a900-caab-11e9-9566-4ca23c54b2d4.gif) | ![sac humanoid](https://user-images.githubusercontent.com/8209263/63994883-0befe400-caab-11e9-9bcc-c30c885aad73.gif) |
| ant | halfcheetah | hopper | humanoid |
| ![sac doublependulum](https://user-images.githubusercontent.com/8209263/63994879-07c3c680-caab-11e9-974c-06cdd25bfd68.gif) | ![sac pendulum](https://user-images.githubusercontent.com/8209263/63994880-085c5d00-caab-11e9-850d-049401540e3b.gif) | ![sac reacher](https://user-images.githubusercontent.com/8209263/63994881-098d8a00-caab-11e9-8e19-a3b32d601b10.gif) | ![sac walker](https://user-images.githubusercontent.com/8209263/63994882-0abeb700-caab-11e9-9e19-b59dc5c43393.gif) |
| inv.doublependulum | invertedpendulum | reacher | walker |

"
"garage","[![docs](https://readthedocs.org/projects/garage/badge)](http://garage.readthedocs.org/en/latest/)
[![garage ci](https://github.com/rlworkgroup/garage/workflows/garage%20ci/badge.svg?event=schedule)](https://github.com/rlworkgroup/garage/actions?query=workflow%3a%22garage+ci%22)
[![license](https://img.shields.io/badge/license-mit-blue.svg)](https://github.com/rlworkgroup/garage/blob/master/license)
[![codecov](https://codecov.io/gh/rlworkgroup/garage/branch/master/graph/badge.svg)](https://codecov.io/gh/rlworkgroup/garage)
[![pypi version](https://badge.fury.io/py/garage.svg)](https://badge.fury.io/py/garage)

# garage

garage is a toolkit for developing and evaluating reinforcement learning
algorithms, and an accompanying library of state-of-the-art implementations
built using that toolkit.

the toolkit provides wide range of modular tools for implementing rl algorithms,
including:

* composable neural network models
* replay buffers
* high-performance samplers
* an expressive experiment definition interface
* tools for reproducibility (e.g. set a global random seed which all components
  respect)
* logging to many outputs, including tensorboard
* reliable experiment checkpointing and resuming
* environment interfaces for many popular benchmark suites
* supporting for running garage in diverse environments, including always
  up-to-date docker containers

see the [latest documentation](https://garage.readthedocs.org/en/latest/) for
getting started instructions and detailed apis.

## installation

```
pip install --user garage
```

## examples

starting from version v2020.10.0, garage comes packaged with examples. to get a
list of examples, run:

```
garage examples
```

you can also run `garage examples --help`, or visit
[the documentation](https://garage.readthedocs.io/en/latest/user/get_started.html#running-examples)
for even more details.

## join the community

**join the [garage-announce mailing list](https://groups.google.com/forum/#!forum/garage-announce/join)**
for infrequent updates (<1/mo.) on the status of the project and new releases.

need some help? want to ask garage is right for your project? have a question
which is not quite a bug and not quite a feature request?

**join the community slack** by filling out
[this google form](https://docs.google.com/forms/d/e/1faipqlsf4axriba1clgjku4lirq6btstwpeimeg3j17i4_fhfqu8x0g/viewform).

## algorithms

the table below summarizes the algorithms available in garage.

| algorithm              | framework(s)        |
| ---------------------- | ------------------- |
| cem                    | numpy               |
| cma-es                 | numpy               |
| reinforce (a.k.a. vpg) | pytorch, tensorflow |
| ddpg                   | pytorch, tensorflow |
| dqn                    | pytorch, tensorflow |
| ddqn                   | pytorch, tensorflow |
| erwr                   | tensorflow          |
| npo                    | tensorflow          |
| ppo                    | pytorch, tensorflow |
| reps                   | tensorflow          |
| td3                    | pytorch, tensorflow |
| tnpg                   | tensorflow          |
| trpo                   | pytorch, tensorflow |
| maml                   | pytorch             |
| rl2                    | tensorflow          |
| pearl                  | pytorch             |
| sac                    | pytorch             |
| mtsac                  | pytorch             |
| mtppo                  | pytorch, tensorflow |
| mttrpo                 | pytorch, tensorflow |
| task embedding         | tensorflow          |
| behavioral cloning     | pytorch             |

## supported tools and frameworks

garage requires python 3.6+. if you need python 3.5 support, the last garage
release to support python 3.5 was
[v2020.06](https://github.com/rlworkgroup/garage/releases/tag/v2020.06.0).

the package is tested on ubuntu 18.04. it is also known to run on ubuntu 16.04,
18.04, and 20.04, and recent versions of macos using homebrew. windows users can
install garage via wsl, or by making use of the docker containers.

we currently support [pytorch](https://pytorch.org/) and
[tensorflow](https://www.tensorflow.org/) for implementing the neural network
portions of rl algorithms, and additions of new framework support are always
welcome. pytorch modules can be found in the package
[`garage.torch`](https://github.com/rlworkgroup/garage/tree/master/src/garage/torch)
and tensorflow modules can be found in the package
[`garage.tf`](https://github.com/rlworkgroup/garage/tree/master/src/garage/tf).
algorithms which do not require neural networks are found in the package
[`garage.np`](https://github.com/rlworkgroup/garage/tree/master/src/garage/np).

the package is available for download on pypi, and we ensure that it installs
successfully into environments defined using
[conda](https://docs.conda.io/en/latest/),
[pipenv](https://pipenv.readthedocs.io/en/latest/), and
[virtualenv](https://virtualenv.pypa.io/en/latest/).

## testing

the most important feature of garage is its comprehensive automated unit test
and benchmarking suite, which helps ensure that the algorithms and modules in
garage maintain state-of-the-art performance as the software changes.

our testing strategy has three pillars:

* **automation:**
  we use continuous integration to test all modules and algorithms in garage
  before adding any change. the full installation and test suite is also run
  nightly, to detect regressions.
* **acceptance testing:**
  any commit which might change the performance of an algorithm is subjected to
  comprehensive benchmarks on the relevant algorithms before it is merged
* **benchmarks and monitoring:**
  we benchmark the full suite of algorithms against their relevant benchmarks
  and widely-used implementations regularly, to detect regressions and
  improvements we may have missed.

## supported releases

| release | build status | last date of support |
| ------- | ------------ | -------------------- |
| [v2021.03](https://github.com/rlworkgroup/garage/releases/tag/v2021.03.0) | [![garage ci release-2021.03](https://github.com/rlworkgroup/garage/workflows/garage%20ci%20release-2021.03/badge.svg)](https://github.com/rlworkgroup/garage/actions?query=workflow%3a%22garage+ci+release-2021.03%22) | may 31st, 2021 |

maintenance releases have a stable api and dependency tree,
and receive bug fixes and critical improvements but not new features. we
currently support each release for a window of 2 months.

## citing garage

if you use garage for academic research, please cite the repository using the
following bibtex entry. you should update the `commit` field with the commit or
release tag your publication uses.

```latex
@misc{garage,
 author = {the garage contributors},
 title = {garage: a toolkit for reproducible reinforcement learning research},
 year = {2019},
 publisher = {github},
 journal = {github repository},
 howpublished = {\url{https://github.com/rlworkgroup/garage}},
 commit = {be070842071f736eb24f28e4b902a9f144f5c97b}
}
```

## credits

the earliest code for garage was adopted from predecessor project called
[rllab](https://github.com/rll/rllab). the garage project is grateful for the
contributions of the original rllab authors, and hopes to continue advancing the
state of reproducibility in rl research in the same spirit. garage has
previously been supported by the amazon research award ""watch, practice, learn,
do: unsupervised learning of robust and composable robot motion skills by fusing
expert demonstrations with robot experience.""

---
<p align=""center"" style=""align-items:center; display:inline-block"">made with &#10084; &nbsp;at <a href=""https://robotics.usc.edu/resl/"" target=""_blank""><img align=""absmiddle"" src=""https://github.com/rlworkgroup/garage/blob/master/docs/_static/resl_logo.png?raw=true"" height=""60px""></a> and &nbsp;<a href=""https://viterbischool.usc.edu/"" target=""_blank""><img align=""absmiddle"" src=""https://github.com/rlworkgroup/garage/blob/master/docs/_static/viterbi_logo.png?raw=true"" height=""30px""></a></p>
"
"Maze","![banner](https://github.com/enlite-ai/maze/raw/main/docs/source/logos/main_logo.png)  
<a href=""https://lgtm.com/projects/g/enlite-ai/maze/context:python"">
    <img src=""https://img.shields.io/lgtm/grade/python/g/enlite-ai/maze.svg?logo=lgtm&logowidth=18"" alt=""language grade: python"" />
</a>
![pypi](https://img.shields.io/pypi/v/maze-rl)
![pypi - python version](https://img.shields.io/pypi/pyversions/maze-rl)
[![maze docker image](https://github.com/enlite-ai/maze/actions/workflows/github-ci.yml/badge.svg)](https://github.com/enlite-ai/maze/actions/workflows/github-ci.yml)
![read the docs](https://img.shields.io/readthedocs/maze-rl)
[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/enlite-ai/maze/issues)

# applied reinforcement learning with python

mazerl is an application oriented deep reinforcement learning (rl) framework, addressing real-world decision problems.
our vision is to cover the complete development life cycle of rl applications ranging from simulation 
engineering up to agent development, training and deployment.

*this is a preliminary, non-stable release of maze. it is not yet complete and not all of our interfaces have settled
yet. hence, there might be some breaking changes on our way towards the first stable release.*

## spotlight features

below we list a few selected maze features.

 - design and visualize your policy and value networks with the 
   [perception module](https://maze-rl.readthedocs.io/en/latest/policy_and_value_networks/perception_overview.html). 
   it is based on pytorch and provides a large variety of neural network building blocks and model styles. 
   quickly compose powerful representation learners from building blocks such as: dense, 
   convolution, graph convolution and attention, recurrent architectures, action- and observation masking, 
   self-attention etc.
 - create the conditions for efficient rl training without writing boiler plate code, e.g. by supporting 
   best practices like [pre-processing](https://maze-rl.readthedocs.io/en/latest/environment_customization/observation_pre_processing.html) and 
   [normalizing](https://maze-rl.readthedocs.io/en/latest/environment_customization/observation_normalization.html) your observations.
 - maze supports advanced [environment structures](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/env_hierarchy.html) reflecting 
   the requirements of real-world industrial decision problems such as multi-step and multi-agent scenarios. 
   you can of course work with existing [gym-compatible environments](https://maze-rl.readthedocs.io/en/latest/best_practices_and_tutorials/integrating_gym_environment.html).
 - use the provided [maze trainers](https://maze-rl.readthedocs.io/en/latest/trainers/maze_trainers.html) (a2c, ppo, impala, sac, evolution strategies), 
   which are supporting dictionary action and observation spaces as well as multi-step (auto-regressive policies) training. 
   or stick to your favorite tools and trainers by [combining maze with other rl frameworks](todo/best_practices_and_tutorials/maze_and_other_frameworks.html).
 - out of the box support for advanced training workflows such as [imitation learning from teacher policies and 
   policy fine-tuning](https://maze-rl.readthedocs.io/en/latest/workflow/imitation_and_fine_tuning.html). 
 - keep even complex application and experiment configuration manageable with the [hydra config system](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/hydra.html).
 
## get started

* make sure [pytorch](https://pytorch.org/get-started/locally/) is installed and then get the latest released version of maze as follows:

      pip install -u maze-rl
      
  [read more about other options](https://maze-rl.readthedocs.io/en/latest/getting_started/installation.html) like the installation of the latest 
  development version.  

  :zap:  we encourage you to start with **python 3.7**, as many popular environments like atari or box2d can not easily 
  be installed in newer python environments. maze itself supports newer python versions, but for python 3.9 you might have to
  [install additional binary dependencies manually](https://maze-rl.readthedocs.io/en/latest/getting_started/installation.html)
* alternatively you can work with maze in a <img alt=""docker"" src=""https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/docker_%28container_engine%29_logo.svg/1280px-docker_%28container_engine%29_logo.svg.png"" width=""100"" height=""22"" /> container with pre-installed jupyter lab: run `docker run -p 8888:8888 enliteai/maze:playground` and open `localhost:8888` in your browser. this loads jupyter 
* to see maze in action, check out a [first example](https://maze-rl.readthedocs.io/en/latest/getting_started/first_example.html).
* [try your own gym env](https://maze-rl.readthedocs.io/en/latest/best_practices_and_tutorials/integrating_gym_environment.html)
  or visit our [maze step-by-step tutorial](https://maze-rl.readthedocs.io/en/latest/getting_started/step_by_step_tutorial.html).

<table><tbody><tr>
    <td align=""center""><a href=""https://maze-rl.readthedocs.io/en/latest/getting_started/installation.html"">
        <img src=""https://github.com/enlite-ai/maze/raw/main/.github/pip.png"" alt=""pip"" width=""128px""><br>
        <strong>installation</strong>
    </a></td>
    <td align=""center""><a href=""https://maze-rl.readthedocs.io/en/latest/getting_started/first_example.html"">
        <img src=""https://github.com/enlite-ai/maze/raw/main/.github/start.png"" alt=""first example"" width=""128px""><br>
        <strong>first example</strong>
    </a></td>
    <td align=""center""><a href=""https://maze-rl.readthedocs.io/en/latest/getting_started/step_by_step_tutorial.html"">
        <img src=""https://github.com/enlite-ai/maze/raw/main/.github/steps.png"" alt=""tutorial"" width=""128px""><br>
        <strong>step by step tutorial</strong>
    </a></td>
    <td align=""center""><a href=""https://maze-rl.readthedocs.io/en/latest/"">
        <img src=""https://github.com/enlite-ai/maze/raw/main/.github/paper.png"" alt=""documentation"" width=""128px""><br>
        <strong>documentation</strong>
    </a></td>
</tr></tbody></table>

* clone this [project template repo](https://github.com/enlite-ai/maze-cartpole>) to start your own maze project.

## learn more about maze

the [documentation](https://maze-rl.readthedocs.io/en/latest/index.html#documentation-overview) is the starting point to learn more about
  the underlying concepts, but most importantly also provides code snippets and minimum working examples to 
  get you started quickly.

* the *workflow* section guides you through typical tasks in a rl project
  * [training](https://maze-rl.readthedocs.io/en/latest/workflow/training.html)
  * [rollouts](https://maze-rl.readthedocs.io/en/latest/workflow/rollouts.html)
  * [collection and visualizing rollouts](https://maze-rl.readthedocs.io/en/latest/workflow/rollouts_trajectories_viewer.html)
  * [imitation learning and fine-tuning](https://maze-rl.readthedocs.io/en/latest/workflow/imitation_and_fine_tuning.html) 

* *policy and value networks* introduces you to the 
  [perception module](https://maze-rl.readthedocs.io/en/latest/policy_and_value_networks/perception_overview.html), 
  how to [customize action spaces and the underlying action probability distributions](https://maze-rl.readthedocs.io/en/latest/policy_and_value_networks/distributions_and_action_heads.html) 
  and two styles of policy and value networks construction:
  
  * [template models](https://maze-rl.readthedocs.io/en/latest/policy_and_value_networks/perception_template_models.html) 
    are composed directly from an environment's observation and action space,
    allowing you to train with suitable agent networks on a new environment within minutes.
    
  * [custom models](https://maze-rl.readthedocs.io/en/latest/policy_and_value_networks/perception_custom_models.html) gives you the full 
    flexibility of application specific models, either with the provided maze building blocks or 
    directly with pytorch. 

* learn more about core *concepts and structures* such as the 
  [maze environment hierarchy](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/env_hierarchy.html), the 
  [maze event system](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/event_system.html) providing a convenient way to collect 
  statistics and kpis, enable flexible reward formulation and supporting offline analysis. 

* [structured environments and action masking](https://maze-rl.readthedocs.io/en/latest/best_practices_and_tutorials/struct_env_tutorial.html) 
  introduces you to a general concept, which can greatly improve
  the performance of the trained agents in practical rl problems.

## license

maze is freely available for research and non-commercial use. a commercial license is available, if interested please 
contact us on our [company website](https://enlite.ai) or write us an [email](mailto:office@enlite.ai).

we believe in open source principles and aim at transitioning maze to a commercial open source project, 
releasing larger parts of the framework under a permissive license in the near future.  
"
